{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:45.984537\n",
      "0:00:33.841669\n",
      "20190102 unzip finished\n",
      "0:00:43.560642\n",
      "0:01:05.921339\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:08:44.763251\n",
      "0:00:30.660770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.001463\n",
      "no massive missing\n",
      "0:01:47.504351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.025820\n",
      "0:00:34.239178\n",
      "20190103 unzip finished\n",
      "0:00:45.921740\n",
      "0:01:10.714202\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:08:57.652666\n",
      "0:00:31.642739\n",
      "0:00:03.152455\n",
      "no massive missing\n",
      "0:01:45.283587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:53.146966\n",
      "0:00:46.573862\n",
      "20190104 unzip finished\n",
      "0:00:53.639910\n",
      "0:01:15.000744\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:58.459681\n",
      "0:00:36.982629\n",
      "0:00:03.524324\n",
      "no massive missing\n",
      "0:01:57.626434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190104"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:53.500348\n",
      "0:00:41.798848\n",
      "20190107 unzip finished\n",
      "0:00:49.163369\n",
      "0:01:17.648687\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:07.260687\n",
      "0:00:37.993502\n",
      "0:00:03.390918\n",
      "no massive missing\n",
      "0:02:19.672149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.538487\n",
      "0:01:20.969300\n",
      "20190108 unzip finished\n",
      "0:00:48.212476\n",
      "0:01:09.669859\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:39.776289\n",
      "0:00:35.204770\n",
      "0:00:03.615784\n",
      "no massive missing\n",
      "0:01:51.527356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:51.774512\n",
      "0:00:58.562927\n",
      "20190109 unzip finished\n",
      "0:00:51.663963\n",
      "0:01:18.108653\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:33.712160\n",
      "0:00:38.923074\n",
      "0:00:03.828584\n",
      "no massive missing\n",
      "0:02:04.435076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.367732\n",
      "0:00:46.325992\n",
      "20190110 unzip finished\n",
      "0:00:47.318049\n",
      "0:01:14.120923\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:54.943642\n",
      "0:00:34.307670\n",
      "0:00:03.315368\n",
      "no massive missing\n",
      "0:02:02.792536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.568453\n",
      "0:00:34.760278\n",
      "20190111 unzip finished\n",
      "0:00:47.735262\n",
      "0:01:12.261136\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:58.218760\n",
      "0:00:46.214046\n",
      "0:00:03.319891\n",
      "no massive missing\n",
      "0:01:54.913931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.421452\n",
      "0:00:40.544814\n",
      "20190114 unzip finished\n",
      "0:00:47.323716\n",
      "0:01:11.356472\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:51.436820\n",
      "0:00:33.543315\n",
      "0:00:03.295977\n",
      "no massive missing\n",
      "0:01:52.377864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.122061\n",
      "0:00:41.991049\n",
      "20190115 unzip finished\n",
      "0:00:50.084752\n",
      "0:01:13.572987\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:57.961863\n",
      "0:00:35.490900\n",
      "0:00:03.523785\n",
      "no massive missing\n",
      "0:02:20.376810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.778289\n",
      "0:00:42.971410\n",
      "20190116 unzip finished\n",
      "0:00:51.111281\n",
      "0:01:14.030373\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:06.684940\n",
      "0:00:35.079690\n",
      "0:00:03.512448\n",
      "no massive missing\n",
      "0:01:54.940473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.820420\n",
      "0:00:43.479462\n",
      "20190117 unzip finished\n",
      "0:00:53.447839\n",
      "0:01:15.367850\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:04.095791\n",
      "0:00:39.924835\n",
      "0:00:03.892494\n",
      "no massive missing\n",
      "0:02:01.339959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.305213\n",
      "0:00:41.853579\n",
      "20190118 unzip finished\n",
      "0:00:51.789398\n",
      "0:01:15.194699\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:22.155529\n",
      "0:00:46.213244\n",
      "0:00:03.525210\n",
      "no massive missing\n",
      "0:02:24.239531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:52.614943\n",
      "0:00:40.922228\n",
      "20190121 unzip finished\n",
      "0:00:50.648644\n",
      "0:01:16.601965\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:14.286730\n",
      "0:00:35.460695\n",
      "0:00:04.245376\n",
      "no massive missing\n",
      "0:01:52.451651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.017454\n",
      "0:00:42.652202\n",
      "20190122 unzip finished\n",
      "0:01:28.504796\n",
      "0:01:15.575280\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:56.304878\n",
      "0:00:36.950585\n",
      "0:00:03.650371\n",
      "no massive missing\n",
      "0:01:58.852388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.445936\n",
      "0:00:48.649601\n",
      "20190123 unzip finished\n",
      "0:00:47.965847\n",
      "0:01:15.209873\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:05.216043\n",
      "0:00:47.432279\n",
      "0:00:04.316615\n",
      "no massive missing\n",
      "0:02:05.967453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:47.360547\n",
      "0:00:51.514876\n",
      "20190124 unzip finished\n",
      "0:00:49.846707\n",
      "0:01:14.128699\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:16.363401\n",
      "0:00:42.331633\n",
      "0:00:03.894810\n",
      "no massive missing\n",
      "0:02:03.195172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.177360\n",
      "0:00:41.225798\n",
      "20190125 unzip finished\n",
      "0:00:51.816792\n",
      "0:01:17.840703\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:13.652094\n",
      "0:00:34.453189\n",
      "0:00:03.495564\n",
      "no massive missing\n",
      "0:01:51.632266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.826015\n",
      "0:00:45.778265\n",
      "20190128 unzip finished\n",
      "0:00:51.560450\n",
      "0:01:14.809191\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:35.678018\n",
      "0:00:38.741532\n",
      "0:00:03.665574\n",
      "no massive missing\n",
      "0:01:57.009592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.864005\n",
      "0:00:41.279607\n",
      "20190129 unzip finished\n",
      "0:00:49.271007\n",
      "0:01:19.822848\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:13.842107\n",
      "0:00:35.723109\n",
      "0:00:04.395274\n",
      "no massive missing\n",
      "0:02:16.600238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.323909\n",
      "0:00:36.592427\n",
      "20190130 unzip finished\n",
      "0:00:48.707559\n",
      "0:01:12.197984\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:51.758839\n",
      "0:00:35.708949\n",
      "0:00:03.585133\n",
      "no massive missing\n",
      "0:01:50.976828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190130"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.787257\n",
      "0:00:41.358792\n",
      "20190131 unzip finished\n",
      "0:00:54.779245\n",
      "0:01:17.855764\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:16.317892\n",
      "0:00:36.652977\n",
      "0:00:03.779393\n",
      "no massive missing\n",
      "0:02:03.678260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.507389\n",
      "0:00:40.255558\n",
      "20190201 unzip finished\n",
      "0:00:48.909609\n",
      "0:01:13.536638\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:51.859938\n",
      "0:00:34.040086\n",
      "0:00:03.431051\n",
      "no massive missing\n",
      "0:01:55.914691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190201"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:53.513977\n",
      "0:01:02.298861\n",
      "20190211 unzip finished\n",
      "0:00:51.520887\n",
      "0:01:17.296483\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:18.867982\n",
      "0:00:36.933072\n",
      "0:00:03.744542\n",
      "no massive missing\n",
      "0:02:06.364624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.142593\n",
      "0:00:48.501699\n",
      "20190212 unzip finished\n",
      "0:00:54.445138\n",
      "0:01:21.726556\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:48.411095\n",
      "0:00:36.191451\n",
      "0:00:03.921313\n",
      "no massive missing\n",
      "0:01:59.729548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.985067\n",
      "0:00:49.522806\n",
      "20190213 unzip finished\n",
      "0:00:57.832036\n",
      "0:01:26.802130\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:18.753690\n",
      "0:00:42.056242\n",
      "0:00:04.095586\n",
      "no massive missing\n",
      "0:02:32.988362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.559951\n",
      "0:00:56.064423\n",
      "20190214 unzip finished\n",
      "0:00:54.692361\n",
      "0:01:19.549742\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:41.278030\n",
      "0:00:40.378683\n",
      "0:00:03.819867\n",
      "no massive missing\n",
      "0:02:19.317955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.817877\n",
      "0:00:47.686291\n",
      "20190215 unzip finished\n",
      "0:00:52.181199\n",
      "0:01:20.875058\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:05.480544\n",
      "0:00:40.351011\n",
      "0:00:03.919106\n",
      "no massive missing\n",
      "0:02:25.749925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.258970\n",
      "0:00:56.888968\n",
      "20190218 unzip finished\n",
      "0:00:55.988169\n",
      "0:01:26.206695\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:27.897163\n",
      "0:00:41.909578\n",
      "0:00:04.157175\n",
      "no massive missing\n",
      "0:02:17.734738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:17.931353\n",
      "0:01:01.176531\n",
      "20190219 unzip finished\n",
      "0:00:58.450487\n",
      "0:01:27.013116\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:06.439192\n",
      "0:00:42.914452\n",
      "0:00:04.027947\n",
      "no massive missing\n",
      "0:02:10.279807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.376786\n",
      "0:00:47.533531\n",
      "20190220 unzip finished\n",
      "0:00:54.772412\n",
      "0:01:24.176498\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:41.373820\n",
      "0:00:40.279348\n",
      "0:00:03.942291\n",
      "no massive missing\n",
      "0:02:05.549757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.261674\n",
      "0:00:54.383751\n",
      "20190221 unzip finished\n",
      "0:01:02.001735\n",
      "0:01:29.696119\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:11.969460\n",
      "0:00:43.702106\n",
      "0:00:04.132642\n",
      "no massive missing\n",
      "0:02:19.574896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:20.137664\n",
      "0:00:57.695764\n",
      "20190222 unzip finished\n",
      "0:00:54.008651\n",
      "0:01:25.275801\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:50.455490\n",
      "0:00:41.125291\n",
      "0:00:04.134363\n",
      "no massive missing\n",
      "0:02:15.146807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.349748\n",
      "0:01:03.985086\n",
      "20190225 unzip finished\n",
      "0:01:17.430013\n",
      "0:01:36.213121\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:52.466767\n",
      "0:00:53.771300\n",
      "0:00:04.404259\n",
      "no massive missing\n",
      "0:02:28.320305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:32.041726\n",
      "0:01:06.210841\n",
      "20190226 unzip finished\n",
      "0:01:07.341209\n",
      "0:01:38.906534\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:10.097639\n",
      "0:00:44.813499\n",
      "0:00:04.527017\n",
      "no massive missing\n",
      "0:02:32.808467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190226"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:35.068344\n",
      "0:00:52.373781\n",
      "20190227 unzip finished\n",
      "0:01:04.177360\n",
      "0:01:35.229524\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:41.405144\n",
      "0:00:45.303236\n",
      "0:00:04.590643\n",
      "no massive missing\n",
      "0:02:42.207251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.117852\n",
      "0:00:56.342222\n",
      "20190228 unzip finished\n",
      "0:00:57.027257\n",
      "0:01:33.153947\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:17.633224\n",
      "0:00:43.333673\n",
      "0:00:04.310095\n",
      "no massive missing\n",
      "0:02:29.409967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.822508\n",
      "0:00:58.818688\n",
      "20190301 unzip finished\n",
      "0:00:59.787652\n",
      "0:01:31.517664\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:56.096308\n",
      "0:00:42.449342\n",
      "0:00:04.142125\n",
      "no massive missing\n",
      "0:02:18.255491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.307485\n",
      "0:01:08.002547\n",
      "20190304 unzip finished\n",
      "0:01:09.116092\n",
      "0:01:39.022474\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:51.580659\n",
      "0:00:51.528609\n",
      "0:00:04.555144\n",
      "no massive missing\n",
      "0:02:46.433641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.969425\n",
      "0:00:52.278947\n",
      "20190305 unzip finished\n",
      "0:01:02.811176\n",
      "0:01:36.994744\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:50.121480\n",
      "0:00:47.150147\n",
      "0:00:04.565923\n",
      "no massive missing\n",
      "0:02:32.482171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.261110\n",
      "0:01:02.021399\n",
      "20190306 unzip finished\n",
      "0:01:10.316418\n",
      "0:01:41.871397\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:29.254216\n",
      "0:00:47.431598\n",
      "0:00:04.421499\n",
      "no massive missing\n",
      "0:02:31.162628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.128354\n",
      "0:01:10.513474\n",
      "20190307 unzip finished\n",
      "0:01:10.996408\n",
      "0:01:39.421326\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:35.154775\n",
      "0:00:46.563553\n",
      "0:00:04.516704\n",
      "no massive missing\n",
      "0:02:44.799176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190307"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:31.663909\n",
      "0:00:58.404471\n",
      "20190308 unzip finished\n",
      "0:01:07.211702\n",
      "0:01:38.694658\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:03.156434\n",
      "0:00:52.089011\n",
      "0:00:04.605328\n",
      "no massive missing\n",
      "0:03:10.526532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:33.111020\n",
      "0:01:10.055364\n",
      "20190311 unzip finished\n",
      "0:01:04.680973\n",
      "0:01:45.487273\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:52.384479\n",
      "0:00:43.515227\n",
      "0:00:04.335994\n",
      "no massive missing\n",
      "0:02:58.377502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:25.064226\n",
      "0:01:14.677722\n",
      "20190312 unzip finished\n",
      "0:01:07.744961\n",
      "0:01:53.146467\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:46.684668\n",
      "0:00:46.831292\n",
      "0:00:04.422855\n",
      "no massive missing\n",
      "0:02:30.528612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:28.515950\n",
      "0:00:59.434072\n",
      "20190313 unzip finished\n",
      "0:01:04.268569\n",
      "0:01:38.180483\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:37.970230\n",
      "0:00:48.000640\n",
      "0:00:04.545102\n",
      "no massive missing\n",
      "0:02:43.319100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190313"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:14.426785\n",
      "0:01:08.445770\n",
      "20190314 unzip finished\n",
      "0:01:01.664551\n",
      "0:01:37.224214\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:55.324572\n",
      "0:00:44.496257\n",
      "0:00:04.377166\n",
      "no massive missing\n",
      "0:02:30.923008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.829585\n",
      "0:01:40.166280\n",
      "20190315 unzip finished\n",
      "0:00:58.422265\n",
      "0:01:35.425616\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190101'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:44.737086\n",
      "0:00:46.278985\n",
      "20190404 unzip finished\n",
      "0:01:03.732189\n",
      "0:01:42.145168\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:54.793987\n",
      "0:00:49.827541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.317053\n",
      "no massive missing\n",
      "0:02:27.564455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:24.926164\n",
      "0:00:47.661270\n",
      "20190408 unzip finished\n",
      "0:01:03.468443\n",
      "0:01:45.135777\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:49.612126\n",
      "0:00:45.379315\n",
      "0:00:04.459657\n",
      "no massive missing\n",
      "0:02:28.437998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:27.075206\n",
      "0:00:46.131886\n",
      "20190409 unzip finished\n",
      "0:01:05.294475\n",
      "0:01:42.781908\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:21.938244\n",
      "0:00:51.226768\n",
      "0:00:04.627472\n",
      "no massive missing\n",
      "0:02:22.532368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:32.584276\n",
      "0:00:45.009972\n",
      "20190410 unzip finished\n",
      "0:01:00.512704\n",
      "0:01:40.938759\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:34.011283\n",
      "0:00:46.650469\n",
      "0:00:04.282685\n",
      "no massive missing\n",
      "0:02:26.699949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.209660\n",
      "0:00:45.173687\n",
      "20190411 unzip finished\n",
      "0:01:00.472527\n",
      "0:01:41.739439\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:33.283835\n",
      "0:00:42.997688\n",
      "0:00:04.286310\n",
      "no massive missing\n",
      "0:02:23.499989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.504753\n",
      "0:00:44.223090\n",
      "20190412 unzip finished\n",
      "0:00:59.307537\n",
      "0:01:38.647604\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:51.814128\n",
      "0:00:42.263547\n",
      "0:00:04.286566\n",
      "no massive missing\n",
      "0:02:20.129593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.948166\n",
      "0:00:44.061311\n",
      "20190415 unzip finished\n",
      "0:01:00.320900\n",
      "0:01:40.480783\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:59.333821\n",
      "0:00:43.908883\n",
      "0:00:04.185500\n",
      "no massive missing\n",
      "0:02:18.697451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190415"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.246871\n",
      "0:00:45.902015\n",
      "20190416 unzip finished\n",
      "0:01:03.226203\n",
      "0:01:42.504117\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:19.032859\n",
      "0:00:42.967412\n",
      "0:00:04.341413\n",
      "no massive missing\n",
      "0:02:22.017195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:30.419283\n",
      "0:00:55.519503\n",
      "20190417 unzip finished\n",
      "0:00:59.761428\n",
      "0:01:41.371895\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:30.907860\n",
      "0:00:45.607213\n",
      "0:00:04.503319\n",
      "no massive missing\n",
      "0:02:26.476750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190417"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:21.077790\n",
      "0:00:46.584933\n",
      "20190418 unzip finished\n",
      "0:00:59.960018\n",
      "0:01:40.721263\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:06.128015\n",
      "0:00:43.694033\n",
      "0:00:04.629444\n",
      "no massive missing\n",
      "0:02:44.974655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190418"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.699541\n",
      "0:00:43.173262\n",
      "20190419 unzip finished\n",
      "0:00:58.004310\n",
      "0:01:44.213876\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:28.960156\n",
      "0:00:47.153221\n",
      "0:00:04.746797\n",
      "no massive missing\n",
      "0:02:37.899732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190419"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.840862\n",
      "0:00:47.190677\n",
      "20190422 unzip finished\n",
      "0:01:02.398595\n",
      "0:01:38.757822\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:14.566310\n",
      "0:00:45.483416\n",
      "0:00:04.649754\n",
      "no massive missing\n",
      "0:02:30.631588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190422"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.299564\n",
      "0:00:47.880109\n",
      "20190423 unzip finished\n",
      "0:01:01.589322\n",
      "0:01:41.262475\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:09.076378\n",
      "0:00:45.278169\n",
      "0:00:04.579237\n",
      "no massive missing\n",
      "0:02:38.228676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190423"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.341567\n",
      "0:00:44.713789\n",
      "20190424 unzip finished\n",
      "0:00:57.021447\n",
      "0:01:35.810124\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:41.635782\n",
      "0:00:43.414567\n",
      "0:00:04.383456\n",
      "no massive missing\n",
      "0:02:40.023823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190424"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.519565\n",
      "0:00:43.062073\n",
      "20190425 unzip finished\n",
      "0:01:02.261895\n",
      "0:01:41.898379\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:04.922073\n",
      "0:00:43.290271\n",
      "0:00:04.260365\n",
      "no massive missing\n",
      "0:02:33.218485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190425"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:26.538522\n",
      "0:00:46.670395\n",
      "20190426 unzip finished\n",
      "0:01:01.635443\n",
      "0:01:38.719289\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:42.781720\n",
      "0:00:44.129704\n",
      "0:00:04.417841\n",
      "no massive missing\n",
      "0:02:26.838503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:23.599784\n",
      "0:00:43.952056\n",
      "20190429 unzip finished\n",
      "0:00:59.099280\n",
      "0:01:46.259085\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:38.902237\n",
      "0:00:44.469567\n",
      "0:00:05.107945\n",
      "no massive missing\n",
      "0:02:26.793999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:19.441313\n",
      "0:00:40.788032\n",
      "20190430 unzip finished\n",
      "0:00:55.835066\n",
      "0:01:46.187529\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:13.410849\n",
      "0:00:41.988429\n",
      "0:00:04.042118\n",
      "no massive missing\n",
      "0:02:17.165371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190430"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.301662\n",
      "0:00:43.704190\n",
      "20190506 unzip finished\n",
      "0:00:57.833918\n",
      "0:01:49.911020\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:58.157709\n",
      "0:00:44.673223\n",
      "0:00:04.222155\n",
      "no massive missing\n",
      "0:02:33.386601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190506"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.800104\n",
      "0:00:43.779672\n",
      "20190507 unzip finished\n",
      "0:00:57.720211\n",
      "0:01:35.698369\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:06.568463\n",
      "0:00:47.672390\n",
      "0:00:04.393496\n",
      "no massive missing\n",
      "0:02:27.581739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.656068\n",
      "0:00:39.880622\n",
      "20190508 unzip finished\n",
      "0:00:57.586799\n",
      "0:01:35.150756\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:03.773746\n",
      "0:00:40.804596\n",
      "0:00:04.469971\n",
      "no massive missing\n",
      "0:02:23.079472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:26.869879\n",
      "0:00:40.090253\n",
      "20190509 unzip finished\n",
      "0:00:55.092674\n",
      "0:01:29.904602\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.900210\n",
      "0:00:38.145700\n",
      "0:00:03.980153\n",
      "no massive missing\n",
      "0:02:06.426172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190509"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.749151\n",
      "0:00:43.666275\n",
      "20190510 unzip finished\n",
      "0:01:04.903579\n",
      "0:01:36.539230\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:19.936118\n",
      "0:00:42.724252\n",
      "0:00:04.429182\n",
      "no massive missing\n",
      "0:02:27.649862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190510"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:17.625113\n",
      "0:00:36.667851\n",
      "20190513 unzip finished\n",
      "0:00:51.231684\n",
      "0:01:27.247639\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:45.514821\n",
      "0:00:40.771804\n",
      "0:00:03.953454\n",
      "no massive missing\n",
      "0:02:11.343120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190513"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.136376\n",
      "0:00:42.924923\n",
      "20190514 unzip finished\n",
      "0:00:57.487620\n",
      "0:01:34.295889\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:02.387810\n",
      "0:00:42.393539\n",
      "0:00:04.218906\n",
      "no massive missing\n",
      "0:02:10.154520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.918076\n",
      "0:00:39.746229\n",
      "20190515 unzip finished\n",
      "0:00:54.962221\n",
      "0:01:35.071519\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:24.811962\n",
      "0:00:40.555852\n",
      "0:00:03.958397\n",
      "no massive missing\n",
      "0:02:12.896857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:17.375018\n",
      "0:00:40.909000\n",
      "20190516 unzip finished\n",
      "0:00:57.376196\n",
      "0:01:34.399037\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:14.328069\n",
      "0:00:38.860188\n",
      "0:00:03.955805\n",
      "no massive missing\n",
      "0:02:08.313173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.404403\n",
      "0:00:46.585044\n",
      "20190517 unzip finished\n",
      "0:00:53.509952\n",
      "0:01:33.806753\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:33.900032\n",
      "0:00:40.376331\n",
      "0:00:04.053795\n",
      "no massive missing\n",
      "0:02:17.021799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190517"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:26.221307\n",
      "0:00:38.405468\n",
      "20190520 unzip finished\n",
      "0:00:53.034520\n",
      "0:01:27.787971\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:37.789233\n",
      "0:00:42.409810\n",
      "0:00:03.909095\n",
      "no massive missing\n",
      "0:02:09.233029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.009634\n",
      "0:00:38.979410\n",
      "20190521 unzip finished\n",
      "0:00:52.815266\n",
      "0:01:30.203402\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:59.957378\n",
      "0:00:40.243299\n",
      "0:00:04.039688\n",
      "no massive missing\n",
      "0:02:11.337044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190521"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:18.040730\n",
      "0:00:37.487121\n",
      "20190522 unzip finished\n",
      "0:00:51.263340\n",
      "0:01:25.982094\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:36.905613\n",
      "0:00:38.067247\n",
      "0:00:03.853521\n",
      "no massive missing\n",
      "0:02:04.554999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.937659\n",
      "0:00:39.311537\n",
      "20190523 unzip finished\n",
      "0:00:52.141165\n",
      "0:01:26.764293\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:28.700324\n",
      "0:00:36.164984\n",
      "0:00:03.640758\n",
      "no massive missing\n",
      "0:02:02.890228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.152301\n",
      "0:00:36.059137\n",
      "20190524 unzip finished\n",
      "0:00:51.106707\n",
      "0:01:22.773149\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:00.596296\n",
      "0:00:35.044173\n",
      "0:00:03.531098\n",
      "no massive missing\n",
      "0:01:57.707771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190524"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.137447\n",
      "0:00:00.117599\n",
      "20190525 unzip finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-288ef277b1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"StockID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mSH\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mSH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190404'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:57.707707\n",
      "0:00:40.057232\n",
      "20190527 unzip finished\n",
      "0:00:52.226121\n",
      "0:01:29.592609\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:30.418661\n",
      "0:00:40.780117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.193168\n",
      "no massive missing\n",
      "0:02:06.289264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190527"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:18.228015\n",
      "0:00:42.506151\n",
      "20190528 unzip finished\n",
      "0:00:52.794362\n",
      "0:01:30.427240\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:33.243365\n",
      "0:00:39.585516\n",
      "0:00:04.210347\n",
      "no massive missing\n",
      "0:02:10.630289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190528"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.154527\n",
      "0:00:37.091933\n",
      "20190529 unzip finished\n",
      "0:00:51.831947\n",
      "0:01:23.538522\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:07.137957\n",
      "0:00:37.229706\n",
      "0:00:03.922981\n",
      "no massive missing\n",
      "0:02:12.334137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.643000\n",
      "0:00:37.703799\n",
      "20190530 unzip finished\n",
      "0:00:54.427807\n",
      "0:01:37.571803\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:26.928351\n",
      "0:00:38.718840\n",
      "0:00:04.084201\n",
      "no massive missing\n",
      "0:02:04.398854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.335476\n",
      "0:00:38.569913\n",
      "20190531 unzip finished\n",
      "0:00:51.640409\n",
      "0:01:27.264068\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:21.194872\n",
      "0:00:38.049308\n",
      "0:00:03.810380\n",
      "no massive missing\n",
      "0:02:06.597370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190531"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.097591\n",
      "0:00:38.082572\n",
      "20190603 unzip finished\n",
      "0:00:57.750131\n",
      "0:01:31.312410\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:29.803799\n",
      "0:00:38.667579\n",
      "0:00:04.080691\n",
      "no massive missing\n",
      "0:02:11.344397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190603"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.841474\n",
      "0:00:36.032298\n",
      "20190604 unzip finished\n",
      "0:00:53.062491\n",
      "0:01:30.955574\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:09.313502\n",
      "0:00:38.581019\n",
      "0:00:03.986889\n",
      "no massive missing\n",
      "0:02:10.219364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190604"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:23.501268\n",
      "0:00:35.427915\n",
      "20190605 unzip finished\n",
      "0:00:50.069064\n",
      "0:01:26.201747\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:03.540718\n",
      "0:00:39.112931\n",
      "0:00:03.988150\n",
      "no massive missing\n",
      "0:02:05.457001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190605"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.943947\n",
      "0:00:37.388285\n",
      "20190606 unzip finished\n",
      "0:00:54.575160\n",
      "0:01:29.369265\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:20.915814\n",
      "0:00:36.765545\n",
      "0:00:03.881667\n",
      "no massive missing\n",
      "0:02:03.097931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.509070\n",
      "0:00:36.159091\n",
      "20190610 unzip finished\n",
      "0:00:53.576718\n",
      "0:01:25.090416\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:02.965350\n",
      "0:00:35.235128\n",
      "0:00:03.744307\n",
      "no massive missing\n",
      "0:01:58.258142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190610"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.721974\n",
      "0:00:40.708079\n",
      "20190611 unzip finished\n",
      "0:00:54.969812\n",
      "0:01:36.865543\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:30.487050\n",
      "0:00:40.807183\n",
      "0:00:04.020296\n",
      "no massive missing\n",
      "0:02:20.589129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.707534\n",
      "0:00:37.366344\n",
      "20190612 unzip finished\n",
      "0:00:54.203833\n",
      "0:01:28.300195\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:44.935086\n",
      "0:00:38.364219\n",
      "0:00:03.824379\n",
      "no massive missing\n",
      "0:02:09.534318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.006434\n",
      "0:00:38.345576\n",
      "20190613 unzip finished\n",
      "0:00:54.487204\n",
      "0:01:27.478859\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:53.770989\n",
      "0:00:37.975151\n",
      "0:00:03.987782\n",
      "no massive missing\n",
      "0:02:09.404406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.962180\n",
      "0:00:36.623950\n",
      "20190614 unzip finished\n",
      "0:00:53.660657\n",
      "0:01:28.692850\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:28.884164\n",
      "0:00:40.128028\n",
      "0:00:04.384057\n",
      "no massive missing\n",
      "0:02:10.068850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.305885\n",
      "0:00:35.895430\n",
      "20190617 unzip finished\n",
      "0:00:50.053240\n",
      "0:01:33.951729\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:50.010081\n",
      "0:00:38.698592\n",
      "0:00:03.850044\n",
      "no massive missing\n",
      "0:02:12.208972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190617"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.157722\n",
      "0:00:38.808990\n",
      "20190618 unzip finished\n",
      "0:00:49.046543\n",
      "0:01:22.462655\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:46.011613\n",
      "0:00:41.713761\n",
      "0:00:03.798736\n",
      "no massive missing\n",
      "0:02:18.744689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190618"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.074750\n",
      "0:00:37.358704\n",
      "20190619 unzip finished\n",
      "0:00:53.265743\n",
      "0:01:30.095176\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.372161\n",
      "0:00:37.844606\n",
      "0:00:03.895166\n",
      "no massive missing\n",
      "0:02:08.052347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.570024\n",
      "0:00:40.052684\n",
      "20190620 unzip finished\n",
      "0:00:57.366631\n",
      "0:01:39.057552\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:37.344368\n",
      "0:00:41.769304\n",
      "0:00:04.262652\n",
      "no massive missing\n",
      "0:02:18.563184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190620"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:18.499565\n",
      "0:00:42.661987\n",
      "20190621 unzip finished\n",
      "0:00:56.639511\n",
      "0:01:38.736698\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:46.470169\n",
      "0:00:42.574635\n",
      "0:00:04.299234\n",
      "no massive missing\n",
      "0:02:20.940815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190621"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.922408\n",
      "0:00:36.747188\n",
      "20190624 unzip finished\n",
      "0:00:54.059201\n",
      "0:01:33.739745\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:56.119095\n",
      "0:00:43.183356\n",
      "0:00:03.950233\n",
      "no massive missing\n",
      "0:02:10.032868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190624"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.501196\n",
      "0:00:38.560857\n",
      "20190625 unzip finished\n",
      "0:00:54.637425\n",
      "0:01:34.680275\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:10.391835\n",
      "0:00:38.797362\n",
      "0:00:03.947475\n",
      "no massive missing\n",
      "0:02:05.145521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.517256\n",
      "0:00:36.600202\n",
      "20190626 unzip finished\n",
      "0:00:50.603949\n",
      "0:01:26.775951\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:22.760621\n",
      "0:00:39.843593\n",
      "0:00:03.749294\n",
      "no massive missing\n",
      "0:02:04.978553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190626"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.512695\n",
      "0:00:49.398424\n",
      "20190627 unzip finished\n",
      "0:00:52.491301\n",
      "0:01:30.908893\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:47.667213\n",
      "0:00:41.780172\n",
      "0:00:03.977319\n",
      "no massive missing\n",
      "0:02:22.350460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.473925\n",
      "0:00:38.074658\n",
      "20190628 unzip finished\n",
      "0:00:53.413594\n",
      "0:01:31.272127\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:47.805144\n",
      "0:00:38.910947\n",
      "0:00:04.214161\n",
      "no massive missing\n",
      "0:02:07.574414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190628"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.721649\n",
      "0:00:39.663234\n",
      "20190701 unzip finished\n",
      "0:00:53.503300\n",
      "0:01:37.632233\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:34.446423\n",
      "0:00:39.989856\n",
      "0:00:04.552235\n",
      "no massive missing\n",
      "0:02:12.313325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.972329\n",
      "0:00:39.605303\n",
      "20190702 unzip finished\n",
      "0:00:57.017209\n",
      "0:01:34.297263\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:15.317211\n",
      "0:00:38.876562\n",
      "0:00:03.877523\n",
      "no massive missing\n",
      "0:02:09.981131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190702"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.242369\n",
      "0:00:38.939483\n",
      "20190703 unzip finished\n",
      "0:00:52.381113\n",
      "0:01:29.062519\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:48.002191\n",
      "0:00:40.274631\n",
      "0:00:03.833359\n",
      "no massive missing\n",
      "0:02:07.649032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.327417\n",
      "0:00:38.992159\n",
      "20190704 unzip finished\n",
      "0:00:54.993837\n",
      "0:01:29.681243\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:31.232711\n",
      "0:00:39.076676\n",
      "0:00:03.803325\n",
      "no massive missing\n",
      "0:02:01.114551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.267873\n",
      "0:00:35.464587\n",
      "20190705 unzip finished\n",
      "0:00:51.162454\n",
      "0:01:26.415601\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:22.452924\n",
      "0:00:40.063538\n",
      "0:00:03.885997\n",
      "no massive missing\n",
      "0:02:03.037607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190705"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.562759\n",
      "0:00:38.278079\n",
      "20190708 unzip finished\n",
      "0:00:52.312434\n",
      "0:01:34.555632\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:18.506702\n",
      "0:00:38.344236\n",
      "0:00:03.887581\n",
      "no massive missing\n",
      "0:02:10.011194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.831347\n",
      "0:00:37.550236\n",
      "20190709 unzip finished\n",
      "0:00:47.919964\n",
      "0:01:25.814156\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:05.661974\n",
      "0:00:36.907653\n",
      "0:00:03.686066\n",
      "no massive missing\n",
      "0:02:02.721785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190709"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.603693\n",
      "0:00:34.872591\n",
      "20190710 unzip finished\n",
      "0:00:50.456443\n",
      "0:01:23.859140\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:03.758661\n",
      "0:00:36.571576\n",
      "0:00:03.632409\n",
      "no massive missing\n",
      "0:02:05.483168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.130319\n",
      "0:00:35.322759\n",
      "20190711 unzip finished\n",
      "0:00:49.810398\n",
      "0:01:24.237519\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:58.773927\n",
      "0:00:35.732331\n",
      "0:00:03.529617\n",
      "no massive missing\n",
      "0:02:02.679515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.115453\n",
      "0:00:35.833458\n",
      "20190712 unzip finished\n",
      "0:00:48.247071\n",
      "0:01:19.726393\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:48.484155\n",
      "0:00:34.988268\n",
      "0:00:03.566217\n",
      "no massive missing\n",
      "0:01:57.434125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.587885\n",
      "0:00:36.925382\n",
      "20190715 unzip finished\n",
      "0:00:54.177291\n",
      "0:01:30.212445\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.368209\n",
      "0:00:38.410066\n",
      "0:00:03.917172\n",
      "no massive missing\n",
      "0:02:06.608606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.460414\n",
      "0:00:33.316893\n",
      "20190716 unzip finished\n",
      "0:00:48.477552\n",
      "0:01:22.559734\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:00.507283\n",
      "0:00:37.190148\n",
      "0:00:03.682803\n",
      "no massive missing\n",
      "0:02:01.207664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:51.277029\n",
      "0:00:32.237243\n",
      "20190717 unzip finished\n",
      "0:00:48.639909\n",
      "0:01:21.485010\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:19.501495\n",
      "0:00:35.702274\n",
      "0:00:03.567719\n",
      "no massive missing\n",
      "0:01:57.794499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190717"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:52.943721\n",
      "0:00:32.501964\n",
      "20190718 unzip finished\n",
      "0:00:46.283440\n",
      "0:01:21.341234\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:40.503282\n",
      "0:00:36.001704\n",
      "0:00:03.680943\n",
      "no massive missing\n",
      "0:01:57.533647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:51.494403\n",
      "0:00:33.183931\n",
      "20190719 unzip finished\n",
      "0:00:47.127647\n",
      "0:01:19.648384\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:41.050863\n",
      "0:00:35.705057\n",
      "0:00:03.521506\n",
      "no massive missing\n",
      "0:01:56.346883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.315490\n",
      "0:00:37.156409\n",
      "20190722 unzip finished\n",
      "0:00:49.451928\n",
      "0:01:28.434711\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:35.058675\n",
      "0:00:38.300624\n",
      "0:00:03.852053\n",
      "no massive missing\n",
      "0:02:06.343464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.953973\n",
      "0:00:39.627144\n",
      "20190723 unzip finished\n",
      "0:00:49.770131\n",
      "0:01:21.860942\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:13.731515\n",
      "0:00:34.826452\n",
      "0:00:03.856261\n",
      "no massive missing\n",
      "0:02:04.653809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190723"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:51.163105\n",
      "0:00:35.927437\n",
      "20190724 unzip finished\n",
      "0:00:48.840316\n",
      "0:01:25.435322\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:14.813127\n",
      "0:00:39.292170\n",
      "0:00:05.216854\n",
      "no massive missing\n",
      "0:02:09.300132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:52.860302\n",
      "0:00:34.969981\n",
      "20190725 unzip finished\n",
      "0:00:52.105497\n",
      "0:01:23.763405\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:53.449283\n",
      "0:00:37.183704\n",
      "0:00:03.817817\n",
      "no massive missing\n",
      "0:02:02.282106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190725"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.682499\n",
      "0:00:33.755065\n",
      "20190726 unzip finished\n",
      "0:00:49.571053\n",
      "0:01:18.963565\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:04.300544\n",
      "0:00:36.793684\n",
      "0:00:04.156223\n",
      "no massive missing\n",
      "0:01:57.580175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:50.539319\n",
      "0:00:39.332092\n",
      "20190729 unzip finished\n",
      "0:00:46.383277\n",
      "0:01:18.038651\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:49.044700\n",
      "0:00:35.681842\n",
      "0:00:03.774857\n",
      "no massive missing\n",
      "0:01:55.688474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190729"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.549415\n",
      "0:00:35.088717\n",
      "20190730 unzip finished\n",
      "0:00:52.389673\n",
      "0:01:24.292813\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.178485\n",
      "0:00:36.554946\n",
      "0:00:03.696514\n",
      "no massive missing\n",
      "0:02:00.448542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190730"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.805868\n",
      "0:00:34.892396\n",
      "20190731 unzip finished\n",
      "0:00:48.676031\n",
      "0:01:23.083003\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:30.175671\n",
      "0:00:39.192630\n",
      "0:00:04.221825\n",
      "no massive missing\n",
      "0:02:06.550497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.782774\n",
      "0:00:49.854370\n",
      "20190801 unzip finished\n",
      "0:00:47.979679\n",
      "0:01:24.564510\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:42.598012\n",
      "0:00:46.600399\n",
      "0:00:06.643676\n",
      "no massive missing\n",
      "0:02:50.647009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.267215\n",
      "0:00:47.691255\n",
      "20190802 unzip finished\n",
      "0:00:56.501778\n",
      "0:01:33.487264\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:08.115249\n",
      "0:01:00.572504\n",
      "0:00:09.427684\n",
      "no massive missing\n",
      "0:02:13.729338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.695762\n",
      "0:01:20.370335\n",
      "20190805 unzip finished\n",
      "0:00:51.187073\n",
      "0:01:29.092456\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190525'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/snapshot***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:46.758274\n",
      "0:01:14.296037\n",
      "20190805 unzip finished\n",
      "0:00:56.339416\n",
      "0:01:20.262936\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:21.550306\n",
      "0:00:35.507205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.686619\n",
      "no massive missing\n",
      "0:01:58.907230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.337919\n",
      "0:00:51.820813\n",
      "20190806 unzip finished\n",
      "0:00:55.062016\n",
      "0:01:25.750637\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:06.988244\n",
      "0:00:38.078616\n",
      "0:00:03.913382\n",
      "no massive missing\n",
      "0:02:08.116246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190806"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.357491\n",
      "0:00:47.934524\n",
      "20190807 unzip finished\n",
      "0:00:50.299404\n",
      "0:01:18.337294\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:49.839146\n",
      "0:00:41.113879\n",
      "0:00:03.716385\n",
      "no massive missing\n",
      "0:01:55.874718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.335457\n",
      "0:00:45.100028\n",
      "20190808 unzip finished\n",
      "0:00:49.257248\n",
      "0:01:24.985821\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:39.065421\n",
      "0:00:35.442772\n",
      "0:00:03.819243\n",
      "no massive missing\n",
      "0:01:55.693803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.307279\n",
      "0:00:46.520203\n",
      "20190809 unzip finished\n",
      "0:00:53.146831\n",
      "0:01:21.196005\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:30.256266\n",
      "0:00:37.975810\n",
      "0:00:03.787210\n",
      "no massive missing\n",
      "0:01:58.319972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.998462\n",
      "0:00:41.337375\n",
      "20190812 unzip finished\n",
      "0:00:51.369871\n",
      "0:01:19.050264\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:08.509631\n",
      "0:00:36.160649\n",
      "0:00:03.847450\n",
      "no massive missing\n",
      "0:01:59.326628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190812"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.577598\n",
      "0:01:15.045665\n",
      "20190813 unzip finished\n",
      "0:00:49.286361\n",
      "0:01:17.100247\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:03.416645\n",
      "0:00:35.554235\n",
      "0:00:03.558015\n",
      "no massive missing\n",
      "0:01:56.486508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190813"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.237230\n",
      "0:00:48.565296\n",
      "20190814 unzip finished\n",
      "0:00:52.651360\n",
      "0:01:22.078487\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:29.652518\n",
      "0:00:35.640401\n",
      "0:00:03.720213\n",
      "no massive missing\n",
      "0:01:59.923737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.955332\n",
      "0:00:48.296249\n",
      "20190815 unzip finished\n",
      "0:00:50.853348\n",
      "0:01:22.407037\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:05.885527\n",
      "0:00:36.898774\n",
      "0:00:03.703555\n",
      "no massive missing\n",
      "0:02:00.344635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190815"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.256223\n",
      "0:00:47.978453\n",
      "20190816 unzip finished\n",
      "0:00:50.066057\n",
      "0:01:25.561881\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:11.015906\n",
      "0:00:35.677617\n",
      "0:00:03.714006\n",
      "no massive missing\n",
      "0:01:57.525226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190816"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.971745\n",
      "0:00:44.437747\n",
      "20190819 unzip finished\n",
      "0:00:53.958013\n",
      "0:01:29.260708\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:02.799013\n",
      "0:00:39.601618\n",
      "0:00:04.081963\n",
      "no massive missing\n",
      "0:02:10.004192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.413927\n",
      "0:00:41.152386\n",
      "20190820 unzip finished\n",
      "0:00:53.255355\n",
      "0:01:25.799432\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:38.350910\n",
      "0:00:37.448263\n",
      "0:00:03.793706\n",
      "no massive missing\n",
      "0:02:04.810119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190820"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:16.489006\n",
      "0:00:40.722131\n",
      "20190821 unzip finished\n",
      "0:00:50.624838\n",
      "0:01:22.093699\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:24.625119\n",
      "0:00:37.229207\n",
      "0:00:03.718602\n",
      "no massive missing\n",
      "0:02:00.286341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190821"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.117334\n",
      "0:00:51.383472\n",
      "20190822 unzip finished\n",
      "0:00:48.904752\n",
      "0:01:22.062864\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:22.474535\n",
      "0:00:41.751147\n",
      "0:00:03.865709\n",
      "no massive missing\n",
      "0:02:03.942948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:14.909199\n",
      "0:00:41.040758\n",
      "20190823 unzip finished\n",
      "0:00:51.249316\n",
      "0:01:22.813088\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:15.595484\n",
      "0:00:35.503981\n",
      "0:00:03.568831\n",
      "no massive missing\n",
      "0:01:57.873319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190823"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.101372\n",
      "0:00:39.092577\n",
      "20190826 unzip finished\n",
      "0:00:53.300178\n",
      "0:01:22.941969\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:21.936151\n",
      "0:00:37.470332\n",
      "0:00:03.843514\n",
      "no massive missing\n",
      "0:02:03.841327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190826"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.348542\n",
      "0:00:44.576512\n",
      "20190827 unzip finished\n",
      "0:00:51.718042\n",
      "0:01:24.880284\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:00.699627\n",
      "0:00:38.293818\n",
      "0:00:03.863433\n",
      "no massive missing\n",
      "0:02:08.481447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190827"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.302806\n",
      "0:00:56.356795\n",
      "20190828 unzip finished\n",
      "0:00:52.723658\n",
      "0:01:24.417503\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:04.758366\n",
      "0:00:37.575679\n",
      "0:00:03.875258\n",
      "no massive missing\n",
      "0:02:03.817282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.244295\n",
      "0:00:48.771167\n",
      "20190829 unzip finished\n",
      "0:00:50.505405\n",
      "0:01:20.019752\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:43.419869\n",
      "0:00:37.319269\n",
      "0:00:03.841894\n",
      "no massive missing\n",
      "0:02:04.167687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.879918\n",
      "0:00:42.915725\n",
      "20190830 unzip finished\n",
      "0:00:54.587486\n",
      "0:01:24.354624\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:55.536056\n",
      "0:00:39.020689\n",
      "0:00:03.946374\n",
      "no massive missing\n",
      "0:02:09.685296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190830"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.613415\n",
      "0:00:38.800814\n",
      "20190902 unzip finished\n",
      "0:00:54.142285\n",
      "0:01:27.444018\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:57.751163\n",
      "0:00:38.587544\n",
      "0:00:03.836498\n",
      "no massive missing\n",
      "0:02:10.244209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:16.495277\n",
      "0:00:39.395880\n",
      "20190903 unzip finished\n",
      "0:00:52.428505\n",
      "0:01:26.430878\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:44.980914\n",
      "0:00:38.719477\n",
      "0:00:03.975979\n",
      "no massive missing\n",
      "0:02:05.779654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190903"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.355231\n",
      "0:00:40.190425\n",
      "20190904 unzip finished\n",
      "0:00:53.402399\n",
      "0:01:27.927130\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:19.188343\n",
      "0:00:42.914554\n",
      "0:00:04.101818\n",
      "no massive missing\n",
      "0:02:09.227507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190904"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.125254\n",
      "0:00:43.456820\n",
      "20190905 unzip finished\n",
      "0:00:58.659784\n",
      "0:01:31.299742\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:01.835410\n",
      "0:00:44.378016\n",
      "0:00:04.324667\n",
      "no massive missing\n",
      "0:02:20.282850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:28.567837\n",
      "0:00:37.799442\n",
      "20190906 unzip finished\n",
      "0:00:53.452131\n",
      "0:01:28.940842\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:08.094081\n",
      "0:00:40.648003\n",
      "0:00:04.079135\n",
      "no massive missing\n",
      "0:02:11.218811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.193195\n",
      "0:00:40.966516\n",
      "20190909 unzip finished\n",
      "0:00:57.295999\n",
      "0:01:31.870881\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:33.739850\n",
      "0:00:44.298625\n",
      "0:00:04.324772\n",
      "no massive missing\n",
      "0:02:18.709162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190909"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.957112\n",
      "0:00:42.397283\n",
      "20190910 unzip finished\n",
      "0:00:54.894907\n",
      "0:01:33.397750\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:10.164301\n",
      "0:00:41.047381\n",
      "0:00:05.666677\n",
      "no massive missing\n",
      "0:02:17.205591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190910"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.877785\n",
      "0:00:39.962433\n",
      "20190911 unzip finished\n",
      "0:00:53.984967\n",
      "0:01:28.326078\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:09.967960\n",
      "0:00:39.926934\n",
      "0:00:05.204962\n",
      "no massive missing\n",
      "0:02:15.647765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190911"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.215850\n",
      "0:00:37.956087\n",
      "20190912 unzip finished\n",
      "0:00:55.256476\n",
      "0:01:28.014334\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:57.909048\n",
      "0:00:38.591691\n",
      "0:00:04.053049\n",
      "no massive missing\n",
      "0:02:07.914651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.648285\n",
      "0:00:39.713779\n",
      "20190916 unzip finished\n",
      "0:00:54.583469\n",
      "0:01:29.353209\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:04.767801\n",
      "0:00:41.046359\n",
      "0:00:04.024162\n",
      "no massive missing\n",
      "0:02:11.999877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190916"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.287589\n",
      "0:00:39.640316\n",
      "20190917 unzip finished\n",
      "0:00:55.902879\n",
      "0:01:32.137480\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:23.642891\n",
      "0:00:43.310987\n",
      "0:00:04.199195\n",
      "no massive missing\n",
      "0:02:16.389143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190917"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.764523\n",
      "0:00:39.455514\n",
      "20190918 unzip finished\n",
      "0:00:51.762757\n",
      "0:01:25.302570\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:48.874249\n",
      "0:00:38.812174\n",
      "0:00:03.954155\n",
      "no massive missing\n",
      "0:02:07.363040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190918"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.024677\n",
      "0:00:37.179728\n",
      "20190919 unzip finished\n",
      "0:00:53.334964\n",
      "0:01:26.350975\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:33.823548\n",
      "0:00:38.765875\n",
      "0:00:03.973830\n",
      "no massive missing\n",
      "0:02:05.865037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190919"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.878992\n",
      "0:00:38.921180\n",
      "20190920 unzip finished\n",
      "0:00:53.552855\n",
      "0:01:26.791466\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:41.796256\n",
      "0:00:40.691279\n",
      "0:00:03.920336\n",
      "no massive missing\n",
      "0:02:08.983161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:02.600226\n",
      "0:00:40.132592\n",
      "20190923 unzip finished\n",
      "0:00:52.556420\n",
      "0:01:26.702747\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.028453\n",
      "0:00:39.090430\n",
      "0:00:03.902506\n",
      "no massive missing\n",
      "0:02:06.104961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.764674\n",
      "0:00:38.686221\n",
      "20190924 unzip finished\n",
      "0:00:52.609401\n",
      "0:01:27.850438\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:53.661011\n",
      "0:00:38.680352\n",
      "0:00:03.963418\n",
      "no massive missing\n",
      "0:02:13.740151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.955169\n",
      "0:00:38.858236\n",
      "20190925 unzip finished\n",
      "0:00:51.635855\n",
      "0:01:26.114708\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:44.178644\n",
      "0:00:41.506959\n",
      "0:00:04.493513\n",
      "no massive missing\n",
      "0:02:07.303484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.415019\n",
      "0:00:38.738834\n",
      "20190926 unzip finished\n",
      "0:00:53.915020\n",
      "0:01:28.888421\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:53.072041\n",
      "0:00:39.353894\n",
      "0:00:03.998290\n",
      "no massive missing\n",
      "0:02:14.008774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190926"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.511347\n",
      "0:00:35.143792\n",
      "20190927 unzip finished\n",
      "0:00:51.190833\n",
      "0:01:22.765846\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:12.473130\n",
      "0:00:37.123226\n",
      "0:00:03.748748\n",
      "no massive missing\n",
      "0:02:02.753814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.303246\n",
      "0:00:34.889001\n",
      "20190930 unzip finished\n",
      "0:00:47.121157\n",
      "0:01:18.762021\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:39.768924\n",
      "0:00:35.176026\n",
      "0:00:03.642637\n",
      "no massive missing\n",
      "0:01:54.105367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20190930"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:51.565236\n",
      "0:00:34.529061\n",
      "20191008 unzip finished\n",
      "0:00:49.272122\n",
      "0:01:18.688185\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:44.774960\n",
      "0:00:35.165115\n",
      "0:00:03.675727\n",
      "no massive missing\n",
      "0:01:57.777789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.823827\n",
      "0:00:34.293427\n",
      "20191009 unzip finished\n",
      "0:00:47.803818\n",
      "0:01:20.171781\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:51.010396\n",
      "0:00:34.884942\n",
      "0:00:03.635818\n",
      "no massive missing\n",
      "0:01:54.328464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:50.847240\n",
      "0:00:35.762872\n",
      "20191010 unzip finished\n",
      "0:00:49.325249\n",
      "0:01:20.394574\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:04.069316\n",
      "0:00:36.881598\n",
      "0:00:03.720026\n",
      "no massive missing\n",
      "0:02:04.159048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:04.404297\n",
      "0:00:36.704655\n",
      "20191011 unzip finished\n",
      "0:00:51.872459\n",
      "0:01:23.856669\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:22.147716\n",
      "0:00:36.840704\n",
      "0:00:03.779417\n",
      "no massive missing\n",
      "0:02:01.290829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.102102\n",
      "0:00:38.726818\n",
      "20191014 unzip finished\n",
      "0:00:50.999909\n",
      "0:01:23.467181\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:54.883610\n",
      "0:00:38.299719\n",
      "0:00:03.913269\n",
      "no massive missing\n",
      "0:02:13.511418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.479529\n",
      "0:00:38.020364\n",
      "20191015 unzip finished\n",
      "0:00:51.639656\n",
      "0:01:23.063392\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:13.555309\n",
      "0:00:37.509525\n",
      "0:00:03.853809\n",
      "no massive missing\n",
      "0:02:02.368458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.842125\n",
      "0:00:36.337371\n",
      "20191016 unzip finished\n",
      "0:00:50.813812\n",
      "0:01:21.666749\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:10.507754\n",
      "0:00:35.872465\n",
      "0:00:03.598331\n",
      "no massive missing\n",
      "0:02:02.393686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.319352\n",
      "0:00:34.737250\n",
      "20191017 unzip finished\n",
      "0:00:47.671653\n",
      "0:01:19.226091\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:07.037203\n",
      "0:00:37.796724\n",
      "0:00:03.701497\n",
      "no massive missing\n",
      "0:01:59.483569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:57.718914\n",
      "0:00:41.379911\n",
      "20191018 unzip finished\n",
      "0:01:18.971207\n",
      "0:01:22.808483\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:30.182457\n",
      "0:00:38.060044\n",
      "0:00:03.927026\n",
      "no massive missing\n",
      "0:02:03.091602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.110551\n",
      "0:00:34.376037\n",
      "20191021 unzip finished\n",
      "0:00:47.740634\n",
      "0:01:20.679010\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:00.215175\n",
      "0:00:35.841561\n",
      "0:00:03.664666\n",
      "no massive missing\n",
      "0:01:56.948309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.209374\n",
      "0:00:33.759430\n",
      "20191022 unzip finished\n",
      "0:00:48.345235\n",
      "0:01:18.415162\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:56.916033\n",
      "0:00:35.649406\n",
      "0:00:03.661391\n",
      "no massive missing\n",
      "0:01:56.537418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.101511\n",
      "0:00:34.481887\n",
      "20191023 unzip finished\n",
      "0:00:49.295574\n",
      "0:01:16.903648\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:53.365825\n",
      "0:00:36.152968\n",
      "0:00:03.610364\n",
      "no massive missing\n",
      "0:01:54.064844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:52.459724\n",
      "0:00:33.819851\n",
      "20191024 unzip finished\n",
      "0:00:50.162732\n",
      "0:01:20.673707\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:59.412441\n",
      "0:00:36.991371\n",
      "0:00:03.797731\n",
      "no massive missing\n",
      "0:02:04.179902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:52.348645\n",
      "0:00:35.800711\n",
      "20191025 unzip finished\n",
      "0:00:48.535981\n",
      "0:01:18.958193\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:18.937125\n",
      "0:00:37.045635\n",
      "0:00:03.817282\n",
      "no massive missing\n",
      "0:02:08.341326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.902375\n",
      "0:00:39.554581\n",
      "20191028 unzip finished\n",
      "0:00:53.788929\n",
      "0:01:25.460392\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:55.595040\n",
      "0:00:39.470901\n",
      "0:00:04.038694\n",
      "no massive missing\n",
      "0:02:06.801626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.944420\n",
      "0:00:40.443395\n",
      "20191029 unzip finished\n",
      "0:00:54.766673\n",
      "0:01:25.018229\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:53.020654\n",
      "0:00:39.371192\n",
      "0:00:03.991807\n",
      "no massive missing\n",
      "0:02:09.408559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.160870\n",
      "0:00:35.676310\n",
      "20191030 unzip finished\n",
      "0:00:50.012638\n",
      "0:01:22.402914\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:39.229129\n",
      "0:00:38.442004\n",
      "0:00:03.897255\n",
      "no massive missing\n",
      "0:02:06.565458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191030"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.518727\n",
      "0:00:37.721108\n",
      "20191031 unzip finished\n",
      "0:00:53.080571\n",
      "0:01:22.950801\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:42.807934\n",
      "0:00:38.705833\n",
      "0:00:03.883379\n",
      "no massive missing\n",
      "0:02:06.383392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.921356\n",
      "0:00:38.024523\n",
      "20191101 unzip finished\n",
      "0:00:50.384757\n",
      "0:01:22.186701\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:36.611923\n",
      "0:00:39.359511\n",
      "0:00:03.932377\n",
      "no massive missing\n",
      "0:02:08.233767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.284526\n",
      "0:00:35.255320\n",
      "20191104 unzip finished\n",
      "0:00:53.085948\n",
      "0:01:20.111100\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:19.040739\n",
      "0:00:36.785202\n",
      "0:00:03.704835\n",
      "no massive missing\n",
      "0:02:00.465948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191104"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:54.612709\n",
      "0:00:37.834455\n",
      "20191105 unzip finished\n",
      "0:00:52.269464\n",
      "0:01:21.985394\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:37.985305\n",
      "0:00:38.269553\n",
      "0:00:03.865474\n",
      "no massive missing\n",
      "0:02:07.740772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191105"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.648940\n",
      "0:00:37.304611\n",
      "20191106 unzip finished\n",
      "0:00:51.030922\n",
      "0:01:20.556287\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:27.987935\n",
      "0:00:39.051414\n",
      "0:00:04.181219\n",
      "no massive missing\n",
      "0:02:19.458218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:55.162723\n",
      "0:00:35.142425\n",
      "20191107 unzip finished\n",
      "0:00:48.880124\n",
      "0:01:26.618395\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:23.249587\n",
      "0:00:38.011570\n",
      "0:00:04.147578\n",
      "no massive missing\n",
      "0:02:08.398069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.820240\n",
      "0:00:35.176376\n",
      "20191108 unzip finished\n",
      "0:00:49.770634\n",
      "0:01:27.980760\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:31.377794\n",
      "0:00:38.039071\n",
      "0:00:03.974718\n",
      "no massive missing\n",
      "0:02:07.889088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.762082\n",
      "0:00:39.987239\n",
      "20191111 unzip finished\n",
      "0:00:53.848393\n",
      "0:01:29.206514\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:26.834465\n",
      "0:00:39.711276\n",
      "0:00:04.283562\n",
      "no massive missing\n",
      "0:02:07.279362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.331804\n",
      "0:00:34.530200\n",
      "20191112 unzip finished\n",
      "0:00:48.607548\n",
      "0:01:17.469969\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:23.895621\n",
      "0:00:41.040888\n",
      "0:00:03.793321\n",
      "no massive missing\n",
      "0:02:04.772628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191112"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.119363\n",
      "0:00:43.080094\n",
      "20191113 unzip finished\n",
      "0:00:48.437358\n",
      "0:01:14.646319\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:02.786892\n",
      "0:00:36.527875\n",
      "0:00:03.979899\n",
      "no massive missing\n",
      "0:01:59.925121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.286328\n",
      "0:00:42.057243\n",
      "20191114 unzip finished\n",
      "0:00:50.139956\n",
      "0:01:14.107956\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:17.055269\n",
      "0:00:35.887001\n",
      "0:00:03.899938\n",
      "no massive missing\n",
      "0:01:59.490770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.807578\n",
      "0:00:35.073944\n",
      "20191115 unzip finished\n",
      "0:00:49.624790\n",
      "0:01:16.026545\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:18.806935\n",
      "0:00:37.406469\n",
      "0:00:03.945573\n",
      "no massive missing\n",
      "0:02:13.455838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:58.123226\n",
      "0:00:37.034920\n",
      "20191118 unzip finished\n",
      "0:00:48.770058\n",
      "0:01:17.136086\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:06.557217\n",
      "0:00:38.037355\n",
      "0:00:03.925538\n",
      "no massive missing\n",
      "0:02:19.277005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.966658\n",
      "0:00:37.875306\n",
      "20191119 unzip finished\n",
      "0:00:51.966899\n",
      "0:01:18.512156\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:10:57.629050\n",
      "0:00:40.286188\n",
      "0:00:04.037700\n",
      "no massive missing\n",
      "0:02:04.301680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:06.844777\n",
      "0:00:37.365523\n",
      "20191120 unzip finished\n",
      "0:00:54.054275\n",
      "0:01:20.211576\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:22.305222\n",
      "0:00:43.293525\n",
      "0:00:03.953741\n",
      "no massive missing\n",
      "0:02:08.920299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.682707\n",
      "0:00:39.745994\n",
      "20191121 unzip finished\n",
      "0:00:53.991151\n",
      "0:01:22.264635\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:23.526644\n",
      "0:00:39.476045\n",
      "0:00:04.071005\n",
      "no massive missing\n",
      "0:02:07.789334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.959833\n",
      "0:00:45.073495\n",
      "20191122 unzip finished\n",
      "0:00:56.791622\n",
      "0:01:32.845247\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:59.201195\n",
      "0:00:43.202732\n",
      "0:00:04.286893\n",
      "no massive missing\n",
      "0:02:24.525685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.206472\n",
      "0:00:47.211611\n",
      "20191125 unzip finished\n",
      "0:00:59.073434\n",
      "0:01:30.034847\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:39.620056\n",
      "0:00:40.034378\n",
      "0:00:04.254904\n",
      "no massive missing\n",
      "0:02:09.664333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:20.739633\n",
      "0:00:41.166475\n",
      "20191126 unzip finished\n",
      "0:00:55.900452\n",
      "0:01:25.762071\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:21.293378\n",
      "0:00:40.183042\n",
      "0:00:04.174751\n",
      "no massive missing\n",
      "0:02:07.983150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191126"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:00.972707\n",
      "0:00:40.305433\n",
      "20191127 unzip finished\n",
      "0:00:58.958846\n",
      "0:01:26.730931\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:28.043994\n",
      "0:00:39.330019\n",
      "0:00:05.198833\n",
      "no massive missing\n",
      "0:02:08.960240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:03.342078\n",
      "0:00:39.943697\n",
      "20191128 unzip finished\n",
      "0:00:55.828898\n",
      "0:01:26.267306\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:12.264906\n",
      "0:00:38.492192\n",
      "0:00:03.825416\n",
      "no massive missing\n",
      "0:02:03.663906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:56.588313\n",
      "0:01:09.102856\n",
      "20191129 unzip finished\n",
      "0:00:55.880762\n",
      "0:01:25.606688\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:19.614817\n",
      "0:00:41.571006\n",
      "0:00:04.066216\n",
      "no massive missing\n",
      "0:02:10.318840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:59.877879\n",
      "0:00:59.926185\n",
      "20191202 unzip finished\n",
      "0:00:59.900525\n",
      "0:01:30.197061\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:59.672915\n",
      "0:00:41.439596\n",
      "0:00:04.096592\n",
      "no massive missing\n",
      "0:02:13.433692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.731563\n",
      "0:00:43.710694\n",
      "20191203 unzip finished\n",
      "0:01:02.469286\n",
      "0:01:35.325594\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:28.801293\n",
      "0:00:40.908491\n",
      "0:00:04.255551\n",
      "no massive missing\n",
      "0:02:26.080866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:01.906371\n",
      "0:01:19.663077\n",
      "20191204 unzip finished\n",
      "0:00:59.286046\n",
      "0:01:36.082246\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:10.601794\n",
      "0:00:42.780581\n",
      "0:00:04.099028\n",
      "no massive missing\n",
      "0:02:16.318165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:20.468362\n",
      "0:01:07.488510\n",
      "20191205 unzip finished\n",
      "0:00:59.986643\n",
      "0:01:35.019696\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:38.228832\n",
      "0:00:53.291720\n",
      "0:00:04.094598\n",
      "no massive missing\n",
      "0:02:21.921988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.602496\n",
      "0:00:44.740807\n",
      "20191206 unzip finished\n",
      "0:01:00.155868\n",
      "0:01:35.257597\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:25.426390\n",
      "0:00:46.644557\n",
      "0:00:04.448186\n",
      "no massive missing\n",
      "0:02:22.009967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:28.663993\n",
      "0:00:53.890629\n",
      "20191209 unzip finished\n",
      "0:01:02.805766\n",
      "0:01:46.292655\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:54.348665\n",
      "0:00:42.803966\n",
      "0:00:04.370124\n",
      "no massive missing\n",
      "0:02:29.821624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191209"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:26.144773\n",
      "0:01:07.349269\n",
      "20191210 unzip finished\n",
      "0:01:02.814865\n",
      "0:01:33.802515\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:04.474517\n",
      "0:00:48.570611\n",
      "0:00:04.713623\n",
      "no massive missing\n",
      "0:02:25.701951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191210"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:21.079342\n",
      "0:06:37.908069\n",
      "20191211 unzip finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ddb283b1e4a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"StockID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mSH\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mSH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190805'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/snapshot***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:02.894416\n",
      "0:00:00.449359\n",
      "20191211 unzip finished\n",
      "0:02:46.652392\n",
      "0:01:34.334777\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:12.577035\n",
      "0:00:42.653749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:04.155281\n",
      "no massive missing\n",
      "0:02:22.345053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:08.401486\n",
      "0:00:00.712164\n",
      "20191212 unzip finished\n",
      "0:02:03.652652\n",
      "0:01:35.222243\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:20.945182\n",
      "0:00:45.309722\n",
      "0:00:04.022625\n",
      "no massive missing\n",
      "0:02:17.207886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:14.354238\n",
      "0:00:00.629685\n",
      "20191213 unzip finished\n",
      "0:04:08.458863\n",
      "0:01:38.801259\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:08.789862\n",
      "0:00:44.417304\n",
      "0:00:04.624116\n",
      "no massive missing\n",
      "0:02:33.946027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:21.845709\n",
      "0:00:00.574479\n",
      "20191216 unzip finished\n",
      "0:02:49.655229\n",
      "0:01:42.530140\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:04.490019\n",
      "0:00:45.983097\n",
      "0:00:04.928800\n",
      "no massive missing\n",
      "0:02:33.929906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:34.610243\n",
      "0:00:01.767299\n",
      "20191217 unzip finished\n",
      "0:04:15.853336\n",
      "0:01:49.843461\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:43.686261\n",
      "0:00:49.479845\n",
      "0:00:05.167028\n",
      "no massive missing\n",
      "0:02:47.408400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.334777\n",
      "0:00:54.957307\n",
      "20191218 unzip finished\n",
      "0:01:01.211602\n",
      "0:01:41.574053\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:25.200194\n",
      "0:00:47.491886\n",
      "0:00:04.722066\n",
      "no massive missing\n",
      "0:02:34.311300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.681440\n",
      "0:00:48.360453\n",
      "20191219 unzip finished\n",
      "0:01:12.898225\n",
      "0:01:44.176444\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:12.950755\n",
      "0:00:46.996816\n",
      "0:00:05.279675\n",
      "no massive missing\n",
      "0:02:41.190433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.521391\n",
      "0:00:45.578808\n",
      "20191220 unzip finished\n",
      "0:01:01.770232\n",
      "0:01:43.805872\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:29.754896\n",
      "0:00:52.438203\n",
      "0:00:04.427702\n",
      "no massive missing\n",
      "0:02:34.285372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:18.840354\n",
      "0:00:46.546763\n",
      "20191223 unzip finished\n",
      "0:01:06.320526\n",
      "0:01:54.038790\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:28.553618\n",
      "0:00:47.814887\n",
      "0:00:04.645779\n",
      "no massive missing\n",
      "0:02:37.829126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:14.535939\n",
      "0:00:46.409716\n",
      "20191224 unzip finished\n",
      "0:01:04.388511\n",
      "0:01:38.035442\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:41.893405\n",
      "0:00:55.268608\n",
      "0:00:04.588251\n",
      "no massive missing\n",
      "0:02:32.322649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.083234\n",
      "0:00:43.537187\n",
      "20191225 unzip finished\n",
      "0:00:55.928387\n",
      "0:01:34.599809\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:42.226689\n",
      "0:00:45.306221\n",
      "0:00:04.615397\n",
      "no massive missing\n",
      "0:02:32.938271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.203224\n",
      "0:00:44.656102\n",
      "20191226 unzip finished\n",
      "0:01:05.695836\n",
      "0:01:45.961467\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:12:57.829234\n",
      "0:00:46.803221\n",
      "0:00:04.354820\n",
      "no massive missing\n",
      "0:02:32.030972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191226"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.841860\n",
      "0:00:48.235913\n",
      "20191227 unzip finished\n",
      "0:01:05.434638\n",
      "0:01:43.615647\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:41.440981\n",
      "0:00:45.884654\n",
      "0:00:04.684739\n",
      "no massive missing\n",
      "0:02:35.304356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:23.179043\n",
      "0:00:52.184113\n",
      "20191230 unzip finished\n",
      "0:01:07.842161\n",
      "0:01:47.241564\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:04.198294\n",
      "0:00:51.643452\n",
      "0:00:04.503669\n",
      "no massive missing\n",
      "0:02:41.570663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.149651\n",
      "0:00:46.975146\n",
      "20191231 unzip finished\n",
      "0:01:02.070020\n",
      "0:01:43.842126\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:36.903536\n",
      "0:00:45.901378\n",
      "0:00:04.424820\n",
      "no massive missing\n",
      "0:02:41.579730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20191231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.927147\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6941418fd737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m \u001b[0mwr_ong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0mmi_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_ss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20191211'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/snapshot***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:06.483193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:247: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-30\n",
      "trade finished\n",
      "0:07:23.171362\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "\n",
    "dataPathLs = np.array(glob.glob('/mnt/ShareWithServer/data/20170630/tick/***'))\n",
    "dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "TradeLog = []\n",
    "ll = []\n",
    "\n",
    "for i in dataPathLs:\n",
    "    try:\n",
    "        df = pd.read_csv(i)\n",
    "    except:\n",
    "        print(\"empty data\")\n",
    "        print(i)\n",
    "        ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "        continue\n",
    "    df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "    TradeLog += [df]\n",
    "TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "\n",
    "TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                    \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                   \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                   \"BuyNo\":\"BidApplSeqNum\"})\n",
    "TradeLog[\"trade_type\"] = 1\n",
    "TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "    TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "    TradeLog[col] = TradeLog[col].astype('int32')\n",
    "\n",
    "da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "db1 = db[db[\"date\"] == da_te]\n",
    "db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "del db1\n",
    "t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "try:\n",
    "    assert(t1.shape[0] == t2.shape[0])\n",
    "    assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "    assert(re[re[\"cum_amount\"].round(2) != re[\"max_amount\"]].shape[0] == 0)\n",
    "except:\n",
    "    display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "    display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "    display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "del t1\n",
    "del t2\n",
    "del re\n",
    "\n",
    "TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                             \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "print(da_te)\n",
    "print(\"trade finished\")\n",
    "\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db1.write('md_trade', TradeLog)\n",
    "\n",
    "print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:11.121475\n",
      "0:00:21.032066\n",
      "20200506 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:278: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-06\n",
      "trade finished\n",
      "0:05:16.937514\n",
      "0:00:17.451250\n",
      "20200507 unzip finished\n",
      "2020-05-07\n",
      "trade finished\n",
      "0:05:03.534487\n",
      "0:00:17.931552\n",
      "20200508 unzip finished\n",
      "2020-05-08\n",
      "trade finished\n",
      "0:05:13.983078\n",
      "0:00:17.649890\n",
      "20200511 unzip finished\n",
      "2020-05-11\n",
      "trade finished\n",
      "0:05:11.435855\n",
      "0:00:16.552835\n",
      "20200512 unzip finished\n",
      "2020-05-12\n",
      "trade finished\n",
      "0:04:52.961254\n",
      "0:00:16.559856\n",
      "20200513 unzip finished\n",
      "2020-05-13\n",
      "trade finished\n",
      "0:04:26.539266\n",
      "0:00:15.773796\n",
      "20200514 unzip finished\n",
      "2020-05-14\n",
      "trade finished\n",
      "0:04:40.319277\n",
      "0:00:16.288275\n",
      "20200515 unzip finished\n",
      "2020-05-15\n",
      "trade finished\n",
      "0:04:34.987253\n",
      "0:00:20.273271\n",
      "20200518 unzip finished\n",
      "2020-05-18\n",
      "trade finished\n",
      "0:05:13.331732\n",
      "0:00:15.993500\n",
      "20200519 unzip finished\n",
      "2020-05-19\n",
      "trade finished\n",
      "0:04:30.518270\n",
      "0:00:17.351618\n",
      "20200520 unzip finished\n",
      "2020-05-20\n",
      "trade finished\n",
      "0:05:10.707828\n",
      "0:00:18.251177\n",
      "20200521 unzip finished\n",
      "2020-05-21\n",
      "trade finished\n",
      "0:04:58.702026\n",
      "0:00:21.850634\n",
      "20200522 unzip finished\n",
      "2020-05-22\n",
      "trade finished\n",
      "0:04:49.803463\n",
      "0:00:16.474634\n",
      "20200525 unzip finished\n",
      "2020-05-25\n",
      "trade finished\n",
      "0:04:24.279184\n",
      "0:00:16.320616\n",
      "20200526 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>SH603789</td>\n",
       "      <td>2233216.0</td>\n",
       "      <td>29173185.0</td>\n",
       "      <td>1603789</td>\n",
       "      <td>6373</td>\n",
       "      <td>7319569.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  max_volume  max_amount     skey  cum_volume  cum_amount\n",
       "1381  SH603789   2233216.0  29173185.0  1603789        6373   7319569.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>SH603789</td>\n",
       "      <td>2233216.0</td>\n",
       "      <td>29173185.0</td>\n",
       "      <td>1603789</td>\n",
       "      <td>6373</td>\n",
       "      <td>7319569.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  max_volume  max_amount     skey  cum_volume  cum_amount\n",
       "1381  SH603789   2233216.0  29173185.0  1603789        6373   7319569.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-26\n",
      "trade finished\n",
      "0:04:15.712656\n",
      "0:00:17.435995\n",
      "20200527 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:278: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-27\n",
      "trade finished\n",
      "0:04:27.080255\n",
      "0:00:15.837193\n",
      "20200528 unzip finished\n",
      "2020-05-28\n",
      "trade finished\n",
      "0:04:39.396191\n",
      "0:00:15.150494\n",
      "20200529 unzip finished\n",
      "2020-05-29\n",
      "trade finished\n",
      "0:04:16.560306\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200506'\n",
    "endDate = '20200529'\n",
    "readPath = '/home/work516/KR_upload_code/SH/202005_tick/202005/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/tick.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/tick/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    TradeLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i)\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        TradeLog += [df]\n",
    "    TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "    \n",
    "    TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "    TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                        \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                       \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                       \"BuyNo\":\"BidApplSeqNum\"})\n",
    "    TradeLog[\"trade_type\"] = 1\n",
    "    TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "    TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "        TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "        TradeLog[col] = TradeLog[col].astype('int32')\n",
    "    \n",
    "    da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "    db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "    t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "    del db1\n",
    "    t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "    trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "    trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "    trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "    t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "    re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "    re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "    try:\n",
    "        assert(t1.shape[0] == t2.shape[0])\n",
    "        assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "        assert(re[re[\"cum_amount\"].round(2) != re[\"max_amount\"]].shape[0] == 0)\n",
    "    except:\n",
    "        display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "        display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "        display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "    del t1\n",
    "    del t2\n",
    "    del re\n",
    " \n",
    "    TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                                 \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "    print(da_te)\n",
    "    print(\"trade finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_trade', TradeLog)\n",
    "    \n",
    "    del TradeLog\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1601778, 1603950, 1605001, 1688466, 1688516, 1688566, 1688588, 1688598}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>SH601778</td>\n",
       "      <td>291455558.0</td>\n",
       "      <td>2.562939e+09</td>\n",
       "      <td>1601778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>SH603950</td>\n",
       "      <td>303933.0</td>\n",
       "      <td>6.920554e+06</td>\n",
       "      <td>1603950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>SH605001</td>\n",
       "      <td>38565131.0</td>\n",
       "      <td>1.026371e+09</td>\n",
       "      <td>1605001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>SH688466</td>\n",
       "      <td>2049329.0</td>\n",
       "      <td>7.870924e+07</td>\n",
       "      <td>1688466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>SH688516</td>\n",
       "      <td>10408688.0</td>\n",
       "      <td>6.201066e+08</td>\n",
       "      <td>1688516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>SH688566</td>\n",
       "      <td>7812428.0</td>\n",
       "      <td>3.447628e+08</td>\n",
       "      <td>1688566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>SH688588</td>\n",
       "      <td>6477003.0</td>\n",
       "      <td>2.630149e+08</td>\n",
       "      <td>1688588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>SH688598</td>\n",
       "      <td>3531236.0</td>\n",
       "      <td>2.802792e+08</td>\n",
       "      <td>1688598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   max_volume    max_amount     skey  cum_volume  cum_amount\n",
       "939   SH601778  291455558.0  2.562939e+09  1601778         NaN         NaN\n",
       "1467  SH603950     303933.0  6.920554e+06  1603950         NaN         NaN\n",
       "1500  SH605001   38565131.0  1.026371e+09  1605001         NaN         NaN\n",
       "1601  SH688466    2049329.0  7.870924e+07  1688466         NaN         NaN\n",
       "1602  SH688516   10408688.0  6.201066e+08  1688516         NaN         NaN\n",
       "1603  SH688566    7812428.0  3.447628e+08  1688566         NaN         NaN\n",
       "1604  SH688588    6477003.0  2.630149e+08  1688588         NaN         NaN\n",
       "1605  SH688598    3531236.0  2.802792e+08  1688598         NaN         NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>SH601778</td>\n",
       "      <td>291455558.0</td>\n",
       "      <td>2.562939e+09</td>\n",
       "      <td>1601778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>SH603950</td>\n",
       "      <td>303933.0</td>\n",
       "      <td>6.920554e+06</td>\n",
       "      <td>1603950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>SH605001</td>\n",
       "      <td>38565131.0</td>\n",
       "      <td>1.026371e+09</td>\n",
       "      <td>1605001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>SH688466</td>\n",
       "      <td>2049329.0</td>\n",
       "      <td>7.870924e+07</td>\n",
       "      <td>1688466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>SH688516</td>\n",
       "      <td>10408688.0</td>\n",
       "      <td>6.201066e+08</td>\n",
       "      <td>1688516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>SH688566</td>\n",
       "      <td>7812428.0</td>\n",
       "      <td>3.447628e+08</td>\n",
       "      <td>1688566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>SH688588</td>\n",
       "      <td>6477003.0</td>\n",
       "      <td>2.630149e+08</td>\n",
       "      <td>1688588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>SH688598</td>\n",
       "      <td>3531236.0</td>\n",
       "      <td>2.802792e+08</td>\n",
       "      <td>1688598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   max_volume    max_amount     skey  cum_volume  cum_amount\n",
       "939   SH601778  291455558.0  2.562939e+09  1601778         NaN         NaN\n",
       "1467  SH603950     303933.0  6.920554e+06  1603950         NaN         NaN\n",
       "1500  SH605001   38565131.0  1.026371e+09  1605001         NaN         NaN\n",
       "1601  SH688466    2049329.0  7.870924e+07  1688466         NaN         NaN\n",
       "1602  SH688516   10408688.0  6.201066e+08  1688516         NaN         NaN\n",
       "1603  SH688566    7812428.0  3.447628e+08  1688566         NaN         NaN\n",
       "1604  SH688588    6477003.0  2.630149e+08  1688588         NaN         NaN\n",
       "1605  SH688598    3531236.0  2.802792e+08  1688598         NaN         NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-26\n",
      "trade finished\n",
      "0:00:00.486368\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "\n",
    "\n",
    "# startTm = datetime.datetime.now()\n",
    "# readPath = '/home/work516/day_stock/***'\n",
    "# dataPathLs = np.array(glob.glob(readPath))\n",
    "# dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "# db = pd.DataFrame()\n",
    "# for p in dataPathLs:\n",
    "#     dayData = pd.read_csv(p, compression='gzip')\n",
    "#     db = pd.concat([db, dayData])\n",
    "# print(datetime.datetime.now() - startTm)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "\n",
    "startDate = '20200526'\n",
    "endDate = '20200526'\n",
    "df = []\n",
    "bad = []\n",
    "readPath = '/mnt/Kevin_zhenyu/rawData/logs_***_zs_92_01_day_data'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[1] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "for data in dataPathLs:\n",
    "    readPath = data + '/mdTradeLog***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    SH = pd.read_csv(dataPathLs[0], encoding=\"utf-8\").loc[:, [\"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"ExecType\", \"TradeBSFlag\",\n",
    "                                                 \"TradePrice\", \"TradeQty\", \"TradeMoney\", \"BidApplSeqNum\",\n",
    "                                                 \"OfferApplSeqNum\"]]\n",
    "    SH = SH[(SH[\"SecurityID\"] >= 600000) & (SH[\"SecurityID\"] < 700000)]             \n",
    "    SH = SH.rename(columns={\"TradeBSFlag\":\"trade_flag\", \"TradeMoney\":\"trade_money\", \"TradePrice\":\"trade_price\",\n",
    "                                             \"TradeQty\":'trade_qty', \"ExecType\":\"trade_type\"})\n",
    "    SH['date'] = int(os.path.basename(dataPathLs[0]).split('_')[1])\n",
    "    SH[\"skey\"] = SH[\"SecurityID\"] + 1000000\n",
    "    SH[\"time\"] = SH['TransactTime'].astype(np.int64)*1000\n",
    "    SH['TransactTime'] = SH['TransactTime'] + SH['date'] * 1000000000\n",
    "    SH[\"clockAtArrival\"] = SH[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    SH[\"trade_type\"] = np.where(SH[\"trade_type\"] == 'F', 1, SH[\"trade_type\"])\n",
    "    SH['trade_flag'] = np.where(SH[\"trade_flag\"] == 'B', 1, np.where(\n",
    "        SH[\"trade_flag\"] == 'S', 2, 0))\n",
    "\n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    for cols in [\"trade_money\", 'trade_price']:\n",
    "        SH[cols] = SH[cols]/10000\n",
    "    display(SH[\"trade_price\"].astype(str).apply(lambda x: len(x.split('.')[1])).unique())\n",
    "    display(SH[\"trade_money\"].astype(str).apply(lambda x: len(x.split('.')[1])).unique())\n",
    "\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 1000000).unique()\n",
    "    db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "    db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "    t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "    del db1\n",
    "    t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    trade1 = SH[SH[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "    trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "    trade2 = SH[SH[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "    trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "    t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "    re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "    re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "    try:\n",
    "        assert(t1.shape[0] == t2.shape[0])\n",
    "        assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "        assert(re[re[\"cum_amount\"] != re[\"max_amount\"]].shape[0] == 0)\n",
    "    except:\n",
    "        display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "        display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "        display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "    del t1\n",
    "    del t2\n",
    "    del re\n",
    "\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                                 \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "    print(da_te)\n",
    "    print(\"trade finished\")\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skey</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "      <th>ApplSeqNum</th>\n",
       "      <th>trade_type</th>\n",
       "      <th>trade_flag</th>\n",
       "      <th>trade_price</th>\n",
       "      <th>trade_qty</th>\n",
       "      <th>BidApplSeqNum</th>\n",
       "      <th>OfferApplSeqNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>92500110000</td>\n",
       "      <td>1588728300110000</td>\n",
       "      <td>2020-05-06 09:25:00.110</td>\n",
       "      <td>23238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.05</td>\n",
       "      <td>100</td>\n",
       "      <td>85856</td>\n",
       "      <td>27284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>92500110000</td>\n",
       "      <td>1588728300110000</td>\n",
       "      <td>2020-05-06 09:25:00.110</td>\n",
       "      <td>23239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.05</td>\n",
       "      <td>1400</td>\n",
       "      <td>85856</td>\n",
       "      <td>145070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>92500110000</td>\n",
       "      <td>1588728300110000</td>\n",
       "      <td>2020-05-06 09:25:00.110</td>\n",
       "      <td>23240</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.05</td>\n",
       "      <td>100</td>\n",
       "      <td>85856</td>\n",
       "      <td>57977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>92500110000</td>\n",
       "      <td>1588728300110000</td>\n",
       "      <td>2020-05-06 09:25:00.110</td>\n",
       "      <td>23241</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.05</td>\n",
       "      <td>4200</td>\n",
       "      <td>85856</td>\n",
       "      <td>145087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>92500110000</td>\n",
       "      <td>1588728300110000</td>\n",
       "      <td>2020-05-06 09:25:00.110</td>\n",
       "      <td>23242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.05</td>\n",
       "      <td>3000</td>\n",
       "      <td>85856</td>\n",
       "      <td>146303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>150000610000</td>\n",
       "      <td>1588748400610000</td>\n",
       "      <td>2020-05-06 15:00:00.610</td>\n",
       "      <td>2970282</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>700</td>\n",
       "      <td>4795252</td>\n",
       "      <td>4784595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>150000610000</td>\n",
       "      <td>1588748400610000</td>\n",
       "      <td>2020-05-06 15:00:00.610</td>\n",
       "      <td>2970283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2000</td>\n",
       "      <td>4798999</td>\n",
       "      <td>4784595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>150000610000</td>\n",
       "      <td>1588748400610000</td>\n",
       "      <td>2020-05-06 15:00:00.610</td>\n",
       "      <td>2970284</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>1500</td>\n",
       "      <td>4800464</td>\n",
       "      <td>4784595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>150000610000</td>\n",
       "      <td>1588748400610000</td>\n",
       "      <td>2020-05-06 15:00:00.610</td>\n",
       "      <td>2970285</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>800</td>\n",
       "      <td>4800710</td>\n",
       "      <td>4784595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>1603789</td>\n",
       "      <td>20200506</td>\n",
       "      <td>150000610000</td>\n",
       "      <td>1588748400610000</td>\n",
       "      <td>2020-05-06 15:00:00.610</td>\n",
       "      <td>2970286</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>1300</td>\n",
       "      <td>4800710</td>\n",
       "      <td>4793870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2238 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         skey      date          time    clockAtArrival  \\\n",
       "0     1603789  20200506   92500110000  1588728300110000   \n",
       "1     1603789  20200506   92500110000  1588728300110000   \n",
       "2     1603789  20200506   92500110000  1588728300110000   \n",
       "3     1603789  20200506   92500110000  1588728300110000   \n",
       "4     1603789  20200506   92500110000  1588728300110000   \n",
       "...       ...       ...           ...               ...   \n",
       "2233  1603789  20200506  150000610000  1588748400610000   \n",
       "2234  1603789  20200506  150000610000  1588748400610000   \n",
       "2235  1603789  20200506  150000610000  1588748400610000   \n",
       "2236  1603789  20200506  150000610000  1588748400610000   \n",
       "2237  1603789  20200506  150000610000  1588748400610000   \n",
       "\n",
       "                    datetime  ApplSeqNum  trade_type  trade_flag  trade_price  \\\n",
       "0    2020-05-06 09:25:00.110       23238           1           0        13.05   \n",
       "1    2020-05-06 09:25:00.110       23239           1           0        13.05   \n",
       "2    2020-05-06 09:25:00.110       23240           1           0        13.05   \n",
       "3    2020-05-06 09:25:00.110       23241           1           0        13.05   \n",
       "4    2020-05-06 09:25:00.110       23242           1           0        13.05   \n",
       "...                      ...         ...         ...         ...          ...   \n",
       "2233 2020-05-06 15:00:00.610     2970282           1           0        13.17   \n",
       "2234 2020-05-06 15:00:00.610     2970283           1           0        13.17   \n",
       "2235 2020-05-06 15:00:00.610     2970284           1           0        13.17   \n",
       "2236 2020-05-06 15:00:00.610     2970285           1           0        13.17   \n",
       "2237 2020-05-06 15:00:00.610     2970286           1           0        13.17   \n",
       "\n",
       "      trade_qty  BidApplSeqNum  OfferApplSeqNum  \n",
       "0           100          85856            27284  \n",
       "1          1400          85856           145070  \n",
       "2           100          85856            57977  \n",
       "3          4200          85856           145087  \n",
       "4          3000          85856           146303  \n",
       "...         ...            ...              ...  \n",
       "2233        700        4795252          4784595  \n",
       "2234       2000        4798999          4784595  \n",
       "2235       1500        4800464          4784595  \n",
       "2236        800        4800710          4784595  \n",
       "2237       1300        4800710          4793870  \n",
       "\n",
       "[2238 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db1.read('md_trade', '20200506', '20200506', symbol=[1603789])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

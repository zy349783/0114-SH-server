{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "base = pd.read_csv(r'/mnt/data/20201208/logs_20201208_zs_96_03_day_96data/mdOrderLog_20201208_0832.csv',\n",
    "                    encoding=\"utf-8\").loc[:, [\"clockAtArrival\", \"sequenceNo\", \"exchId\", \"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"Side\", \"OrderType\", \"Price\",\n",
    "                                                 \"OrderQty\", \"mdSource\"]]\n",
    "data1 = pd.read_csv(r'/mnt/data/20201208/zs_7201/zs_7201_postmdgw_20201208_10.0.101.56/Logs_full/mdOrderLog_20201208_0857.csv',\n",
    "                    encoding=\"utf-8\").loc[:, [\"clockAtArrival\", \"sequenceNo\", \"exchId\", \"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"Side\", \"OrderType\", \"Price\",\n",
    "                                                 \"OrderQty\", \"mdSource\"]]\n",
    "data2 = pd.read_csv(r'/mnt/data/20201208/zs_7201/zs_7201_postmdgw_20201208_10.0.101.57/Logs_full/mdOrderLog_20201208_0857.csv',\n",
    "                    encoding=\"utf-8\").loc[:, [\"clockAtArrival\", \"sequenceNo\", \"exchId\", \"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"Side\", \"OrderType\", \"Price\",\n",
    "                                                 \"OrderQty\", \"mdSource\"]]\n",
    "data3 = pd.read_csv(r'/mnt/data/20201208/zs_7201/zs_7201_20201208_10.0.101.57_ama/Logs_full/mdOrderLog_20201208_0856.csv',\n",
    "                    encoding=\"utf-8\").loc[:, [\"clockAtArrival\", \"sequenceNo\", \"exchId\", \"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"Side\", \"OrderType\", \"Price\",\n",
    "                                                 \"OrderQty\", \"mdSource\"]]\n",
    "dataPathLs = np.array(glob.glob(os.path.join('/mnt/data/20201208/zs_7201/zs_7201_20201208_10.0.97.146_postmdgw/Logs_full', 'mdOrderLog***.csv')))\n",
    "data4 = []\n",
    "for d in dataPathLs:\n",
    "    data4 += [pd.read_csv(d, encoding=\"utf-8\").loc[:, [\"clockAtArrival\", \"sequenceNo\", \"exchId\", \"TransactTime\",\n",
    "                                                 \"ApplSeqNum\", \"SecurityID\", \"Side\", \"OrderType\", \"Price\",\n",
    "                                                 \"OrderQty\", \"mdSource\"]]]\n",
    "data4 = pd.concat(data4)\n",
    "stocks = list((set(base[\"SecurityID\"].unique())) & (set(data1[\"SecurityID\"].unique())) & (set(data2['SecurityID'].unique())) \\\n",
    "                                            & (set(data3['SecurityID'].unique())) & (set(data4['SecurityID'].unique())))\n",
    "display(len(stocks))\n",
    "              \n",
    "base = base[base['SecurityID'].isin(stocks)]\n",
    "data1 = data1[data1['SecurityID'].isin(stocks)] \n",
    "data2 = data2[data2['SecurityID'].isin(stocks)]         \n",
    "data3 = data3[data3['SecurityID'].isin(stocks)]         \n",
    "data4 = data4[data4['SecurityID'].isin(stocks)]   \n",
    "              \n",
    "base[\"OrderType\"] = base[\"OrderType\"].apply(lambda x: str(x))\n",
    "data1[\"OrderType\"] = data1[\"OrderType\"].apply(lambda x: str(x))\n",
    "data2[\"OrderType\"] = data2[\"OrderType\"].apply(lambda x: str(x))\n",
    "data3[\"OrderType\"] = data3[\"OrderType\"].apply(lambda x: str(x))\n",
    "data4[\"OrderType\"] = data4[\"OrderType\"].apply(lambda x: str(x))\n",
    "              \n",
    "base['tag'] = 'base'\n",
    "data1['tag'] = '56'\n",
    "data2['tag'] = '57'\n",
    "data3['tag'] = 'ama'\n",
    "data4['tag'] = 'postmdgw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38084645\n",
      "38084645\n",
      "38084645\n",
      "-----------------------------------------------\n",
      "38084645\n",
      "38084645\n",
      "38084645\n",
      "-----------------------------------------------\n",
      "38084645\n",
      "38084645\n",
      "38084645\n",
      "-----------------------------------------------\n",
      "38084645\n",
      "31846631\n",
      "38084645\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test is not complete:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactTime</th>\n",
       "      <th>ApplSeqNum</th>\n",
       "      <th>Side</th>\n",
       "      <th>OrderType</th>\n",
       "      <th>Price</th>\n",
       "      <th>OrderQty</th>\n",
       "      <th>SecurityID</th>\n",
       "      <th>base</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>ama</th>\n",
       "      <th>postmdgw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91500000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172500</td>\n",
       "      <td>75300</td>\n",
       "      <td>2885</td>\n",
       "      <td>1607390100008036</td>\n",
       "      <td>1607390100038404</td>\n",
       "      <td>1607390100038225</td>\n",
       "      <td>1607390100038214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172500</td>\n",
       "      <td>141700</td>\n",
       "      <td>2885</td>\n",
       "      <td>1607390100008037</td>\n",
       "      <td>1607390100038412</td>\n",
       "      <td>1607390100038231</td>\n",
       "      <td>1607390100038221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91500000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>209800</td>\n",
       "      <td>134700</td>\n",
       "      <td>2647</td>\n",
       "      <td>1607390100008037</td>\n",
       "      <td>1607390100038412</td>\n",
       "      <td>1607390100038232</td>\n",
       "      <td>1607390100038221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91500000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50800</td>\n",
       "      <td>374400</td>\n",
       "      <td>850</td>\n",
       "      <td>1607390100008038</td>\n",
       "      <td>1607390100038413</td>\n",
       "      <td>1607390100038232</td>\n",
       "      <td>1607390100038223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91500000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172500</td>\n",
       "      <td>92500</td>\n",
       "      <td>2885</td>\n",
       "      <td>1607390100008038</td>\n",
       "      <td>1607390100038440</td>\n",
       "      <td>1607390100038233</td>\n",
       "      <td>1607390100038226</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070922</th>\n",
       "      <td>101142800</td>\n",
       "      <td>6909819</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>91600</td>\n",
       "      <td>1000</td>\n",
       "      <td>516</td>\n",
       "      <td>1607393502781977</td>\n",
       "      <td>1607393502803860</td>\n",
       "      <td>1607393502803848</td>\n",
       "      <td>1607393502803873</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070923</th>\n",
       "      <td>101142800</td>\n",
       "      <td>6909822</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2161</td>\n",
       "      <td>1607393502781980</td>\n",
       "      <td>1607393502803864</td>\n",
       "      <td>1607393502803853</td>\n",
       "      <td>1607393502803876</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070924</th>\n",
       "      <td>101142800</td>\n",
       "      <td>6750192</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90600</td>\n",
       "      <td>300</td>\n",
       "      <td>2385</td>\n",
       "      <td>1607393502782012</td>\n",
       "      <td>1607393502803885</td>\n",
       "      <td>1607393502803881</td>\n",
       "      <td>1607393502803930</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070925</th>\n",
       "      <td>101142800</td>\n",
       "      <td>6750194</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>400</td>\n",
       "      <td>300222</td>\n",
       "      <td>1607393502782016</td>\n",
       "      <td>1607393502803907</td>\n",
       "      <td>1607393502803922</td>\n",
       "      <td>1607393502803933</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070926</th>\n",
       "      <td>101142800</td>\n",
       "      <td>7175565</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>495000</td>\n",
       "      <td>100</td>\n",
       "      <td>300073</td>\n",
       "      <td>1607393502782113</td>\n",
       "      <td>1607393502803957</td>\n",
       "      <td>1607393502803971</td>\n",
       "      <td>1607393502803988</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6238014 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactTime ApplSeqNum Side OrderType   Price OrderQty SecurityID  \\\n",
       "0            91500000          2    1         2  172500    75300       2885   \n",
       "1            91500000          3    1         2  172500   141700       2885   \n",
       "2            91500000          4    2         2  209800   134700       2647   \n",
       "3            91500000          5    1         2   50800   374400        850   \n",
       "4            91500000          6    1         2  172500    92500       2885   \n",
       "...               ...        ...  ...       ...     ...      ...        ...   \n",
       "14070922    101142800    6909819    2         2   91600     1000        516   \n",
       "14070923    101142800    6909822    1         2   64000     2000       2161   \n",
       "14070924    101142800    6750192    2         2   90600      300       2385   \n",
       "14070925    101142800    6750194    1         2  120000      400     300222   \n",
       "14070926    101142800    7175565    1         2  495000      100     300073   \n",
       "\n",
       "                      base                56                57  \\\n",
       "0         1607390100008036  1607390100038404  1607390100038225   \n",
       "1         1607390100008037  1607390100038412  1607390100038231   \n",
       "2         1607390100008037  1607390100038412  1607390100038232   \n",
       "3         1607390100008038  1607390100038413  1607390100038232   \n",
       "4         1607390100008038  1607390100038440  1607390100038233   \n",
       "...                    ...               ...               ...   \n",
       "14070922  1607393502781977  1607393502803860  1607393502803848   \n",
       "14070923  1607393502781980  1607393502803864  1607393502803853   \n",
       "14070924  1607393502782012  1607393502803885  1607393502803881   \n",
       "14070925  1607393502782016  1607393502803907  1607393502803922   \n",
       "14070926  1607393502782113  1607393502803957  1607393502803971   \n",
       "\n",
       "                       ama postmdgw  \n",
       "0         1607390100038214      NaN  \n",
       "1         1607390100038221      NaN  \n",
       "2         1607390100038221      NaN  \n",
       "3         1607390100038223      NaN  \n",
       "4         1607390100038226      NaN  \n",
       "...                    ...      ...  \n",
       "14070922  1607393502803873      NaN  \n",
       "14070923  1607393502803876      NaN  \n",
       "14070924  1607393502803930      NaN  \n",
       "14070925  1607393502803933      NaN  \n",
       "14070926  1607393502803988      NaN  \n",
       "\n",
       "[6238014 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6238014\n",
      "[91500000 91500010 91500020 ... 101142780 101142790 101142800]\n",
      "2128\n",
      "[2885 2647 850 ... 2473 803 2719]\n"
     ]
    }
   ],
   "source": [
    "columns = [\"TransactTime\", \"ApplSeqNum\",\"Side\",'OrderType', 'Price', 'OrderQty', \"SecurityID\"]\n",
    "base = base[columns + ['clockAtArrival']]\n",
    "for d in [data1, data2, data3, data4]:\n",
    "    base = pd.merge(base, d[columns + ['clockAtArrival']], left_on=columns, right_on=columns, how=\"left\", validate='one_to_one')\n",
    "    if 'clockAtArrival_x' in base.columns:\n",
    "        base = base.rename(columns={'clockAtArrival_x':'base', 'clockAtArrival_y':d['tag'].iloc[0]})\n",
    "    else:\n",
    "        base = base.rename(columns={'clockAtArrival':d['tag'].iloc[0]})\n",
    "    n1 = base[\"base\"].count()\n",
    "    n2 = base[d['tag'].iloc[0]].count()\n",
    "    len1 = len(base)\n",
    "    print(n1)\n",
    "    print(n2)\n",
    "    print(len1)\n",
    "    print(\"-----------------------------------------------\")\n",
    "    if n2 < len1:\n",
    "        display(\"test is not complete:\")\n",
    "        display(base[base[str(d['tag'].iloc[0])].isnull()])\n",
    "        print(len(base[base[str(d['tag'].iloc[0])].isnull()]))\n",
    "        print(np.sort(base[base[str(d['tag'].iloc[0])].isnull()][\"TransactTime\"].unique()))\n",
    "        print(len(base[base[str(d['tag'].iloc[0])].isnull()][\"SecurityID\"].unique()))\n",
    "        print(base[base[str(d['tag'].iloc[0])].isnull()][\"SecurityID\"].unique())\n",
    "    if (len1 == n2) & (n1 < len1):\n",
    "        display(\"test is complete, baseline is not complete:\")\n",
    "        display(base[np.isnan(base[\"base\"])])\n",
    "        print(np.sort(base[np.isnan(base[\"base\"])][\"TransactTime\"].unique()))\n",
    "        print(len(base[np.isnan(base[\"base\"])][\"SecurityID\"].unique()))\n",
    "        print(base[np.isnan(base[\"base\"])][\"SecurityID\"].unique())\n",
    "        display(n2-n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = base[(~base['56'].isnull()) & (~base['57'].isnull()) & (~base['ama'].isnull()) & (~base['postmdgw'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.0\n",
      "99.0\n",
      "119.0\n",
      "557.0\n",
      "1443.0\n",
      "2823.0\n",
      "0.9899639933655776\n",
      "0.010029883537759457\n"
     ]
    }
   ],
   "source": [
    "diff = re[\"postmdgw\"] - re[\"ama\"]\n",
    "print(np.quantile(abs(diff[diff < 0]), 0.25))\n",
    "print(np.median(abs(diff[diff < 0])))\n",
    "print(np.quantile(abs(diff[diff < 0]), 0.75))\n",
    "print(np.quantile(diff[diff > 0], 0.25))\n",
    "print(np.median(diff[diff > 0]))\n",
    "print(np.quantile(diff[diff > 0], 0.75))\n",
    "print(sum(diff < 0)/len(diff))\n",
    "print(sum(diff > 0)/len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.726018\n",
      "20181228\n",
      "0:03:04.372292\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "def dailyDB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    url = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    client = pymongo.MongoClient(url, maxPoolSize=None)\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def read_stock_daily(db, name, start_date=None, end_date=None, skey=None, index_name=None, interval=None, col=None, return_sdi=True):\n",
    "    collection = db[name]\n",
    "    # Build projection\n",
    "    prj = {'_id': 0}\n",
    "    if col is not None:\n",
    "        if return_sdi:\n",
    "            col = ['skey', 'date'] + col\n",
    "        for col_name in col:\n",
    "            prj[col_name] = 1\n",
    "\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if skey is not None:\n",
    "        query['skey'] = {'$in': skey}\n",
    "    if index_name is not None:\n",
    "        query['index_name'] = {'$in': index_name}\n",
    "    if start_date is not None:\n",
    "        if end_date is not None:\n",
    "            query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "        else:\n",
    "            query['date'] = {'$gte': start_date}\n",
    "    elif end_date is not None:\n",
    "        query['date'] = {'$lte': end_date}\n",
    "\n",
    "    # Load data\n",
    "    cur = collection.find(query, prj)\n",
    "    df = pd.DataFrame.from_records(cur)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = df.sort_values(by=['date', 'skey'])\n",
    "    return df   \n",
    "\n",
    "def read_memb_daily(db, name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, col=None, return_sdi=True):\n",
    "    collection = db[name]\n",
    "    # Build projection\n",
    "    prj = {'_id': 0}\n",
    "    if col is not None:\n",
    "        if return_sdi:\n",
    "            col = ['skey', 'date', 'index_id'] + col\n",
    "        for col_name in col:\n",
    "            prj[col_name] = 1\n",
    "\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if skey is not None:\n",
    "        query['skey'] = {'$in': skey}\n",
    "    if index_id is not None:\n",
    "        query['index_id'] = {'$in': index_id}\n",
    "    if interval is not None:\n",
    "        query['interval'] = {'$in': interval}\n",
    "    if start_date is not None:\n",
    "        if end_date is not None:\n",
    "            query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "        else:\n",
    "            query['date'] = {'$gte': start_date}\n",
    "    elif end_date is not None:\n",
    "        query['date'] = {'$lte': end_date}\n",
    "\n",
    "    # Load data\n",
    "    cur = collection.find(query, prj)\n",
    "    df = pd.DataFrame.from_records(cur)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = df.sort_values(by=['date', 'index_id', 'skey'])\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "year = \"2018\"\n",
    "startDate = '20180102'\n",
    "endDate = '20181231'\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db2 = dailyDB(\"192.168.10.178\", database_name, user, password)\n",
    "save = {}\n",
    "save['date'] = []\n",
    "save['secid'] = []\n",
    "mdOrderLog = db1.read('md_order', start_date=startDate, end_date=endDate, symbol=[2000001])\n",
    "datelist = mdOrderLog['date'].unique()\n",
    "ss = pd.read_csv('/mnt/ShareWithServer/result/shangshi.csv')\n",
    "ss['skey'] = np.where(ss['证券代码'].str[-2:] == 'SZ', ss['证券代码'].str[:6].astype(int) + 2000000, ss['证券代码'].str[:6].astype(int) + 1000000)\n",
    "ss['date'] = (ss['上市日期'].str[:4] + ss['上市日期'].str[5:7] + ss['上市日期'].str[8:10]).astype(int)\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "for d in [20181228]:\n",
    "    print(d)\n",
    "    sl1 = read_memb_daily(db2, 'index_memb', index_id=[1000852], start_date=20170901, end_date=20201203)['skey'].unique()\n",
    "    sl1 = sl1[sl1 > 2000000]\n",
    "    data1 = db1.read('md_snapshot_l2', start_date=str(d), end_date=str(d), symbol=list(sl1))\n",
    "    sl1 = data1['skey'].unique()\n",
    "    op = read_stock_daily(db2, 'mdbar1d_tr', start_date=int(d), end_date=int(d))\n",
    "#     sl1 = data1[(data1['cum_volume'] > 0) & (data1['time'] <= 145655000000) & (data1['ApplSeqNum'] == -1)]['skey'].unique()\n",
    "#     print(sl1)\n",
    "    if len(sl1) != 0:\n",
    "        for s in sl1:\n",
    "            mbd = db1.read('md_snapshot_mbd', start_date=str(d), end_date=str(d), symbol=s)\n",
    "            if mbd is None:\n",
    "                if ss[ss['skey'] == s]['date'].iloc[0] == d:\n",
    "                    continue\n",
    "                else:\n",
    "                    save['date'].append(d)\n",
    "                    save['secid'].append(s)\n",
    "                    print(s)\n",
    "                    continue\n",
    "            try:\n",
    "                assert(mbd.shape[1] == 83)\n",
    "            except:\n",
    "                print('mdb data column unupdated')\n",
    "                print(s)\n",
    "            try:\n",
    "                op1 = op[op['skey'] == s]['open'].iloc[0]\n",
    "                assert(mbd[mbd['cum_volume'] > 0]['open'].iloc[0] == op1)\n",
    "            except:\n",
    "                print('%s have no information in mdbar1d_tr' % str(s))\n",
    "            l2 = data1[data1['skey'] == s]\n",
    "            cols = ['skey', 'date', 'cum_volume', 'prev_close', 'open', 'close', 'cum_trades_cnt', 'bid10p', 'bid9p',\n",
    "                   'bid8p', 'bid7p', 'bid6p', 'bid5p', 'bid4p', 'bid3p', 'bid2p', 'bid1p', 'ask1p', 'ask2p',\n",
    "                   'ask3p', 'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p', 'bid10q', 'bid9q', \n",
    "                   'bid8q', 'bid7q', 'bid6q', 'bid5q', 'bid4q', 'bid3q', 'bid2q', 'bid1q', 'ask1q', 'ask2q', 'ask3q', \n",
    "                   'ask4q', 'ask5q', 'ask6q','ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n', 'ask1n', 'total_bid_quantity', 'total_ask_quantity']\n",
    "            mbd1 = mbd.drop_duplicates(cols, keep='first')\n",
    "            mbd = mbd1[cols+['ApplSeqNum']]\n",
    "            if l2.shape[1] == 192:\n",
    "                l2 = l2[l2.columns[:-1]]\n",
    "            rl2 = pd.merge(l2, mbd, on=cols, how='left')\n",
    "            try:\n",
    "                assert(rl2[(rl2['ApplSeqNum'].isnull()) & (rl2['cum_volume'] > 0) & (rl2['time'] <= 145655000000)].shape[0] == 0)\n",
    "            except:\n",
    "                print(rl2[(rl2['ApplSeqNum'].isnull()) & (rl2['cum_volume'] > 0) & (rl2['time'] <= 145655000000)][['skey', 'date', 'time', 'cum_volume', 'close', 'bid1p', 'bid2p','bid1q', 'bid2q', 'ask1p', 'ask2p', 'ask1q', 'ask2q']])\n",
    "                print(mbd1.tail(1)[['skey', 'date', 'time', 'cum_volume', 'close', 'bid1p', 'bid2p','bid1q', 'bid2q', 'ask1p', 'ask2p', 'ask1q', 'ask2q']])\n",
    "            rl2.loc[rl2['ApplSeqNum'].isnull(), 'ApplSeqNum'] = -1\n",
    "            rl2['ApplSeqNum'] = rl2['ApplSeqNum'].astype('int32') \n",
    "            assert(rl2.shape[0] == l2.shape[0])\n",
    "            db1.write('md_snapshot_l2', rl2)\n",
    "        print(datetime.datetime.now() - startTm)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200221\n",
      "0:06:43.501868\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "def dailyDB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    url = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    client = pymongo.MongoClient(url, maxPoolSize=None)\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def read_stock_daily(db, name, start_date=None, end_date=None, skey=None, index_name=None, interval=None, col=None, return_sdi=True):\n",
    "    collection = db[name]\n",
    "    # Build projection\n",
    "    prj = {'_id': 0}\n",
    "    if col is not None:\n",
    "        if return_sdi:\n",
    "            col = ['skey', 'date'] + col\n",
    "        for col_name in col:\n",
    "            prj[col_name] = 1\n",
    "\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if skey is not None:\n",
    "        query['skey'] = {'$in': skey}\n",
    "    if index_name is not None:\n",
    "        query['index_name'] = {'$in': index_name}\n",
    "    if start_date is not None:\n",
    "        if end_date is not None:\n",
    "            query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "        else:\n",
    "            query['date'] = {'$gte': start_date}\n",
    "    elif end_date is not None:\n",
    "        query['date'] = {'$lte': end_date}\n",
    "\n",
    "    # Load data\n",
    "    cur = collection.find(query, prj)\n",
    "    df = pd.DataFrame.from_records(cur)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = df.sort_values(by=['date', 'skey'])\n",
    "    return df   \n",
    "\n",
    "def read_memb_daily(db, name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, col=None, return_sdi=True):\n",
    "    collection = db[name]\n",
    "    # Build projection\n",
    "    prj = {'_id': 0}\n",
    "    if col is not None:\n",
    "        if return_sdi:\n",
    "            col = ['skey', 'date', 'index_id'] + col\n",
    "        for col_name in col:\n",
    "            prj[col_name] = 1\n",
    "\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if skey is not None:\n",
    "        query['skey'] = {'$in': skey}\n",
    "    if index_id is not None:\n",
    "        query['index_id'] = {'$in': index_id}\n",
    "    if interval is not None:\n",
    "        query['interval'] = {'$in': interval}\n",
    "    if start_date is not None:\n",
    "        if end_date is not None:\n",
    "            query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "        else:\n",
    "            query['date'] = {'$gte': start_date}\n",
    "    elif end_date is not None:\n",
    "        query['date'] = {'$lte': end_date}\n",
    "\n",
    "    # Load data\n",
    "    cur = collection.find(query, prj)\n",
    "    df = pd.DataFrame.from_records(cur)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = df.sort_values(by=['date', 'index_id', 'skey'])\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "year = \"2018\"\n",
    "startDate = '20180102'\n",
    "endDate = '20181231'\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db2 = dailyDB(\"192.168.10.178\", database_name, user, password)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "for d in [20200221]:\n",
    "    print(d)\n",
    "    sl1 = read_memb_daily(db2, 'index_memb', index_id=[1000852], start_date=20170901, end_date=20201203)['skey'].unique()\n",
    "    sl1 = sl1[sl1 > 2000000]\n",
    "    data1 = db1.read('md_snapshot_l2', start_date=str(d), end_date=str(d), symbol=list(sl1))\n",
    "    sl1 = data1['skey'].unique()\n",
    "    op = read_stock_daily(db2, 'mdbar1d_tr', start_date=int(d), end_date=int(d))\n",
    "#     sl1 = data1[(data1['cum_volume'] > 0) & (data1['time'] <= 145655000000) & (data1['ApplSeqNum'] == -1)]['skey'].unique()\n",
    "#     print(sl1)\n",
    "    if len(sl1) != 0:\n",
    "        for s in sl1:\n",
    "            mbd = db1.read('md_snapshot_mbd', start_date=str(d), end_date=str(d), symbol=s)\n",
    "            if mbd is None:\n",
    "                if ss[ss['skey'] == s]['date'].iloc[0] == d:\n",
    "                    continue\n",
    "                else:\n",
    "                    save['date'].append(d)\n",
    "                    save['secid'].append(s)\n",
    "                    print(s)\n",
    "                    continue\n",
    "            try:\n",
    "                assert(mbd.shape[1] == 83)\n",
    "            except:\n",
    "                print('mdb data column unupdated')\n",
    "                print(s)\n",
    "            try:\n",
    "                op1 = op[op['skey'] == s]['open'].iloc[0]\n",
    "                assert(mbd[mbd['cum_volume'] > 0]['open'].iloc[0] == op1)\n",
    "            except:\n",
    "                print('%s have no information in mdbar1d_tr' % str(s))\n",
    "            l2 = data1[data1['skey'] == s]\n",
    "            cols = ['skey', 'date', 'cum_volume', 'prev_close', 'open', 'close', 'cum_trades_cnt', 'bid10p', 'bid9p',\n",
    "                   'bid8p', 'bid7p', 'bid6p', 'bid5p', 'bid4p', 'bid3p', 'bid2p', 'bid1p', 'ask1p', 'ask2p',\n",
    "                   'ask3p', 'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p', 'bid10q', 'bid9q', \n",
    "                   'bid8q', 'bid7q', 'bid6q', 'bid5q', 'bid4q', 'bid3q', 'bid2q', 'bid1q', 'ask1q', 'ask2q', 'ask3q', \n",
    "                   'ask4q', 'ask5q', 'ask6q','ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n', 'ask1n', 'total_bid_quantity', 'total_ask_quantity']\n",
    "            mbd1 = mbd.drop_duplicates(cols, keep='first')\n",
    "            mbd = mbd1[cols+['ApplSeqNum']]\n",
    "            if l2.shape[1] == 192:\n",
    "                l2 = l2[l2.columns[:-1]]\n",
    "            rl2 = pd.merge(l2, mbd, on=cols, how='left')\n",
    "            try:\n",
    "                assert(rl2[(rl2['ApplSeqNum'].isnull()) & (rl2['cum_volume'] > 0) & (rl2['time'] <= 145655000000)].shape[0] == 0)\n",
    "            except:\n",
    "                print(rl2[(rl2['ApplSeqNum'].isnull()) & (rl2['cum_volume'] > 0) & (rl2['time'] <= 145655000000)][['skey', 'date', 'time', 'cum_volume', 'close', 'bid1p', 'bid2p','bid1q', 'bid2q', 'ask1p', 'ask2p', 'ask1q', 'ask2q']])\n",
    "                print(mbd1.tail(1)[['skey', 'date', 'time', 'cum_volume', 'close', 'bid1p', 'bid2p','bid1q', 'bid2q', 'ask1p', 'ask2p', 'ask1q', 'ask2q']])\n",
    "            rl2.loc[rl2['ApplSeqNum'].isnull(), 'ApplSeqNum'] = -1\n",
    "            rl2['ApplSeqNum'] = rl2['ApplSeqNum'].astype('int32') \n",
    "            assert(rl2.shape[0] == l2.shape[0])\n",
    "            db1.write('md_snapshot_l2', rl2)\n",
    "        print(datetime.datetime.now() - startTm)\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

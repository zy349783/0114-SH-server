{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200102'\n",
    "endDate = '20200630'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs[:1]:\n",
    "    date = os.path.basename(data)\n",
    "    readPath = '/mnt/e/unzip_data/2020/SH/' + date + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs == 16) | (dateLs == 300) | (dateLs == 852) | (dateLs == 905)]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [17,19,20,21,22,41,42,49])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    \n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "    SH.columns = ['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime']\n",
    "    SH = SH.fillna(0)\n",
    "    assert(sum(SH[SH['cum_volume'] == 0].groupby('skey')['time'].max() \n",
    "               < SH[SH['cum_volume'] > 0].groupby('skey')['time'].min()))\n",
    "    assert(SH[SH['time'] >= 150500000000].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "    m_ax = SH[SH['time'] >= 150500000000].groupby('skey').first()['time'].max()\n",
    "    SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= m_ax)]\n",
    "\n",
    "#     SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "#     SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "#     assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "#     assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "#     SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "#     SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "#     assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "#     assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "#     assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "#     for cols in ['open', 'high', 'prev_close', 'low', 'close']:\n",
    "#         SH[cols] = SH[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "\n",
    "#     SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "#              \"prev_close\", \"open\", \"high\", \"low\", \"close\"]]\n",
    "    \n",
    "#     assert(sum(SH['time']%1000000) == 0)\n",
    "            \n",
    "#     display(SH[\"date\"].iloc[0])\n",
    "#     print(\"index finished\")\n",
    "    \n",
    "#     database_name = 'com_md_eq_cn'\n",
    "#     user = \"zhenyuy\"\n",
    "#     password = \"bnONBrzSMGoE\"\n",
    "\n",
    "#     db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "#     db1.write('md_index', SH)\n",
    "    \n",
    "#     del SH\n",
    "\n",
    "# print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cum_volume, open, high, prev_close, low, close, cum_amount, skey, date, time, clockAtArrival, datetime]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SH[SH['time'] >= 150500000000].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150500710000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SH[SH['time'] >= 150500000000].groupby('skey').first()['time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(SH[SH['cum_volume'] == 0].groupby('skey')['time'].max() < SH[SH['cum_volume'] > 0].groupby('skey')['time'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skey\n",
       "1000016    92503000000\n",
       "1000300    92503000000\n",
       "1000852    92503000000\n",
       "1000905    92503000000\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SH[SH['cum_volume'] > 0].groupby('skey')['time'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skey\n",
       "1000016    92503000000\n",
       "1000300    92503000000\n",
       "1000852    92503000000\n",
       "1000905    92503000000\n",
       "Name: time, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SH[SH['cum_volume'] == 0].groupby('skey')['time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20200701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200702"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200707"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200708"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200709"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200717"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200721"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200722"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200723"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200729"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200730"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200701'\n",
    "endDate = '20200731'\n",
    "readPath = '/mnt/Kevin_zhenyu/KR_daily_data' + '/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in np.sort(dataPathLs):\n",
    "    readPath = data + '/SH/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs == 16) | (dateLs == 300) | (dateLs == 852) | (dateLs == 905)]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [17,19,20,21,22,41,42,49])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    \n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "    SH.columns = ['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime']\n",
    "    SH = SH.fillna(0)\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    for cols in ['open', 'high', 'prev_close', 'low', 'close']:\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "\n",
    "\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "             \"prev_close\", \"open\", \"high\", \"low\", \"close\"]]\n",
    "            \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"index finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_index', SH)\n",
    "    \n",
    "    del SH\n",
    "\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skey</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "      <th>ordering</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000016</td>\n",
       "      <td>20200731</td>\n",
       "      <td>84442000000</td>\n",
       "      <td>1596156282000000</td>\n",
       "      <td>2020-07-31 08:44:42.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3231.5193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3231.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000016</td>\n",
       "      <td>20200731</td>\n",
       "      <td>84542000000</td>\n",
       "      <td>1596156342000000</td>\n",
       "      <td>2020-07-31 08:45:42.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3231.5193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3231.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000016</td>\n",
       "      <td>20200731</td>\n",
       "      <td>84542000000</td>\n",
       "      <td>1596156342000000</td>\n",
       "      <td>2020-07-31 08:45:42.000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3231.5193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3231.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000016</td>\n",
       "      <td>20200731</td>\n",
       "      <td>84642000000</td>\n",
       "      <td>1596156402000000</td>\n",
       "      <td>2020-07-31 08:46:42.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3231.5193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3231.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000016</td>\n",
       "      <td>20200731</td>\n",
       "      <td>84642000000</td>\n",
       "      <td>1596156402000000</td>\n",
       "      <td>2020-07-31 08:46:42.000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3231.5193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3231.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14247</th>\n",
       "      <td>1000905</td>\n",
       "      <td>20200731</td>\n",
       "      <td>150215270000</td>\n",
       "      <td>1596178935270000</td>\n",
       "      <td>2020-07-31 15:02:15.270</td>\n",
       "      <td>3566</td>\n",
       "      <td>179912941</td>\n",
       "      <td>2.078625e+11</td>\n",
       "      <td>6511.4360</td>\n",
       "      <td>6499.3939</td>\n",
       "      <td>6619.2616</td>\n",
       "      <td>6463.415</td>\n",
       "      <td>6579.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14248</th>\n",
       "      <td>1000905</td>\n",
       "      <td>20200731</td>\n",
       "      <td>150215270000</td>\n",
       "      <td>1596178935270000</td>\n",
       "      <td>2020-07-31 15:02:15.270</td>\n",
       "      <td>3567</td>\n",
       "      <td>179912941</td>\n",
       "      <td>2.078625e+11</td>\n",
       "      <td>6511.4360</td>\n",
       "      <td>6499.3939</td>\n",
       "      <td>6619.2616</td>\n",
       "      <td>6463.415</td>\n",
       "      <td>6579.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14249</th>\n",
       "      <td>1000905</td>\n",
       "      <td>20200731</td>\n",
       "      <td>150215270000</td>\n",
       "      <td>1596178935270000</td>\n",
       "      <td>2020-07-31 15:02:15.270</td>\n",
       "      <td>3568</td>\n",
       "      <td>179912941</td>\n",
       "      <td>2.078625e+11</td>\n",
       "      <td>6511.4360</td>\n",
       "      <td>6499.3939</td>\n",
       "      <td>6619.2616</td>\n",
       "      <td>6463.415</td>\n",
       "      <td>6579.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>1000905</td>\n",
       "      <td>20200731</td>\n",
       "      <td>150215270000</td>\n",
       "      <td>1596178935270000</td>\n",
       "      <td>2020-07-31 15:02:15.270</td>\n",
       "      <td>3569</td>\n",
       "      <td>179912941</td>\n",
       "      <td>2.078625e+11</td>\n",
       "      <td>6511.4360</td>\n",
       "      <td>6499.3939</td>\n",
       "      <td>6619.2616</td>\n",
       "      <td>6463.415</td>\n",
       "      <td>6579.6696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14251</th>\n",
       "      <td>1000905</td>\n",
       "      <td>20200731</td>\n",
       "      <td>150215270000</td>\n",
       "      <td>1596178935270000</td>\n",
       "      <td>2020-07-31 15:02:15.270</td>\n",
       "      <td>3570</td>\n",
       "      <td>179912941</td>\n",
       "      <td>2.078625e+11</td>\n",
       "      <td>6511.4360</td>\n",
       "      <td>6499.3939</td>\n",
       "      <td>6619.2616</td>\n",
       "      <td>6463.415</td>\n",
       "      <td>6579.6696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14252 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          skey      date          time    clockAtArrival  \\\n",
       "0      1000016  20200731   84442000000  1596156282000000   \n",
       "1      1000016  20200731   84542000000  1596156342000000   \n",
       "2      1000016  20200731   84542000000  1596156342000000   \n",
       "3      1000016  20200731   84642000000  1596156402000000   \n",
       "4      1000016  20200731   84642000000  1596156402000000   \n",
       "...        ...       ...           ...               ...   \n",
       "14247  1000905  20200731  150215270000  1596178935270000   \n",
       "14248  1000905  20200731  150215270000  1596178935270000   \n",
       "14249  1000905  20200731  150215270000  1596178935270000   \n",
       "14250  1000905  20200731  150215270000  1596178935270000   \n",
       "14251  1000905  20200731  150215270000  1596178935270000   \n",
       "\n",
       "                     datetime  ordering  cum_volume    cum_amount  prev_close  \\\n",
       "0     2020-07-31 08:44:42.000         1           0  0.000000e+00   3231.5193   \n",
       "1     2020-07-31 08:45:42.000         2           0  0.000000e+00   3231.5193   \n",
       "2     2020-07-31 08:45:42.000         3           0  0.000000e+00   3231.5193   \n",
       "3     2020-07-31 08:46:42.000         4           0  0.000000e+00   3231.5193   \n",
       "4     2020-07-31 08:46:42.000         5           0  0.000000e+00   3231.5193   \n",
       "...                       ...       ...         ...           ...         ...   \n",
       "14247 2020-07-31 15:02:15.270      3566   179912941  2.078625e+11   6511.4360   \n",
       "14248 2020-07-31 15:02:15.270      3567   179912941  2.078625e+11   6511.4360   \n",
       "14249 2020-07-31 15:02:15.270      3568   179912941  2.078625e+11   6511.4360   \n",
       "14250 2020-07-31 15:02:15.270      3569   179912941  2.078625e+11   6511.4360   \n",
       "14251 2020-07-31 15:02:15.270      3570   179912941  2.078625e+11   6511.4360   \n",
       "\n",
       "            open       high       low      close  \n",
       "0         0.0000     0.0000     0.000  3231.5193  \n",
       "1         0.0000     0.0000     0.000  3231.5193  \n",
       "2         0.0000     0.0000     0.000  3231.5193  \n",
       "3         0.0000     0.0000     0.000  3231.5193  \n",
       "4         0.0000     0.0000     0.000  3231.5193  \n",
       "...          ...        ...       ...        ...  \n",
       "14247  6499.3939  6619.2616  6463.415  6579.6696  \n",
       "14248  6499.3939  6619.2616  6463.415  6579.6696  \n",
       "14249  6499.3939  6619.2616  6463.415  6579.6696  \n",
       "14250  6499.3939  6619.2616  6463.415  6579.6696  \n",
       "14251  6499.3939  6619.2616  6463.415  6579.6696  \n",
       "\n",
       "[14252 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1.read('md_index', 20200731, 20200731)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

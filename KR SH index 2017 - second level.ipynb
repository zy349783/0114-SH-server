{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.795793\n",
      "20170703 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.440587\n",
      "20170704 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.577588\n",
      "20170705 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170705"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.671740\n",
      "20170706 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.499465\n",
      "20170707 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170707"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.469788\n",
      "20170710 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170710"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.473170\n",
      "20170711 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170711"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.447462\n",
      "20170712 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.527044\n",
      "20170713 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.445162\n",
      "20170714 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170714"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.520915\n",
      "20170717 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170717"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.448121\n",
      "20170718 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.505611\n",
      "20170719 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170719"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.446058\n",
      "20170720 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.446710\n",
      "20170721 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170721"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.467083\n",
      "20170724 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.447394\n",
      "20170725 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170725"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.405245\n",
      "20170726 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.453862\n",
      "20170727 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170727"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.471909\n",
      "20170728 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.416198\n",
      "20170731 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.420439\n",
      "20170801 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.515847\n",
      "20170802 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170802"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.468680\n",
      "20170803 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.463189\n",
      "20170804 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.425533\n",
      "20170807 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.455176\n",
      "20170808 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.536976\n",
      "20170809 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.641556\n",
      "20170810 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.497253\n",
      "20170811 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.484043\n",
      "20170814 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170814"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.375625\n",
      "20170815 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170815"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.459418\n",
      "20170816 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170816"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.441862\n",
      "20170817 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.379514\n",
      "20170818 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170818"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.537985\n",
      "20170821 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170821"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.450398\n",
      "20170822 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.451661\n",
      "20170823 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170823"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.475559\n",
      "20170824 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.504323\n",
      "20170825 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170825"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.459112\n",
      "20170828 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.405091\n",
      "20170829 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:00.564319\n",
      "20170830 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170830"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.888292\n",
      "20170831 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170831"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:37.163149\n",
      "20170901 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:40.851953\n",
      "20170904 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170904"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:58.779334\n",
      "20170905 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:37.054955\n",
      "20170906 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:47.607916\n",
      "20170907 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:33.723518\n",
      "20170908 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170908"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:38.936183\n",
      "20170911 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170911"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:36.031543\n",
      "20170912 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:41.154837\n",
      "20170913 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170913"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:46.492666\n",
      "20170914 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:46.248767\n",
      "20170915 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:34.554084\n",
      "20170918 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170918"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:01.560818\n",
      "20170919 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170919"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.457590\n",
      "20170920 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:37.055489\n",
      "20170921 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:33.877431\n",
      "20170922 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170922"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:52.362001\n",
      "20170925 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.818662\n",
      "20170926 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170926"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:12.434244\n",
      "20170927 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.324583\n",
      "20170928 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170928"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.164341\n",
      "20170929 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20170929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:33.085710\n",
      "20171009 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171009"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.876367\n",
      "20171010 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.881835\n",
      "20171011 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171011"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:37.098711\n",
      "20171012 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171012"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.505501\n",
      "20171013 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:57.452948\n",
      "20171016 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.795811\n",
      "20171017 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.330689\n",
      "20171018 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.239611\n",
      "20171019 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.654685\n",
      "20171020 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171020"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.976331\n",
      "20171023 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.807250\n",
      "20171024 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:34.755683\n",
      "20171025 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:41.603727\n",
      "20171026 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.179518\n",
      "20171027 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171027"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.977728\n",
      "20171030 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171030"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:19.414590\n",
      "20171031 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.159189\n",
      "20171101 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:34.801995\n",
      "20171102 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.262375\n",
      "20171103 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:27.137535\n",
      "20171106 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.548535\n",
      "20171107 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.817929\n",
      "20171108 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.460580\n",
      "20171109 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.902363\n",
      "20171110 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:32.033861\n",
      "20171113 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:31.990146\n",
      "20171114 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.776391\n",
      "20171115 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.708390\n",
      "20171116 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:48.058141\n",
      "20171117 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.621061\n",
      "20171120 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:05.777722\n",
      "20171121 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.333412\n",
      "20171122 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:32.404395\n",
      "20171123 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.214970\n",
      "20171124 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:26.713250\n",
      "20171127 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.285878\n",
      "20171128 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171128"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:04.211013\n",
      "20171129 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171129"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.634515\n",
      "20171130 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171130"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:42.541724\n",
      "20171201 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171201"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:39.778337\n",
      "20171204 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.967548\n",
      "20171205 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.114175\n",
      "20171206 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:06.841176\n",
      "20171207 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:32.279224\n",
      "20171208 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:40.416344\n",
      "20171211 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.892642\n",
      "20171212 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:26.388485\n",
      "20171213 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:38.613131\n",
      "20171214 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.939458\n",
      "20171215 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:28.201285\n",
      "20171218 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.803724\n",
      "20171219 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.525416\n",
      "20171220 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:01:43.341931\n",
      "20171221 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:25.933804\n",
      "20171222 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171222"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.540527\n",
      "20171225 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.639859\n",
      "20171226 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171226"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:29.218963\n",
      "20171227 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:30.456967\n",
      "20171228 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "0:00:27.841558\n",
      "20171229 unzip finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20171229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "year = \"2017\"\n",
    "startDate = '20170701'\n",
    "endDate = '20171231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2017/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs == 16) | (dateLs == 300) | (dateLs == 852) | (dateLs == 905)]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [17,19,20,21,22,34,41,42])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    \n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"SendingTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['SendingTime'] - int(SH['SendingTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "    SH.columns = ['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime']\n",
    "    SH = SH.fillna(0)\n",
    "    SH = SH.drop_duplicates(['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime'])\n",
    "    assert(sum(SH['time']%1000000) == 0)\n",
    "    assert(sum(SH[SH['cum_volume'] == 0].groupby('skey')['time'].max() \n",
    "               < SH[SH['cum_volume'] > 0].groupby('skey')['time'].min()))\n",
    "    m_ax = SH[SH['time'] <= 150500000000].groupby('skey').last()['time'].min()\n",
    "    try:\n",
    "        assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "               (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "        SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150500000000)]\n",
    "    except:\n",
    "        try:\n",
    "            m_ax = SH[SH['time'] <= 150700000000].groupby('skey').last()['time'].min()\n",
    "            assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                                   'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "                   (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150700000000)]\n",
    "        except:\n",
    "            m_ax = SH[SH['time'] <= 150800000000].groupby('skey').last()['time'].min()\n",
    "            assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                                   'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "                   (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150800000000)]\n",
    "\n",
    "    \n",
    "    k1 = SH.groupby('skey')['datetime'].min().reset_index()\n",
    "    k1 = k1.rename(columns={'datetime':'min'})\n",
    "    k2 = SH.groupby('skey')['datetime'].max().reset_index()\n",
    "    k2 = k2.rename(columns={'datetime':'max'})\n",
    "    k = pd.merge(k1, k2, on='skey')\n",
    "    k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "    df = pd.DataFrame()\n",
    "    for i in np.arange(k.shape[0]):\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "        df1['skey'] = k.loc[i, 'skey']\n",
    "        assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "        assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "        df = pd.concat([df, df1])\n",
    "    \n",
    "    SH = pd.merge(SH, df, left_on=['skey', 'datetime'], right_on=['skey', 'datetime1'], how='outer').sort_values(by=['skey', 'datetime1']).reset_index(drop=True)\n",
    "    assert(SH[SH['datetime1'].isnull()].shape[0] == 0)\n",
    "    for cols in ['date', 'cum_volume', 'cum_amount', 'prev_close', 'open', 'high', 'low', 'close']:\n",
    "        SH[cols] = SH.groupby('skey')[cols].ffill()\n",
    "    SH.drop([\"datetime\"],axis=1,inplace=True)\n",
    "    SH = SH.rename(columns={'datetime1':'datetime'})\n",
    "    SH['date'] = SH['date'].iloc[0]\n",
    "    SH['date'] = SH['date'].astype('int32')\n",
    "    SH['skey'] = SH['skey'].astype('int32')\n",
    "    SH[\"time\"] = SH['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "    SH['SendingTime'] = SH['date'] * 1000000 + SH['time']\n",
    "    SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    SH['time'] = SH['time'] * 1000000\n",
    "    \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    for cols in ['open', 'high', 'prev_close', 'low', 'close']:\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "             \"prev_close\", \"open\", \"high\", \"low\", \"close\"]]    \n",
    "    m_in = SH[SH['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "    m_ax = SH[SH['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "    try:\n",
    "        assert((SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "        SH = pd.concat([SH[SH['time'] <= 113500000000], SH[SH['time'] >= 125500000000]])\n",
    "    except:\n",
    "        print(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep='first').groupby('skey')['time'].unique())\n",
    "        tt = SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep='first').groupby('skey')['time'].last().unique().max()\n",
    "        if tt < 121500000000:\n",
    "            m_in = tt\n",
    "            assert((SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = pd.concat([SH[SH['time'] <= m_in], SH[SH['time'] >= 125500000000]])\n",
    "        else:\n",
    "            m_ax = tt\n",
    "            assert((SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = pd.concat([SH[SH['time'] <= 113500000000], SH[SH['time'] >= m_ax]])\n",
    "    \n",
    "    SH = SH.sort_values(by=['skey', 'time', 'cum_volume'])\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    SH['ordering'] = SH['ordering'].astype('int32')\n",
    "    SH['cum_volume'] = SH['cum_volume'].astype('int64')\n",
    "    \n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "             \"open\", \"close\"]]\n",
    "            \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"index finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_index', SH)\n",
    "    \n",
    "    del SH\n",
    "\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "year = \"2017\"\n",
    "startDate = '20170630'\n",
    "endDate = '20170630'\n",
    "readPath = '/mnt/ShareWithServer/data/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    readPath = data + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs == 16) | (dateLs == 300) | (dateLs == 852) | (dateLs == 905)]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [17,19,20,21,22,34,41,42])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    \n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"SendingTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['SendingTime'] - int(SH['SendingTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "    SH.columns = ['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime']\n",
    "    SH = SH.fillna(0)\n",
    "    SH = SH.drop_duplicates(['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "                  'date', 'time', 'clockAtArrival', 'datetime'])\n",
    "    assert(sum(SH['time']%1000000) == 0)\n",
    "    assert(sum(SH[SH['cum_volume'] == 0].groupby('skey')['time'].max() \n",
    "               < SH[SH['cum_volume'] > 0].groupby('skey')['time'].min()))\n",
    "    m_ax = SH[SH['time'] <= 150500000000].groupby('skey').last()['time'].min()\n",
    "    try:\n",
    "        assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "               (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "        SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150500000000)]\n",
    "    except:\n",
    "        try:\n",
    "            m_ax = SH[SH['time'] <= 150700000000].groupby('skey').last()['time'].min()\n",
    "            assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                                   'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "                   (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150700000000)]\n",
    "        except:\n",
    "            m_ax = SH[SH['time'] <= 150800000000].groupby('skey').last()['time'].min()\n",
    "            assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                                   'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "                   (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150800000000)]\n",
    "\n",
    "    \n",
    "    k1 = SH.groupby('skey')['datetime'].min().reset_index()\n",
    "    k1 = k1.rename(columns={'datetime':'min'})\n",
    "    k2 = SH.groupby('skey')['datetime'].max().reset_index()\n",
    "    k2 = k2.rename(columns={'datetime':'max'})\n",
    "    k = pd.merge(k1, k2, on='skey')\n",
    "    k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "    df = pd.DataFrame()\n",
    "    for i in np.arange(k.shape[0]):\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "        df1['skey'] = k.loc[i, 'skey']\n",
    "        assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "        assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "        df = pd.concat([df, df1])\n",
    "    \n",
    "    SH = pd.merge(SH, df, left_on=['skey', 'datetime'], right_on=['skey', 'datetime1'], how='outer').sort_values(by=['skey', 'datetime1']).reset_index(drop=True)\n",
    "    assert(SH[SH['datetime1'].isnull()].shape[0] == 0)\n",
    "    for cols in ['date', 'cum_volume', 'cum_amount', 'prev_close', 'open', 'high', 'low', 'close']:\n",
    "        SH[cols] = SH.groupby('skey')[cols].ffill()\n",
    "    SH.drop([\"datetime\"],axis=1,inplace=True)\n",
    "    SH = SH.rename(columns={'datetime1':'datetime'})\n",
    "    SH['date'] = SH['date'].iloc[0]\n",
    "    SH['date'] = SH['date'].astype('int32')\n",
    "    SH['skey'] = SH['skey'].astype('int32')\n",
    "    SH[\"time\"] = SH['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "    SH['SendingTime'] = SH['date'] * 1000000 + SH['time']\n",
    "    SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    SH['time'] = SH['time'] * 1000000\n",
    "    \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    for cols in ['open', 'high', 'prev_close', 'low', 'close']:\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "             \"prev_close\", \"open\", \"high\", \"low\", \"close\"]]    \n",
    "    m_in = SH[SH['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "    m_ax = SH[SH['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "    try:\n",
    "        assert((SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "        SH = pd.concat([SH[SH['time'] <= 113500000000], SH[SH['time'] >= 125500000000]])\n",
    "    except:\n",
    "        print(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep='first').groupby('skey')['time'].unique())\n",
    "        tt = SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep='first').groupby('skey')['time'].last().unique().max()\n",
    "        if tt < 121500000000:\n",
    "            m_in = tt\n",
    "            assert((SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = pd.concat([SH[SH['time'] <= m_in], SH[SH['time'] >= 125500000000]])\n",
    "        else:\n",
    "            m_ax = tt\n",
    "            assert((SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "          & (sum(SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(SH[(SH['time'] >= m_in) & (SH['time'] < m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "            SH = pd.concat([SH[SH['time'] <= 113500000000], SH[SH['time'] >= m_ax]])\n",
    "    \n",
    "    SH = SH.sort_values(by=['skey', 'time', 'cum_volume'])\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    SH['ordering'] = SH['ordering'].astype('int32')\n",
    "    SH['cum_volume'] = SH['cum_volume'].astype('int64')\n",
    "    \n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "             \"open\", \"close\"]]\n",
    "            \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"index finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_index', SH)\n",
    "    \n",
    "    del SH\n",
    "\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skey</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000016</th>\n",
       "      <td>20170630</td>\n",
       "      <td>113624000000</td>\n",
       "      <td>1498793784000000</td>\n",
       "      <td>2017-06-30 11:36:24</td>\n",
       "      <td>10180068.0</td>\n",
       "      <td>1.340949e+10</td>\n",
       "      <td>2552.9754</td>\n",
       "      <td>2543.7865</td>\n",
       "      <td>2548.1345</td>\n",
       "      <td>2533.8595</td>\n",
       "      <td>2542.5826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000300</th>\n",
       "      <td>20170630</td>\n",
       "      <td>113624000000</td>\n",
       "      <td>1498793784000000</td>\n",
       "      <td>2017-06-30 11:36:24</td>\n",
       "      <td>45001697.0</td>\n",
       "      <td>5.756643e+10</td>\n",
       "      <td>3668.8279</td>\n",
       "      <td>3654.7348</td>\n",
       "      <td>3660.0172</td>\n",
       "      <td>3646.2272</td>\n",
       "      <td>3658.4181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000852</th>\n",
       "      <td>20170630</td>\n",
       "      <td>113624000000</td>\n",
       "      <td>1498793784000000</td>\n",
       "      <td>2017-06-30 11:36:24</td>\n",
       "      <td>34000656.0</td>\n",
       "      <td>3.877080e+10</td>\n",
       "      <td>7456.6719</td>\n",
       "      <td>7435.7002</td>\n",
       "      <td>7443.7011</td>\n",
       "      <td>7416.4484</td>\n",
       "      <td>7440.0605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000905</th>\n",
       "      <td>20170630</td>\n",
       "      <td>113624000000</td>\n",
       "      <td>1498793784000000</td>\n",
       "      <td>2017-06-30 11:36:24</td>\n",
       "      <td>28273982.0</td>\n",
       "      <td>3.515648e+10</td>\n",
       "      <td>6116.9932</td>\n",
       "      <td>6101.3648</td>\n",
       "      <td>6110.7696</td>\n",
       "      <td>6090.2523</td>\n",
       "      <td>6110.0707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date          time    clockAtArrival            datetime  \\\n",
       "skey                                                                    \n",
       "1000016  20170630  113624000000  1498793784000000 2017-06-30 11:36:24   \n",
       "1000300  20170630  113624000000  1498793784000000 2017-06-30 11:36:24   \n",
       "1000852  20170630  113624000000  1498793784000000 2017-06-30 11:36:24   \n",
       "1000905  20170630  113624000000  1498793784000000 2017-06-30 11:36:24   \n",
       "\n",
       "         cum_volume    cum_amount  prev_close       open       high  \\\n",
       "skey                                                                  \n",
       "1000016  10180068.0  1.340949e+10   2552.9754  2543.7865  2548.1345   \n",
       "1000300  45001697.0  5.756643e+10   3668.8279  3654.7348  3660.0172   \n",
       "1000852  34000656.0  3.877080e+10   7456.6719  7435.7002  7443.7011   \n",
       "1000905  28273982.0  3.515648e+10   6116.9932  6101.3648  6110.7696   \n",
       "\n",
       "               low      close  \n",
       "skey                           \n",
       "1000016  2533.8595  2542.5826  \n",
       "1000300  3646.2272  3658.4181  \n",
       "1000852  7416.4484  7440.0605  \n",
       "1000905  6090.2523  6110.0707  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep='first').groupby('skey').last()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

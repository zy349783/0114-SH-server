{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "20200102\n",
      "SH lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.52%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------\n",
      "SZ lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8598613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8449796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8598613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8598613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8449796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ lv2 is complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.73%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "SH & SZ trade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trade data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.05%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------\n",
      "SZ order data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'order data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.02%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "SH index data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "809335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "809335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "809335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16382"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index data is complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:590: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "final concat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>sequenceNo</th>\n",
       "      <th>exchId</th>\n",
       "      <th>securityType</th>\n",
       "      <th>__isRepeated</th>\n",
       "      <th>TransactTime</th>\n",
       "      <th>ChannelNo</th>\n",
       "      <th>ApplSeqNum</th>\n",
       "      <th>SecurityID</th>\n",
       "      <th>ExecType</th>\n",
       "      <th>TradeBSFlag</th>\n",
       "      <th>__origTickSeq</th>\n",
       "      <th>TradePrice</th>\n",
       "      <th>TradeQty</th>\n",
       "      <th>TradeMoney</th>\n",
       "      <th>BidApplSeqNum</th>\n",
       "      <th>OfferApplSeqNum</th>\n",
       "      <th>skey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47567254</th>\n",
       "      <td>1577948140458219</td>\n",
       "      <td>112176837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145533490</td>\n",
       "      <td>2012</td>\n",
       "      <td>16833947</td>\n",
       "      <td>651</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>172595140</td>\n",
       "      <td>679200</td>\n",
       "      <td>200</td>\n",
       "      <td>135840000</td>\n",
       "      <td>16825092</td>\n",
       "      <td>16833945</td>\n",
       "      <td>2000651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            clockAtArrival  sequenceNo  exchId  securityType  __isRepeated  \\\n",
       "47567254  1577948140458219   112176837       2             1             0   \n",
       "\n",
       "          TransactTime  ChannelNo  ApplSeqNum  SecurityID ExecType  \\\n",
       "47567254     145533490       2012    16833947         651        F   \n",
       "\n",
       "         TradeBSFlag  __origTickSeq  TradePrice  TradeQty  TradeMoney  \\\n",
       "47567254           N      172595140      679200       200   135840000   \n",
       "\n",
       "          BidApplSeqNum  OfferApplSeqNum     skey  \n",
       "47567254       16825092         16833945  2000651  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>sequenceNo</th>\n",
       "      <th>exchId</th>\n",
       "      <th>securityType</th>\n",
       "      <th>__isRepeated</th>\n",
       "      <th>TransactTime</th>\n",
       "      <th>ChannelNo</th>\n",
       "      <th>ApplSeqNum</th>\n",
       "      <th>SecurityID</th>\n",
       "      <th>Side</th>\n",
       "      <th>OrderType</th>\n",
       "      <th>__origTickSeq</th>\n",
       "      <th>Price</th>\n",
       "      <th>OrderQty</th>\n",
       "      <th>skey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35584127</th>\n",
       "      <td>1577948140458224</td>\n",
       "      <td>112176837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145533490</td>\n",
       "      <td>2011</td>\n",
       "      <td>16864635</td>\n",
       "      <td>300092</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>172595142</td>\n",
       "      <td>73500</td>\n",
       "      <td>3600</td>\n",
       "      <td>2300092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            clockAtArrival  sequenceNo  exchId  securityType  __isRepeated  \\\n",
       "35584127  1577948140458224   112176837       2             1             0   \n",
       "\n",
       "          TransactTime  ChannelNo  ApplSeqNum  SecurityID  Side OrderType  \\\n",
       "35584127     145533490       2011    16864635      300092     2         2   \n",
       "\n",
       "          __origTickSeq  Price  OrderQty     skey  \n",
       "35584127      172595142  73500      3600  2300092  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:33.103485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "99604124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "99604183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:776: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99604183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skey</th>\n",
       "      <th>date</th>\n",
       "      <th>num</th>\n",
       "      <th>sequenceNo</th>\n",
       "      <th>seq1</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>nan</th>\n",
       "      <th>count</th>\n",
       "      <th>tag</th>\n",
       "      <th>dup1</th>\n",
       "      <th>seq2</th>\n",
       "      <th>ApplSeqNum</th>\n",
       "      <th>sum_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34617486</th>\n",
       "      <td>2000651</td>\n",
       "      <td>20200102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112211382.0</td>\n",
       "      <td>112176837.0</td>\n",
       "      <td>1.577948e+15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>trade</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19624752</td>\n",
       "      <td>16833947.0</td>\n",
       "      <td>34545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89951872</th>\n",
       "      <td>2300092</td>\n",
       "      <td>20200102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112211382.0</td>\n",
       "      <td>112176837.0</td>\n",
       "      <td>1.577948e+15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>order</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26529981</td>\n",
       "      <td>16864635.0</td>\n",
       "      <td>34545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             skey        date  num   sequenceNo         seq1  clockAtArrival  \\\n",
       "34617486  2000651  20200102.0  NaN  112211382.0  112176837.0    1.577948e+15   \n",
       "89951872  2300092  20200102.0  NaN  112211382.0  112176837.0    1.577948e+15   \n",
       "\n",
       "          nan  count    tag  dup1      seq2  ApplSeqNum  sum_nan  \n",
       "34617486    0    0.0  trade   1.0  19624752  16833947.0    34545  \n",
       "89951872    0    0.0  order   1.0  26529981  16864635.0    34545  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200102finished\n",
      "--------------------------------------------------------------------------------------------\n",
      "20200103\n",
      "SH lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.65%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------\n",
      "SZ lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8454191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8285615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8454191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8454191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8285615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ lv2 is complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.99%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "SH & SZ trade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trade data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.06%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------\n",
      "SZ order data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'order data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.02%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "SH index data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "909046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "909046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "909046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16365"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index data is complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:590: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:591: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "final concat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:44.869803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "97651359"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "97651405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:776: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97651405"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200103finished\n",
      "--------------------------------------------------------------------------------------------\n",
      "20200106\n",
      "SH lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.28%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------\n",
      "SZ lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8807962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8583363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8807962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8807962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8583363"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ lv2 is complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.55%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "SH & SZ trade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trade data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.29%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------\n",
      "SZ order data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'order data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.09%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "SH index data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "388024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "388024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "388024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14267"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index data is complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "final concat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'3. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'4. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5. here~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:03.721825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "109360756"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "109360813"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "109360813"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200106finished\n",
      "--------------------------------------------------------------------------------------------\n",
      "20200107\n",
      "SH lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.56%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------\n",
      "SZ lv2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8623392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8455169"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8623392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8623392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8455169"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ lv2 is complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.95%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "SH & SZ trade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trade data is complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "startDate = 20200102\n",
    "endDate = 20200529\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db = DB(\"192.168.10.178\", database_name, user, password)\n",
    "test = db.read('md_index', start_date=startDate, end_date=endDate, symbol=[1000300])\n",
    "date_list = test['date'].unique()\n",
    "del test\n",
    "\n",
    "new_trade_data = []\n",
    "new_order_data = []\n",
    "\n",
    "for i in date_list:\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print(i)\n",
    "    print('SH lv2')\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "    db = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    SH = db.read('md_snapshot_l2', start_date=startDate, end_date=endDate)\n",
    "    SZ = SH[SH['skey'] > 2000000]\n",
    "    SH = SH[SH['skey'] < 2000000]\n",
    "    SH['num'] = SH['skey'] * 10000 + SH['ordering']\n",
    "    SZ['num'] = SZ['skey'] * 10000 + SZ['ordering']\n",
    "    \n",
    "    SH = SH[['date', 'skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\", 'num']]\n",
    "    SZ = SZ[['date', 'skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\", 'num']]\n",
    "\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "\n",
    "    readPath = '/mnt/e/result/logs_***_zs_92_01_day_data'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "\n",
    "    for n in range(len(dataPathLs)):\n",
    "        path1 = np.array(glob.glob(dataPathLs[n] + '/mdLog_SH_***'))\n",
    "        SH1 = pd.read_csv(path1[0])\n",
    "        index1 = SH1[SH1['StockID'].isin([16, 300, 852, 905])]\n",
    "        SH1 = SH1[SH1['source'] == 4]\n",
    "\n",
    "        SH1['skey'] = SH1['StockID'] + 1000000\n",
    "        SH1 = SH1.rename(columns={\"openPrice\":\"open\"})\n",
    "        SH1[\"open\"] = np.where(SH1[\"cum_volume\"] > 0, SH1.groupby(\"skey\")[\"open\"].transform(\"max\"), SH1[\"open\"])\n",
    "        SH1[\"time\"] = SH1[\"time\"].apply(lambda x: int((x.replace(':', \"\")).replace(\".\", \"\")) * 1000)\n",
    "\n",
    "    SH1 = SH1[['clockAtArrival', 'sequenceNo', 'skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\"]]\n",
    "    for cols in ['cum_amount', \"close\", 'open']:\n",
    "        SH1[cols] = SH1[cols].round(2)\n",
    "    cols = ['skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\"]\n",
    "    SH1 = SH1[SH1['skey'].isin(SH['skey'].unique())]\n",
    "    re = pd.merge(SH, SH1, on=cols, how='outer')\n",
    "\n",
    "    p21 = re[(re['date'].isnull())][['clockAtArrival', 'sequenceNo', 'skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\"]]\n",
    "    p22 = re[(re['sequenceNo'].isnull())][[\"skey\", \"date\", \"time\", 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\", 'num']]\n",
    "\n",
    "    p11 = re[(~re['sequenceNo'].isnull()) & (~re['date'].isnull())][re[(~re['sequenceNo'].isnull()) \n",
    "                                                        & (~re['date'].isnull())]['num'].duplicated(keep=False)]\n",
    "    p12 = re[(~re['sequenceNo'].isnull()) & (~re['date'].isnull())].drop_duplicates(['num'], keep=False)\n",
    "    p11 = p11.sort_values(by=['num', 'sequenceNo'])\n",
    "    p11[\"order1\"] = p11.groupby([\"num\"]).cumcount()\n",
    "    p11[\"order2\"] = p11.groupby([\"sequenceNo\"]).cumcount()\n",
    "    p11 = p11[p11['order1'] == p11['order2']]\n",
    "    p11.drop(['order1', 'order2'],axis=1,inplace=True)\n",
    "    p1 = pd.concat([p11, p12])\n",
    "    p2 = pd.merge(p22, p21[['skey', 'time', 'clockAtArrival', 'sequenceNo']], on=['skey', 'time'], how='left')\n",
    "    re1 = pd.concat([p1, p2])\n",
    "    re1 = re1.sort_values(by='num')\n",
    "    re1['seq1'] = re1.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "    sl = list(set(SH['skey'].unique()) - set(SH1['skey'].unique()))\n",
    "    re1.loc[re1['skey'].isin(sl), 'seq1'] = np.nan\n",
    "    re1['count1'] = re1.groupby(['seq1']).cumcount()\n",
    "    re1['count2'] = re1.groupby(['seq1'])['count1'].transform('nunique')\n",
    "    re1['max_seq'] = re1.groupby('skey')['sequenceNo'].transform('max')\n",
    "    re1['count'] = np.where((re1['seq1'] != re1['max_seq']) | (~re1['sequenceNo'].isnull()), re1['count1'] + 1 - re1['count2'], re1['count1'] - re1['count2'])\n",
    "    re1.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "    re1.drop([\"count1\"],axis=1,inplace=True)\n",
    "    re1.drop([\"count2\"],axis=1,inplace=True)\n",
    "    re1['dup'] = np.where(~re1[\"sequenceNo\"].isnull(), re1.groupby(['sequenceNo']).cumcount(), 0)\n",
    "    re1['dup1'] = np.where(~re1[\"sequenceNo\"].isnull(), re1.groupby(['sequenceNo'])['num'].transform('nunique'), 0)\n",
    "    re1['nan'] = np.where((re1['sequenceNo'].isnull()) | (re1['dup'] != 0), 1, 0)\n",
    "    re1.loc[(re1['dup1'] > 1) & (re1['count'] < 0), 'sequenceNo'] = np.nan\n",
    "    assert((len(set(sl) - set(re1[re1['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "               (len(set(re1[re1['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "    assert(re1.shape[0] == SH.shape[0])\n",
    "\n",
    "    display('%.2f%%' % (re1[re1['sequenceNo'].isnull()].shape[0]/re1.shape[0] * 100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('-------------------------------------------------------------------------------------------')\n",
    "    print('SZ lv2')\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "\n",
    "    readPath = '/mnt/e/result/logs_***_zs_92_01_day_data'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "\n",
    "    for n in range(len(dataPathLs)):\n",
    "        path1 = np.array(glob.glob(dataPathLs[n] + '/mdLog_SZ_***'))\n",
    "        SZ1 = pd.read_csv(path1[0])\n",
    "        SZ1 = SZ1[SZ1['source'] == 4]\n",
    "\n",
    "        SZ1['skey'] = SZ1['StockID'] + 2000000\n",
    "        SZ1 = SZ1.rename(columns={\"openPrice\":\"open\"})\n",
    "        SZ1[\"open\"] = np.where(SZ1[\"cum_volume\"] > 0, SZ1.groupby(\"skey\")[\"open\"].transform(\"max\"), SZ1[\"open\"])\n",
    "        SZ1[\"time\"] = SZ1[\"time\"].apply(lambda x: int((x.replace(':', \"\")).replace(\".\", \"\")) * 1000)\n",
    "\n",
    "    SZ1 = SZ1[['clockAtArrival', 'sequenceNo', 'skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\"]]\n",
    "    for cols in ['cum_amount']:\n",
    "        SZ1[cols] = SZ1[cols].round(2)\n",
    "    cols = ['skey', 'time', 'cum_volume', 'cum_amount', \"close\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid1q\", \"bid2q\",\n",
    "               \"bid3q\", \"bid4q\", \"bid5q\", \"ask1p\", \"ask2p\", \"ask3p\", \"ask4p\", \"ask5p\", \"ask1q\", \"ask2q\", \"ask3q\",\n",
    "               \"ask4q\", \"ask5q\", \"open\"]\n",
    "    SZ1 = SZ1[SZ1['skey'].isin(SZ['skey'].unique())]\n",
    "    re = pd.merge(SZ, SZ1, on=cols, how='outer')\n",
    "\n",
    "    display(re.shape[0])\n",
    "    display(re[~re['sequenceNo'].isnull()].shape[0])\n",
    "    display(re[~re['date'].isnull()].shape[0])\n",
    "    display(SZ.shape[0])\n",
    "    display(SZ1.shape[0])\n",
    "\n",
    "    try:\n",
    "        assert(re.shape[0] == re[~re['date'].isnull()].shape[0])\n",
    "        print('SZ lv2 is complete')\n",
    "    except:\n",
    "        display('%.2f%%' % (re[~re['date'].isnull()].shape[0]/re.shape[0] * 100))\n",
    "        print('92 have unique values not shared by database')\n",
    "        re = pd.merge(SZ, SZ1, on=cols, how='left')\n",
    "\n",
    "    if re[re.duplicated('num', keep=False)].shape[0] == 0:\n",
    "        re2 = re.sort_values(by='num')\n",
    "        re2['seq1'] = re2.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "        sl = list(set(SZ['skey'].unique()) - set(SZ1['skey'].unique()))\n",
    "        re2.loc[re2['skey'].isin(sl), 'seq1'] = np.nan\n",
    "        re2['count1'] = re2.groupby(['seq1']).cumcount()\n",
    "        re2['count2'] = re2.groupby(['seq1'])['count1'].transform('nunique')\n",
    "        re2['max_seq'] = re2.groupby('skey')['sequenceNo'].transform('max')\n",
    "        re2['count'] = np.where((re2['seq1'] != re2['max_seq'])|(~re2[\"sequenceNo\"].isnull()), re2['count1'] + 1 - re2['count2'], re2['count1'] - re2['count2'])\n",
    "        re2.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "        re2.drop([\"count1\"],axis=1,inplace=True)\n",
    "        re2.drop([\"count2\"],axis=1,inplace=True)\n",
    "        re2['dup'] = np.where(~re2[\"sequenceNo\"].isnull(), re2.groupby(['sequenceNo']).cumcount(), 0)\n",
    "        re2['dup1'] = np.where(~re2[\"sequenceNo\"].isnull(), re2.groupby(['sequenceNo'])['num'].transform('nunique'), 0)\n",
    "        re2['nan'] = np.where((re2['sequenceNo'].isnull()) | (re2['dup'] != 0), 1, 0)\n",
    "        re2.loc[(re2['dup1'] > 1) & (re2['count'] < 0), 'sequenceNo'] = np.nan\n",
    "        assert((len(set(sl) - set(re2[re2['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "               (len(set(re2[re2['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "        assert(re2.shape[0] == SZ.shape[0])\n",
    "\n",
    "        display('%.2f%%' % (re2[re2['sequenceNo'].isnull()].shape[0]/re2.shape[0] * 100))\n",
    "\n",
    "\n",
    "    else:\n",
    "        p1 = re[re['num'].duplicated(keep=False)]\n",
    "        p2 = re.drop_duplicates(['num'], keep=False)\n",
    "        p1[\"order1\"] = p1.groupby([\"num\"]).cumcount()\n",
    "        p1[\"order2\"] = p1.groupby([\"sequenceNo\"]).cumcount()\n",
    "        p1 = p1[p1['order1'] == p1['order2']]\n",
    "        p1.drop(['order1', 'order2'],axis=1,inplace=True)\n",
    "        re = pd.concat([p1, p2])\n",
    "        re2 = re.sort_values(by='num')\n",
    "        re2['seq1'] = re2.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "        sl = list(set(SZ['skey'].unique()) - set(SZ1['skey'].unique()))\n",
    "        re2.loc[re2['skey'].isin(sl), 'seq1'] = np.nan\n",
    "        re2['count1'] = re2.groupby(['seq1']).cumcount()\n",
    "        re2['count2'] = re2.groupby(['seq1'])['count1'].transform('nunique')\n",
    "        re2['max_seq'] = re2.groupby('skey')['sequenceNo'].transform('max')\n",
    "        re2['count'] = np.where((re2['seq1'] != re2['max_seq'])|(~re2[\"sequenceNo\"].isnull()), re2['count1'] + 1 - re2['count2'], re2['count1'] - re2['count2'])\n",
    "        re2.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "        re2.drop([\"count1\"],axis=1,inplace=True)\n",
    "        re2.drop([\"count2\"],axis=1,inplace=True)\n",
    "        re2['dup'] = np.where(~re2[\"sequenceNo\"].isnull(), re2.groupby(['sequenceNo']).cumcount(), 0)\n",
    "        re2['dup1'] = np.where(~re2[\"sequenceNo\"].isnull(), re2.groupby(['sequenceNo'])['num'].transform('nunique'), 0)\n",
    "        re2['nan'] = np.where((re2['sequenceNo'].isnull()) | (re2['dup'] != 0), 1, 0)\n",
    "        re2.loc[(re2['dup1'] > 1) & (re2['count'] < 0), 'sequenceNo'] = np.nan\n",
    "        assert((len(set(sl) - set(re2[re2['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "               (len(set(re2[re2['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "        assert(re2.shape[0] == SZ.shape[0])\n",
    "\n",
    "        display('%.2f%%' % (re2[re2['sequenceNo'].isnull()].shape[0]/re2.shape[0] * 100))\n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------------')\n",
    "    print('SH & SZ trade')\n",
    "    \n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    trade = db.read('md_trade', start_date=startDate, end_date=endDate)[['skey', 'date', 'ApplSeqNum']]\n",
    "\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "\n",
    "    readPath = '/mnt/e/result/logs_***_zs_92_01_day_data'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    for n in range(len(dataPathLs)):\n",
    "        path1 = np.array(glob.glob(dataPathLs[n] + '/mdTradeLog***'))\n",
    "        trade1 = pd.read_csv(path1[0])\n",
    "    trade1['skey'] = np.where(trade1['exchId'] == 2, trade1['SecurityID'] + 2000000, trade1['SecurityID'] + 1000000)\n",
    "    trade1 = trade1[trade1['skey'].isin(trade['skey'].unique())]\n",
    "    re = pd.merge(trade, trade1[['skey', 'ApplSeqNum', 'sequenceNo', 'clockAtArrival']], on=['skey', 'ApplSeqNum'],\n",
    "                 how='outer')\n",
    "    try:\n",
    "        assert(re.shape[0] == trade.shape[0])\n",
    "        display('trade data is complete')\n",
    "        k = 0\n",
    "    except:\n",
    "        display('%.2f%%' % (trade.shape[0]/re.shape[0] * 100))\n",
    "        k = 1\n",
    "        display('trade data incomplete')\n",
    "        k1 = pd.merge(trade1, re[re['date'].isnull()][['skey', 'ApplSeqNum']], on=['skey', 'ApplSeqNum'], how='right')\n",
    "        display(k1.shape[0])\n",
    "        display(k1['ExecType'].unique())\n",
    "        display(k1['TransactTime'].unique())\n",
    "        k1['date'] = trade['date'].iloc[0]\n",
    "        new_trade_data += [k1[['clockAtArrival', 'sequenceNo', 'TransactTime', 'ApplSeqNum', 'date', 'skey', 'ExecType', 'TradeBSFlag', \n",
    "       'TradePrice', 'TradeQty', 'BidApplSeqNum', 'OfferApplSeqNum']]]\n",
    "        re = pd.merge(trade, trade1[['skey', 'ApplSeqNum', 'sequenceNo', 'clockAtArrival']], on=['skey', 'ApplSeqNum'],\n",
    "                 how='left')\n",
    "        assert(re.shape[0] == trade.shape[0])\n",
    "\n",
    "    re3 = re.sort_values(by=['skey', 'ApplSeqNum'])\n",
    "    re3['seq1'] = re3.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "    sl = list(set(trade['skey'].unique()) - set(trade1['skey'].unique()))\n",
    "    re3.loc[re3['skey'].isin(sl), 'seq1'] = np.nan\n",
    "    re3['count1'] = re3.groupby(['seq1']).cumcount()\n",
    "    re3['count2'] = re3.groupby(['seq1'])['count1'].transform('nunique')\n",
    "    re3['max_seq'] = re3.groupby('skey')['sequenceNo'].transform('max')\n",
    "    re3['count'] = np.where((re3['seq1'] != re3['max_seq'])|(~re3['sequenceNo'].isnull()), re3['count1'] + 1 - re3['count2'], re3['count1'] - re3['count2'])\n",
    "    re3.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "    re3.drop([\"count1\"],axis=1,inplace=True)\n",
    "    re3.drop([\"count2\"],axis=1,inplace=True)\n",
    "    re3['dup'] = np.where(~re3[\"sequenceNo\"].isnull(), re3.groupby(['sequenceNo']).cumcount(), 0)\n",
    "    re3['dup1'] = np.where(~re3[\"sequenceNo\"].isnull(), re3.groupby(['sequenceNo'])['ApplSeqNum'].transform('nunique'), 0)\n",
    "    re3['nan'] = np.where((re3['sequenceNo'].isnull()) | (re3['dup'] != 0), 1, 0)\n",
    "    re3.loc[(re3['dup1'] > 1) & (re3['count'] < 0), 'sequenceNo'] = np.nan\n",
    "    assert((len(set(sl) - set(re3[re3['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "           (len(set(re3[re3['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "    assert(re3.shape[0] == trade.shape[0])\n",
    "    if k == 1:\n",
    "        k1['seq1'] = k1['sequenceNo']\n",
    "        k1['count'] = 0\n",
    "        k1['nan'] = 0\n",
    "        k1['dup1'] = 1\n",
    "        re3 = pd.concat([re3, k1[['clockAtArrival', 'date', 'sequenceNo', 'skey', 'ApplSeqNum', 'seq1', \n",
    "                                  'count', 'nan', 'dup1']]])\n",
    "\n",
    "    display('%.2f%%' % (re3[re3['sequenceNo'].isnull()].shape[0]/re3.shape[0] * 100))\n",
    "\n",
    "    \n",
    "    print('--------------------------------------------------------------------------------------------------')\n",
    "    print('SZ order data')\n",
    "\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    order = db.read('md_order', start_date=startDate, end_date=endDate)[['skey', 'date', 'ApplSeqNum']]\n",
    "\n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "\n",
    "    readPath = '/mnt/e/result/logs_***_zs_92_01_day_data'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    for n in range(len(dataPathLs)):\n",
    "        path1 = np.array(glob.glob(dataPathLs[n] + '/mdOrderLog***'))\n",
    "        order1 = pd.read_csv(path1[0])\n",
    "    order1['skey'] = order1['SecurityID'] + 2000000\n",
    "    order1 = order1[order1['skey'].isin(order['skey'].unique())]\n",
    "    re = pd.merge(order, order1[['skey', 'ApplSeqNum', 'sequenceNo', 'clockAtArrival']], on=['skey', 'ApplSeqNum'],\n",
    "                 how='outer')\n",
    "    try:\n",
    "        assert(re.shape[0] == order.shape[0])\n",
    "        display('order data is complete')\n",
    "        k = 0\n",
    "    except:\n",
    "        display('%.2f%%' % (order.shape[0]/re.shape[0] * 100))\n",
    "        k = 1\n",
    "        display('order data incomplete')\n",
    "        k2 = pd.merge(order1, re[re['date'].isnull()][['skey', 'ApplSeqNum']], on=['skey', 'ApplSeqNum'], how='right')\n",
    "        display(k2.shape[0])\n",
    "        display(k2['SecurityID'].unique())\n",
    "        display(k2['TransactTime'].unique())\n",
    "        k2['date'] = order['date'].iloc[0]\n",
    "        new_order_data += [k2[['clockAtArrival', 'sequenceNo', 'TransactTime', 'ApplSeqNum', 'date', 'skey', 'Side', \n",
    "       'OrderType', 'Price', 'OrderQty']]]\n",
    "        re = pd.merge(order, order1[['skey', 'ApplSeqNum', 'sequenceNo', 'clockAtArrival']], on=['skey', 'ApplSeqNum'],\n",
    "                 how='left')\n",
    "        assert(re.shape[0] == order.shape[0])\n",
    "\n",
    "    re4 = re.sort_values(by=['skey', 'ApplSeqNum'])\n",
    "    re4['seq1'] = re4.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "    sl = list(set(order['skey'].unique()) - set(order1['skey'].unique()))\n",
    "    re4.loc[re4['skey'].isin(sl), 'seq1'] = np.nan\n",
    "    re4['count1'] = re4.groupby(['seq1']).cumcount()\n",
    "    re4['count2'] = re4.groupby(['seq1'])['count1'].transform('nunique')\n",
    "    re4['max_seq'] = re4.groupby('skey')['sequenceNo'].transform('max')\n",
    "    re4['count'] = np.where((re4['seq1'] != re4['max_seq'])|(~re4['sequenceNo'].isnull()), re4['count1'] + 1 - re4['count2'], re4['count1'] - re4['count2'])\n",
    "    re4.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "    re4.drop([\"count1\"],axis=1,inplace=True)\n",
    "    re4.drop([\"count2\"],axis=1,inplace=True)\n",
    "    re4['dup'] = np.where(~re4[\"sequenceNo\"].isnull(), re4.groupby(['sequenceNo']).cumcount(), 0)\n",
    "    re4['dup1'] = np.where(~re4[\"sequenceNo\"].isnull(), re4.groupby(['sequenceNo'])['ApplSeqNum'].transform('nunique'), 0)\n",
    "    re4['nan'] = np.where((re4['sequenceNo'].isnull()) | (re4['dup'] != 0), 1, 0)\n",
    "    re4.loc[(re4['dup1'] > 1) & (re4['count'] < 0), 'sequenceNo'] = np.nan\n",
    "    assert((len(set(sl) - set(re4[re4['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "           (len(set(re4[re4['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "    assert(re4.shape[0] == order.shape[0])\n",
    "    if k == 1:\n",
    "        k2['seq1'] = k2['ApplSeqNum']\n",
    "        k2['count'] = 0\n",
    "        k2['nan'] = 0\n",
    "        k2['dup1'] = 1\n",
    "        re4 = pd.concat([re4, k2[['clockAtArrival', 'date', 'sequenceNo', 'skey', 'ApplSeqNum', 'seq1', \n",
    "                                  'count', 'nan', \"dup1\"]]])\n",
    "\n",
    "\n",
    "    display('%.2f%%' % (re4[re4['sequenceNo'].isnull()].shape[0]/re4.shape[0] * 100))\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    print('SH index data')\n",
    "    \n",
    "    startDate = str(i)\n",
    "    endDate = str(i)\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    index = db.read('md_index', start_date=startDate, end_date=endDate)\n",
    "\n",
    "    index1['skey'] = index1['StockID'] + 1000000\n",
    "    index1 = index1.rename(columns={\"openPrice\":\"open\"})\n",
    "    index1[\"open\"] = np.where(index1[\"cum_volume\"] > 0, index1.groupby(\"skey\")[\"open\"].transform(\"max\"), index1[\"open\"])\n",
    "    index1['close'] = np.where(index1['cum_volume'] == 0, 0, index1['close'])\n",
    "    index1[\"time\"] = index1[\"time\"].apply(lambda x: int((x.replace(':', \"\")).replace(\".\", \"\")) * 1000)\n",
    "    index['close'] = np.where(index['cum_volume'] == 0, 0, index['close'])\n",
    "    index['num'] = index['skey'] * 10000 + index['ordering']\n",
    "    index = index[['skey', 'date', 'cum_volume', 'cum_amount', \"close\", \"open\", 'num']]\n",
    "    index1 = index1[['clockAtArrival', 'sequenceNo', 'skey', 'cum_volume', 'cum_amount', \"close\", \"open\", \"time\"]]\n",
    "    for cols in ['cum_amount']:\n",
    "        index1[cols] = index1[cols].round(1)\n",
    "    cols = ['skey', 'cum_volume', 'cum_amount', \"close\", \"open\"]\n",
    "    index1 = index1[index1['skey'].isin(index['skey'].unique())]\n",
    "    re = pd.merge(index, index1, on=cols, how='outer')\n",
    "\n",
    "    display(re.shape[0])\n",
    "    display(re[~re['sequenceNo'].isnull()].shape[0])\n",
    "    display(re[~re['date'].isnull()].shape[0])\n",
    "    display(index.shape[0])\n",
    "    display(index1.shape[0])\n",
    "    \n",
    "    try:\n",
    "        assert(re.shape[0] == re[~re['date'].isnull()].shape[0])\n",
    "        print('index data is complete')\n",
    "    except:\n",
    "        display('%.2f%%' % (re[~re['date'].isnull()].shape[0]/re.shape[0] * 100))\n",
    "        re = pd.merge(index, index1, on=cols, how='left')\n",
    "        print('92 have unique values not shared by database')\n",
    "\n",
    "    p11 = re[re.duplicated('num', keep=False)]\n",
    "    p2 = re.drop_duplicates('num', keep=False)\n",
    "    p11[\"order1\"] = p11.groupby([\"num\"]).cumcount()\n",
    "    p11[\"order2\"] = p11.groupby([\"sequenceNo\"]).cumcount()\n",
    "    p11 = p11[p11['order1'] == p11['order2']]\n",
    "\n",
    "    p12 = re[re.duplicated('num', keep=False)].drop_duplicates('num')\n",
    "    p12 = pd.merge(p12, p11[['num', 'order1']], on='num', how='left')\n",
    "    p12 = p12[p12['order1'].isnull()]\n",
    "    p12['sequenceNo'] = np.nan\n",
    "    p12['clockAtArrival'] = np.nan\n",
    "\n",
    "    p11.drop(['order1', 'order2'],axis=1,inplace=True)\n",
    "    p12.drop(['order1'],axis=1,inplace=True)\n",
    "    p1 = pd.concat([p11, p12])\n",
    "\n",
    "    re = pd.concat([p1, p2])\n",
    "    assert(re[re.duplicated('num', keep=False)].shape[0] == 0)\n",
    "\n",
    "    if re[re['sequenceNo'].isnull()].shape[0] != 0:\n",
    "        re5 = re.sort_values(by='num')\n",
    "        re5['seq1'] = re5.groupby('skey')['sequenceNo'].bfill().ffill()\n",
    "        sl = list(set(index['skey'].unique()) - set(index1['skey'].unique()))\n",
    "        re5.loc[re5['skey'].isin(sl), 'seq1'] = np.nan\n",
    "        re5['count1'] = re5.groupby(['seq1']).cumcount()\n",
    "        re5['count2'] = re5.groupby(['seq1'])['count1'].transform('nunique')\n",
    "        re5['max_seq'] = re5.groupby('skey')['sequenceNo'].transform('max')\n",
    "        re5['count'] = np.where((re5['seq1'] != re5['max_seq'])|(~re5['sequenceNo'].isnull()), re5['count1'] + 1 - re5['count2'], re5['count1'] - re5['count2'])\n",
    "        re5.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "        re5.drop([\"count1\"],axis=1,inplace=True)\n",
    "        re5.drop([\"count2\"],axis=1,inplace=True)\n",
    "        re5['dup'] = np.where(~re5[\"sequenceNo\"].isnull(), re5.groupby(['sequenceNo']).cumcount(), 0)\n",
    "        re5['dup1'] = np.where(~re5[\"sequenceNo\"].isnull(), re5.groupby(['sequenceNo'])['num'].transform('nunique'), 0)\n",
    "        re5['nan'] = np.where((re5['sequenceNo'].isnull()) | (re5['dup'] != 0), 1, 0)\n",
    "        re5.loc[(re5['dup1'] > 1) & (re5['count'] < 0), 'sequenceNo'] = np.nan\n",
    "        assert((len(set(sl) - set(re5[re5['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "               (len(set(re5[re5['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "        assert(re5.shape[0] == index.shape[0])\n",
    "\n",
    "        display('%.0f%%' % (re5[re5['sequenceNo'].isnull()].shape[0]/re5.shape[0] * 100))\n",
    "    else:\n",
    "        re5 = re.sort_values(by='num')\n",
    "        re5['seq1'] = re5['sequenceNo']\n",
    "        sl = list(set(index['skey'].unique()) - set(index1['skey'].unique()))\n",
    "        re5.loc[re5['skey'].isin(sl), 'seq1'] = np.nan\n",
    "        re5['count1'] = re5.groupby(['seq1']).cumcount()\n",
    "        re5['count2'] = re5.groupby(['seq1'])['count1'].transform('nunique')\n",
    "        re5['max_seq'] = re5.groupby('skey')['sequenceNo'].transform('max')\n",
    "        re5['count'] = np.where((re5['seq1'] != re5['max_seq'])|(~re5['sequenceNo'].isnull()), re5['count1'] + 1 - re5['count2'], re5['count1'] - re5['count2'])\n",
    "        re5.drop([\"max_seq\"],axis=1,inplace=True)\n",
    "        re5.drop([\"count1\"],axis=1,inplace=True)\n",
    "        re5.drop([\"count2\"],axis=1,inplace=True)\n",
    "        re5['dup'] = np.where(~re5[\"sequenceNo\"].isnull(), re5.groupby(['sequenceNo']).cumcount(), 0)\n",
    "        re5['dup1'] = np.where(~re5[\"sequenceNo\"].isnull(), re5.groupby(['sequenceNo'])['num'].transform('nunique'), 0)\n",
    "        re5['nan'] = np.where((re5['sequenceNo'].isnull()) | (re5['dup'] != 0), 1, 0)\n",
    "        re5.loc[(re5['dup1'] > 1) & (re5['count'] < 0), 'sequenceNo'] = np.nan\n",
    "        assert((len(set(sl) - set(re5[re5['seq1'].isnull()]['skey'].unique())) == 0) & \n",
    "               (len(set(re5[re5['seq1'].isnull()]['skey'].unique()) - set(sl)) == 0))\n",
    "        assert(re5.shape[0] == index.shape[0])\n",
    "\n",
    "        display('%.0f%%' % (re5[re5['sequenceNo'].isnull()].shape[0]/re5.shape[0] * 100))\n",
    "    \n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------------------')\n",
    "    print('final concat')\n",
    "    try:\n",
    "        assert(len(set(SZ1['sequenceNo']) & set(SH1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SZ1[SZ1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(SH1['sequenceNo'])))])\n",
    "        display(SH1[SH1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(SH1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SZ1['sequenceNo']) & set(trade1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SZ1[SZ1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(trade1['sequenceNo'])))])\n",
    "        display(trade1[trade1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(trade1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SZ1['sequenceNo']) & set(order1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SZ1[SZ1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "        display(order1[order1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SZ1['sequenceNo']) & set(index1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SZ1[SZ1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "        display(index1[index1['sequenceNo'].isin(list(set(SZ1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SH1['sequenceNo']) & set(index1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SH1[SH1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "        display(index1[index1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SH1['sequenceNo']) & set(trade1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SH1[SH1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(trade1['sequenceNo'])))])\n",
    "        display(trade1[trade1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(trade1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(SH1['sequenceNo']) & set(order1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(SH1[SH1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "        display(order1[order1['sequenceNo'].isin(list(set(SH1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(trade1['sequenceNo']) & set(order1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(trade1[trade1['sequenceNo'].isin(list(set(trade1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "        display(order1[order1['sequenceNo'].isin(list(set(trade1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(trade1['sequenceNo']) & set(index1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(trade1[trade1['sequenceNo'].isin(list(set(trade1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "        display(index1[index1['sequenceNo'].isin(list(set(trade1['sequenceNo']) & set(index1['sequenceNo'])))])\n",
    "    try:\n",
    "        assert(len(set(index1['sequenceNo']) & set(order1['sequenceNo'])) == 0)\n",
    "    except:\n",
    "        display(index1[index1['sequenceNo'].isin(list(set(index1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "        display(order1[order1['sequenceNo'].isin(list(set(index1['sequenceNo']) & set(order1['sequenceNo'])))])\n",
    "\n",
    "    del SH\n",
    "    del SH1\n",
    "    del SZ\n",
    "    del SZ1\n",
    "    del trade\n",
    "    del trade1\n",
    "    del order\n",
    "    del order1\n",
    "    del index\n",
    "    del index1\n",
    "    re1['tag'] = 'SH'\n",
    "    re2['tag'] = 'SZ'\n",
    "    re3['tag'] = 'trade'\n",
    "    re4['tag'] = 'order'\n",
    "    re5['tag'] = 'index'\n",
    "    \n",
    "    re1 = re1[['skey', 'date', 'num', 'sequenceNo', 'seq1', 'clockAtArrival', 'nan', 'count', 'tag', 'dup1']]\n",
    "    re2 = re2[['skey', 'date', 'num', 'sequenceNo', 'seq1', 'clockAtArrival', 'nan', 'count', 'tag', 'dup1']]\n",
    "    re3 = re3[['skey', 'date', 'ApplSeqNum', 'sequenceNo', 'seq1', 'clockAtArrival', 'nan', 'count', 'tag', 'dup1']]\n",
    "    re4 = re4[['skey', 'date', 'ApplSeqNum', 'sequenceNo', 'seq1', 'clockAtArrival', 'nan', 'count', 'tag', 'dup1']]\n",
    "    re5 = re5[['skey', 'date', 'num', 'sequenceNo', 'seq1', 'clockAtArrival', 'nan', 'count', 'tag', 'dup1']]\n",
    "    re1 = re1.sort_values(by='num').reset_index(drop=True)\n",
    "    re1['seq2'] = re1.index\n",
    "    re2 = re2.sort_values(by='num').reset_index(drop=True)\n",
    "    re2['seq2'] = re2.index\n",
    "    re3 = re3.sort_values(by=['skey', 'ApplSeqNum']).reset_index(drop=True)\n",
    "    re3['seq2'] = re3.index\n",
    "    re4 = re4.sort_values(by=['skey', 'ApplSeqNum']).reset_index(drop=True)\n",
    "    re4['seq2'] = re4.index\n",
    "    re5 = re5.sort_values(by='num').reset_index(drop=True)\n",
    "    re5['seq2'] = re5.index\n",
    "\n",
    "    fr1 = []\n",
    "    fr2 = []\n",
    "    fr1 += [re1[re1['seq1'].isnull()]]\n",
    "    fr2 += [re1[~re1['seq1'].isnull()]]\n",
    "    del re1\n",
    "    display('1. here~')\n",
    "    fr1 += [re2[re2['seq1'].isnull()]]\n",
    "    fr2 += [re2[~re2['seq1'].isnull()]]\n",
    "    del re2\n",
    "    display('2. here~')\n",
    "    fr1 += [re3[re3['seq1'].isnull()]]\n",
    "    fr2 += [re3[~re3['seq1'].isnull()]]\n",
    "    del re3\n",
    "    display('3. here~')\n",
    "    fr1 += [re4[re4['seq1'].isnull()]]\n",
    "    fr2 += [re4[~re4['seq1'].isnull()]]\n",
    "    del re4\n",
    "    display('4. here~')\n",
    "    fr1 += [re5[re5['seq1'].isnull()]]\n",
    "    fr2 += [re5[~re5['seq1'].isnull()]]\n",
    "    del re5\n",
    "    display('5. here~')\n",
    "    fr1 = pd.concat(fr1).reset_index(drop=True)\n",
    "    fr2 = pd.concat(fr2).reset_index(drop=True)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    fr2 = fr2.sort_values(by=['seq1', 'seq2'])\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    fr2.loc[(fr2['nan']==0) & (fr2['dup1']==1), 'count'] = 0\n",
    "    fr2['sum_nan'] = fr2['nan'].cumsum()\n",
    "    fr2['sequenceNo'] = fr2['sequenceNo'] + fr2['sum_nan']\n",
    "    fr2['sequenceNo'] = fr2['sequenceNo'].bfill()\n",
    "    fr2['sequenceNo'] = fr2['sequenceNo'] + fr2['count']\n",
    "    fr21 = fr2[~fr2['sequenceNo'].isnull()]\n",
    "    fr22 = fr2[fr2['sequenceNo'].isnull()]\n",
    "    display(fr22.shape[0])\n",
    "    display(fr21.shape[0])\n",
    "    display(fr2.shape[0])\n",
    "    if fr22.shape[0] != 0:\n",
    "        fr22['sequenceNo'] = range(int(fr21['sequenceNo'].max()) + 1, int(fr21['sequenceNo'].max()) + 1 + fr22.shape[0])\n",
    "        fr2 = pd.concat([fr21, fr22])\n",
    "    del fr21\n",
    "    del fr22\n",
    "    display(fr2.shape[0])\n",
    "    try:\n",
    "        assert(fr2[fr2.duplicated('sequenceNo', keep=False)].shape[0] == 0)\n",
    "    except:\n",
    "        te_st = fr2[fr2.duplicated('sequenceNo', keep=False)]\n",
    "        display(te_st)\n",
    "        caa = te_st['clockAtArrival'].max()\n",
    "        seq = te_st['sequenceNo'].iloc[0]\n",
    "        m_in = fr2[fr2['sequenceNo'] > seq]['sequenceNo'].min()\n",
    "        if m_in > seq + 1:\n",
    "            fr2.loc[fr2['sequenceNo'] > seq, 'sequenceNo'] = fr2[fr2['sequenceNo'] > seq]['sequenceNo'] + 1\n",
    "            fr2.loc[(fr2['sequenceNo'] == seq) & (fr2['clockAtArrival'] == caa), 'sequenceNo'] = seq + 1\n",
    "        else:\n",
    "            fr2.loc[fr2['sequenceNo'] > seq, 'sequenceNo'] = fr2[fr2['sequenceNo'] > seq]['sequenceNo'] + 2\n",
    "            fr2.loc[(fr2['sequenceNo'] == seq) & (fr2['clockAtArrival'] == caa), 'sequenceNo'] = seq + 1\n",
    "        assert(fr2[fr2.duplicated('sequenceNo', keep=False)].shape[0] == 0)\n",
    "        \n",
    "    \n",
    "    fr1['sequenceNo'] = range(int(fr2['sequenceNo'].max()) + 1, int(fr2['sequenceNo'].max()) + 1 + fr1.shape[0])\n",
    "    fr2 = pd.concat([fr1, fr2])\n",
    "    del fr1\n",
    "    assert(fr2[fr2.duplicated('sequenceNo', keep=False)].shape[0] == 0)\n",
    "    \n",
    "    import pickle\n",
    "    os.mkdir('/mnt/e/result/' + startDate)\n",
    "    SH = fr2[fr2['tag'] == 'SH'][[\"skey\", \"date\", \"num\", 'sequenceNo', \"clockAtArrival\"]]\n",
    "    SH.to_pickle('/mnt/e/result/' + startDate + '/SH.pkl')\n",
    "    del SH\n",
    "\n",
    "    SZ = fr2[fr2['tag'] == 'SZ'][[\"skey\", \"date\", \"num\", 'sequenceNo', \"clockAtArrival\"]]\n",
    "    SZ.to_pickle('/mnt/e/result/' + startDate + '/SZ.pkl')\n",
    "    del SZ\n",
    "    \n",
    "    trade = fr2[fr2['tag'] == 'trade'][[\"skey\", \"date\", \"ApplSeqNum\", 'sequenceNo', \"clockAtArrival\"]]\n",
    "    trade.to_pickle('/mnt/e/result/' + startDate + '/trade.pkl')\n",
    "    del trade\n",
    "    \n",
    "    order = fr2[fr2['tag'] == 'order'][[\"skey\", \"date\", \"ApplSeqNum\", 'sequenceNo', \"clockAtArrival\"]]\n",
    "    order.to_pickle('/mnt/e/result/' + startDate + '/order.pkl')\n",
    "    del order\n",
    "    \n",
    "    index = fr2[fr2['tag'] == 'index'][[\"skey\", \"date\", \"num\", 'sequenceNo', \"clockAtArrival\"]]\n",
    "    index.to_pickle('/mnt/e/result/' + startDate + '/index.pkl')\n",
    "    del index\n",
    "    del fr2\n",
    "    \n",
    "    print(str(i) + 'finished')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

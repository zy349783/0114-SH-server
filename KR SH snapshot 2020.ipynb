{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:43.581152\n",
      "0:01:01.897450\n",
      "20200102 unzip finished\n",
      "0:01:04.020527\n",
      "0:01:42.055836\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:03.344717\n",
      "0:00:55.418661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:416: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.280623\n",
      "no massive missing\n",
      "0:02:38.690849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:15.609236\n",
      "0:00:46.959314\n",
      "20200103 unzip finished\n",
      "0:01:05.116026\n",
      "0:01:45.800061\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:45.601720\n",
      "0:00:46.775134\n",
      "0:00:05.008132\n",
      "no massive missing\n",
      "0:02:32.109356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:24.056610\n",
      "0:00:52.040523\n",
      "20200106 unzip finished\n",
      "0:01:03.679665\n",
      "0:01:47.631077\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:17.102778\n",
      "0:00:48.778329\n",
      "0:00:04.775919\n",
      "no massive missing\n",
      "0:03:18.371638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:17.764098\n",
      "0:00:57.139122\n",
      "20200107 unzip finished\n",
      "0:01:00.495119\n",
      "0:01:44.063612\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:59.490548\n",
      "0:00:46.558493\n",
      "0:00:04.611517\n",
      "no massive missing\n",
      "0:02:38.795932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:11.561534\n",
      "0:01:13.234987\n",
      "20200108 unzip finished\n",
      "0:01:08.700049\n",
      "0:01:48.746759\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:23.462384\n",
      "0:00:48.388326\n",
      "0:00:04.768193\n",
      "no massive missing\n",
      "0:02:42.504427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:29.850773\n",
      "0:01:02.329711\n",
      "20200109 unzip finished\n",
      "0:01:06.615924\n",
      "0:01:40.627068\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:44.068168\n",
      "0:00:48.526706\n",
      "0:00:05.014118\n",
      "no massive missing\n",
      "0:02:32.879385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:09.677353\n",
      "0:00:49.147175\n",
      "20200110 unzip finished\n",
      "0:01:17.875289\n",
      "0:01:39.725376\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:07.501201\n",
      "0:00:46.288657\n",
      "0:00:04.642233\n",
      "no massive missing\n",
      "0:02:36.807594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.848637\n",
      "0:01:05.220217\n",
      "20200113 unzip finished\n",
      "0:01:39.725110\n",
      "0:01:40.082899\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:20.928946\n",
      "0:00:45.110661\n",
      "0:00:04.405816\n",
      "no massive missing\n",
      "0:02:37.600876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:23.639247\n",
      "0:00:52.967641\n",
      "20200114 unzip finished\n",
      "0:01:12.235969\n",
      "0:01:39.818308\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:56.365773\n",
      "0:00:53.997339\n",
      "0:00:04.767483\n",
      "no massive missing\n",
      "0:02:39.451365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:16.642433\n",
      "0:00:48.392671\n",
      "20200115 unzip finished\n",
      "0:01:03.981171\n",
      "0:01:38.644596\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:07.700689\n",
      "0:00:43.307606\n",
      "0:00:04.392586\n",
      "no massive missing\n",
      "0:02:24.532705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:05.565186\n",
      "0:00:45.641996\n",
      "20200116 unzip finished\n",
      "0:01:04.838191\n",
      "0:01:38.357982\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:11.392022\n",
      "0:00:46.059321\n",
      "0:00:04.495461\n",
      "no massive missing\n",
      "0:02:35.183220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:10.761040\n",
      "0:00:45.283587\n",
      "20200117 unzip finished\n",
      "0:01:05.386927\n",
      "0:01:41.603416\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:18.674844\n",
      "0:00:46.631300\n",
      "0:00:06.204387\n",
      "no massive missing\n",
      "0:03:07.896154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.720969\n",
      "0:00:45.589520\n",
      "20200120 unzip finished\n",
      "0:01:02.872448\n",
      "0:01:54.152672\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:58.664341\n",
      "0:00:49.119267\n",
      "0:00:05.472778\n",
      "no massive missing\n",
      "0:02:55.139902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:07.820419\n",
      "0:00:46.152400\n",
      "20200121 unzip finished\n",
      "0:01:00.906864\n",
      "0:01:41.509395\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:46.758068\n",
      "0:00:45.140256\n",
      "0:00:04.444585\n",
      "no massive missing\n",
      "0:02:27.583693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:21.336246\n",
      "0:00:49.939853\n",
      "20200122 unzip finished\n",
      "0:01:03.111348\n",
      "0:01:42.341381\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:13.941305\n",
      "0:00:48.367045\n",
      "0:00:06.548447\n",
      "no massive missing\n",
      "0:02:35.542066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:23.170960\n",
      "0:00:52.299858\n",
      "20200123 unzip finished\n",
      "0:01:16.479301\n",
      "0:01:49.494268\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:05.176487\n",
      "0:00:53.099253\n",
      "0:00:04.605305\n",
      "no massive missing\n",
      "0:02:43.321240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.085501\n",
      "0:00:33.606181\n",
      "20200203 unzip finished\n",
      "0:00:47.994192\n",
      "0:01:12.651553\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:09:51.084300\n",
      "0:00:36.866081\n",
      "0:00:04.469647\n",
      "no massive missing\n",
      "0:02:06.097937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:00:46.734913\n",
      "0:00:54.081598\n",
      "20200204 unzip finished\n",
      "0:01:04.465999\n",
      "0:01:46.927661\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:11.682829\n",
      "0:00:49.953231\n",
      "0:00:04.782058\n",
      "no massive missing\n",
      "0:02:58.425694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:34.509454\n",
      "0:00:53.356628\n",
      "20200205 unzip finished\n",
      "0:01:10.284719\n",
      "0:01:48.576880\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:15.156772\n",
      "0:00:58.590268\n",
      "0:00:04.708277\n",
      "no massive missing\n",
      "0:03:00.180954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:24.670018\n",
      "0:00:54.641815\n",
      "20200206 unzip finished\n",
      "0:01:11.027289\n",
      "0:01:52.061306\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:31.338139\n",
      "0:00:47.850730\n",
      "0:00:04.530509\n",
      "no massive missing\n",
      "0:02:38.210740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:14.352549\n",
      "0:00:53.109390\n",
      "20200207 unzip finished\n",
      "0:01:04.606956\n",
      "0:01:45.911288\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:23.129103\n",
      "0:00:48.888856\n",
      "0:00:04.596401\n",
      "no massive missing\n",
      "0:02:48.228710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:28.143311\n",
      "0:00:54.936149\n",
      "20200210 unzip finished\n",
      "0:01:02.690180\n",
      "0:01:44.463763\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:59.771278\n",
      "0:00:45.091020\n",
      "0:00:04.421563\n",
      "no massive missing\n",
      "0:02:50.203067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200210"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:35.396438\n",
      "0:00:48.991168\n",
      "20200211 unzip finished\n",
      "0:01:08.041416\n",
      "0:01:46.769473\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:58.324585\n",
      "0:00:47.240130\n",
      "0:00:04.834697\n",
      "no massive missing\n",
      "0:02:42.239018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.684559\n",
      "0:00:51.833989\n",
      "20200212 unzip finished\n",
      "0:01:12.057074\n",
      "0:01:46.873053\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:47.294203\n",
      "0:00:45.772494\n",
      "0:00:05.564853\n",
      "no massive missing\n",
      "0:02:34.745073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:13.858294\n",
      "0:00:50.837035\n",
      "20200213 unzip finished\n",
      "0:01:37.240949\n",
      "0:01:46.151974\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:13:52.193081\n",
      "0:00:52.552626\n",
      "0:00:05.572396\n",
      "no massive missing\n",
      "0:02:35.368138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:29.928861\n",
      "0:01:08.693955\n",
      "20200214 unzip finished\n",
      "0:01:07.763540\n",
      "0:01:42.574674\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:01.832234\n",
      "0:00:49.316363\n",
      "0:00:04.541429\n",
      "no massive missing\n",
      "0:02:31.900243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:24.421176\n",
      "0:00:52.880826\n",
      "20200217 unzip finished\n",
      "0:01:16.478573\n",
      "0:01:49.663963\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:42.899300\n",
      "0:00:57.258300\n",
      "0:00:05.421326\n",
      "no massive missing\n",
      "0:02:39.008302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:12.814781\n",
      "0:00:50.816646\n",
      "20200218 unzip finished\n",
      "0:01:05.270821\n",
      "0:01:48.818173\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:39.918748\n",
      "0:00:52.084771\n",
      "0:00:05.148479\n",
      "no massive missing\n",
      "0:02:47.809779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:28.355416\n",
      "0:01:01.032881\n",
      "20200219 unzip finished\n",
      "0:01:06.669778\n",
      "0:01:47.112589\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:21.186816\n",
      "0:00:50.659706\n",
      "0:00:04.842943\n",
      "no massive missing\n",
      "0:02:48.379668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:21.643201\n",
      "0:00:58.328315\n",
      "20200220 unzip finished\n",
      "0:01:08.657306\n",
      "0:01:48.765435\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:28.834965\n",
      "0:00:53.943426\n",
      "0:00:04.690105\n",
      "no massive missing\n",
      "0:02:38.853502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200220"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:16.229245\n",
      "0:01:15.020878\n",
      "20200221 unzip finished\n",
      "0:01:31.612507\n",
      "0:01:47.989704\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:48.317570\n",
      "0:00:49.597273\n",
      "0:00:05.114570\n",
      "no massive missing\n",
      "0:02:58.276903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:20.951182\n",
      "0:00:54.628656\n",
      "20200224 unzip finished\n",
      "0:01:07.646299\n",
      "0:01:49.683243\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:56.911060\n",
      "0:00:53.752021\n",
      "0:00:05.240861\n",
      "no massive missing\n",
      "0:02:49.306058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:20.835182\n",
      "0:00:57.129342\n",
      "20200225 unzip finished\n",
      "0:01:11.199709\n",
      "0:01:51.700936\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:10.689406\n",
      "0:00:55.065850\n",
      "0:00:04.894438\n",
      "no massive missing\n",
      "0:03:19.969592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200225"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:32.268184\n",
      "0:01:00.598646\n",
      "20200226 unzip finished\n",
      "0:01:08.152732\n",
      "0:01:54.262977\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:44.624631\n",
      "0:00:48.705588\n",
      "0:00:04.853196\n",
      "no massive missing\n",
      "0:02:48.913537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200226"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:31.883602\n",
      "0:01:10.327695\n",
      "20200227 unzip finished\n",
      "0:01:08.732868\n",
      "0:01:51.620417\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:45.428598\n",
      "0:00:50.118850\n",
      "0:00:05.016440\n",
      "no massive missing\n",
      "0:02:42.744357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200227"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:31.791630\n",
      "0:01:13.573828\n",
      "20200228 unzip finished\n",
      "0:01:13.702147\n",
      "0:01:56.719804\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:09.031459\n",
      "0:00:49.602966\n",
      "0:00:04.905434\n",
      "no massive missing\n",
      "0:02:48.004490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:41.605089\n",
      "0:01:06.538274\n",
      "20200302 unzip finished\n",
      "0:01:11.155810\n",
      "0:01:50.674760\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:30.648877\n",
      "0:00:49.428795\n",
      "0:00:05.875308\n",
      "no massive missing\n",
      "0:03:06.829414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200302"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:22.894392\n",
      "0:00:59.126483\n",
      "20200303 unzip finished\n",
      "0:01:13.653038\n",
      "0:01:52.782996\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:19.206605\n",
      "0:00:59.563777\n",
      "0:00:07.153681\n",
      "no massive missing\n",
      "0:02:56.443290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200303"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:30.488415\n",
      "0:01:14.525810\n",
      "20200304 unzip finished\n",
      "0:01:03.819930\n",
      "0:01:50.845488\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200101'\n",
    "endDate = '20200530'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "\n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:53.769965\n",
      "0:02:31.934058\n",
      "0:01:54.780001\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:14:08.502484\n",
      "0:00:50.746042\n",
      "0:00:04.785599\n",
      "no massive missing\n",
      "0:02:38.447721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20201202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH finished\n",
      "0:01:18.340724\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2235ff873ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m \u001b[0mwr_ong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0mmi_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_ss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20201202'\n",
    "endDate = '20201202'\n",
    "readPath = '/mnt/Kevin_zhenyu/KR_daily_data' + '/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in np.sort(dataPathLs):\n",
    "    readPath = data + '/SH/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    SH = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,3,5,7,9,10,11,15,17,18,19,20,21,22,23,25,26,28,29,30,31,32,33,37,39,40,41,\n",
    "                                          42,46,47,49,50])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SH += [df]\n",
    "    del df\n",
    "    SH = pd.concat(SH).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "    SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SH[\"date\"] = int(SH[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SH[\"time\"] = (SH['QuotTime'] - int(SH['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SH[\"clockAtArrival\"] = SH[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SH.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"BidPrice\"] = SH[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferPrice\"] = SH[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidOrderQty\"] = SH[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrderQty\"] = SH[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"BidNumOrders\"] = SH[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferNumOrders\"] = SH[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'p'] = SH[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'p'] = SH[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SH.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'q'] = SH[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'q'] = SH[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SH.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid\" + str(i) + 'n'] = SH[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SH.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask\" + str(i) + 'n'] = SH[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SH.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SH[\"BidOrders\"] = SH[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SH[\"OfferOrders\"] = SH[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"bid1Top\" + str(i) + 'q'] = SH[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SH[\"ask1Top\" + str(i) + 'q'] = SH[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SH.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH.columns = ['cum_trades_cnt', 'ask_trade_max_duration', 'total_bid_orders',\n",
    "       'cum_canceled_sell_amount', 'total_ask_quantity', 'cum_canceled_buy_orders',\n",
    "       'total_ask_vwap', 'cum_canceled_sell_volume', 'cum_volume', 'open',\n",
    "       'high', 'prev_close', 'low', 'total_bid_vwap',\n",
    "       'cum_canceled_sell_orders', 'total_ask_orders', 'total_ask_levels',\n",
    "       'total_bid_quantity', 'cum_canceled_buy_volume', 'bid_trade_max_duration',\n",
    "       'total_bid_levels', 'close', 'cum_amount', 'cum_canceled_buy_amount', 'skey', 'date', 'time', 'clockAtArrival',\n",
    "       'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p', 'bid5p', 'bid6p',\n",
    "       'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p', 'ask2p', 'ask3p',\n",
    "       'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p', 'ask9p', 'ask10p',\n",
    "       'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q', 'bid6q', 'bid7q',\n",
    "       'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q', 'ask3q', 'ask4q',\n",
    "       'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q', 'ask10q', 'bid1n',\n",
    "       'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n', 'bid7n', 'bid8n',\n",
    "       'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n',\n",
    "       'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n', 'bid1Top1q',\n",
    "       'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q', 'bid1Top6q',\n",
    "       'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q', 'bid1Top11q',\n",
    "       'bid1Top12q', 'bid1Top13q', 'bid1Top14q', 'bid1Top15q',\n",
    "       'bid1Top16q', 'bid1Top17q', 'bid1Top18q', 'bid1Top19q',\n",
    "       'bid1Top20q', 'bid1Top21q', 'bid1Top22q', 'bid1Top23q',\n",
    "       'bid1Top24q', 'bid1Top25q', 'bid1Top26q', 'bid1Top27q',\n",
    "       'bid1Top28q', 'bid1Top29q', 'bid1Top30q', 'bid1Top31q',\n",
    "       'bid1Top32q', 'bid1Top33q', 'bid1Top34q', 'bid1Top35q',\n",
    "       'bid1Top36q', 'bid1Top37q', 'bid1Top38q', 'bid1Top39q',\n",
    "       'bid1Top40q', 'bid1Top41q', 'bid1Top42q', 'bid1Top43q',\n",
    "       'bid1Top44q', 'bid1Top45q', 'bid1Top46q', 'bid1Top47q',\n",
    "       'bid1Top48q', 'bid1Top49q', 'bid1Top50q', 'ask1Top1q', 'ask1Top2q',\n",
    "       'ask1Top3q', 'ask1Top4q', 'ask1Top5q', 'ask1Top6q', 'ask1Top7q',\n",
    "       'ask1Top8q', 'ask1Top9q', 'ask1Top10q', 'ask1Top11q', 'ask1Top12q',\n",
    "       'ask1Top13q', 'ask1Top14q', 'ask1Top15q', 'ask1Top16q',\n",
    "       'ask1Top17q', 'ask1Top18q', 'ask1Top19q', 'ask1Top20q',\n",
    "       'ask1Top21q', 'ask1Top22q', 'ask1Top23q', 'ask1Top24q',\n",
    "       'ask1Top25q', 'ask1Top26q', 'ask1Top27q', 'ask1Top28q',\n",
    "       'ask1Top29q', 'ask1Top30q', 'ask1Top31q', 'ask1Top32q',\n",
    "       'ask1Top33q', 'ask1Top34q', 'ask1Top35q', 'ask1Top36q',\n",
    "       'ask1Top37q', 'ask1Top38q', 'ask1Top39q', 'ask1Top40q',\n",
    "       'ask1Top41q', 'ask1Top42q', 'ask1Top43q', 'ask1Top44q',\n",
    "       'ask1Top45q', 'ask1Top46q', 'ask1Top47q', 'ask1Top48q',\n",
    "       'ask1Top49q', 'ask1Top50q']\n",
    "    SH = SH.fillna(0)\n",
    "#     SH[\"p1\"] = SH[\"bid1p\"] + SH[\"ask1p\"]\n",
    "#     tt = SH[(SH[\"cum_volume\"] > 0) & (SH[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SH.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SH = SH[~((SH[\"bid1p\"] == 0) & (SH[\"ask1p\"] == 0))]\n",
    "    SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "    SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "    \n",
    "    SH[\"has_missing\"] = 0\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SH[col] = SH[col].astype('int32')\n",
    "    \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p']:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "#     for cols in ['cum_amount', \"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "# #         SH[cols] = SH[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        \n",
    "    for cols in ['total_bid_vwap', \"total_ask_vwap\"]:\n",
    "#         print(cols)\n",
    "#         print(SH[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "        SH[cols] = SH[cols].apply(lambda x: round(x, 3))\n",
    "        \n",
    "   \n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "    SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "    assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SH[\"group\"] = SH[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SH[\"group\"].unique())\n",
    "    SH[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SH[\"order\"] = SH.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SH[\"t\"] = SH[SH[\"group\"] > i].groupby(\"skey\")[\"time\"].transform(\"min\")\n",
    "            SH[\"has_missing1\"] = np.where((SH[\"time\"] == SH[\"t\"]) & (SH[\"order\"] == 0), 1, 0)\n",
    "        SH.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SH.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SH[\"time_interval\"] = SH.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SH[\"time_interval\"] = SH[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SH[\"tn_update\"] = SH.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SH[(SH[\"time\"] >= 93000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SH[(SH[\"time\"] >= 130000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SH[(SH[\"time\"] >= 150000000000) & (SH[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SH = pd.merge(SH, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SH = pd.merge(SH, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SH = pd.merge(SH, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SH[(SH[\"time\"] > 93000000000) & (SH[\"time\"] < 145700000000) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SH = pd.merge(SH, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SH[\"has_missing2\"] = 0\n",
    "    SH[\"has_missing2\"] = np.where((SH[\"time_interval\"] > 60) & (SH[\"tn_update\"] > SH[\"99%\"]) & \n",
    "         (SH[\"time\"] > SH[\"time1\"]) & (SH[\"time\"] != SH[\"time2\"]) & (SH[\"time\"] != SH[\"time3\"]) & (SH[\"time\"] != 100000000000), 1, 0)\n",
    "    SH.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SH[\"has_missing\"] = np.where((SH[\"has_missing1\"] == 1) | (SH[\"has_missing2\"] == 1), 1, 0)\n",
    "    SH.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SH[SH[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SH[SH[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SH[SH[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SH[\"has_missing\"] = SH[\"has_missing\"].astype('int32')\n",
    "    SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SH[\"date\"].iloc[0])\n",
    "    print(\"SH finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SH)\n",
    "    \n",
    "    del SH\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)\n",
    "print(less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:21.067531\n",
      "20200601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:226: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200602\n",
      "20200603\n",
      "20200604\n",
      "20200605\n",
      "20200608\n",
      "20200609\n",
      "20200610\n",
      "20200611\n",
      "20200612\n",
      "20200615\n",
      "20200616\n",
      "20200617\n",
      "20200618\n",
      "20200619\n",
      "20200622\n",
      "20200623\n",
      "20200624\n",
      "20200629\n",
      "20200630\n",
      "20200701\n",
      "20200702\n",
      "20200703\n",
      "20200706\n",
      "20200707\n",
      "20200708\n",
      "20200709\n",
      "20200710\n",
      "20200713\n",
      "20200714\n",
      "20200715\n",
      "20200716\n",
      "20200717\n",
      "20200720\n",
      "20200721\n",
      "20200722\n",
      "20200723\n",
      "20200724\n",
      "20200727\n",
      "20200728\n",
      "20200729\n",
      "20200730\n",
      "20200731\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/mnt/ShareWithServer/day_stock_20200820/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "startDate = 20200601\n",
    "endDate = 20200731\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "all_SH = db1.read('md_snapshot_l2', start_date=startDate, end_date=endDate, symbol=[1600000])\n",
    "\n",
    "for da_te in all_SH['date'].unique():\n",
    "    print(da_te)\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    SH = db1.read('md_snapshot_l2', start_date=str(da_te), end_date=str(da_te))\n",
    "    SH = SH[SH['skey'] < 2000000]\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SH[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SH[\"cum_max\"] = SH.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    dd = SH[SH[\"cum_volume\"] == SH[\"cum_max\"]].groupby(\"skey\")[\"time\"].first().reset_index()\n",
    "    SH.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    if SH[\"date\"].iloc[0] < 20180820:\n",
    "        s2[\"auction\"] = 0\n",
    "    else:\n",
    "        dd[\"auction\"] = np.where(dd[\"time\"]<=145700000000, 0, 1)\n",
    "        dd = dd.rename(columns={\"skey\": \"ID\"})\n",
    "        s2 = pd.merge(s2, dd[[\"ID\", \"auction\"]], on=\"ID\")\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\", \"auction\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(re[re[\"d_amount_y\"].isnull()])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

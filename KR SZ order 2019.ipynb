{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:20.289637\n",
      "20190102 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190102'\n",
    "endDate = '20190102'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/order.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/order/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    OrderLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, encoding='GBK')\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        OrderLog += [df]\n",
    "    OrderLog = pd.concat(OrderLog).reset_index(drop=True)\n",
    "    OrderLog = OrderLog[OrderLog[\"ChannelNo\"] != 4001]\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"OrdType\": \"OrderType\"})\n",
    "    OrderLog[\"date\"] = OrderLog[\"TransactTime\"].iloc[0]//1000000000\n",
    "    OrderLog[\"OrderType\"] = np.where(OrderLog[\"OrderType\"] == 'U', 3, OrderLog[\"OrderType\"])\n",
    "    OrderLog[\"skey\"] = OrderLog[\"SecurityID\"] + 2000000\n",
    "    OrderLog[\"clockAtArrival\"] = OrderLog[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    OrderLog['datetime'] = OrderLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    OrderLog[\"time\"] = (OrderLog['TransactTime'] - int(OrderLog['TransactTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"OrderQty\", \"Side\", \"OrderType\"]:\n",
    "        OrderLog[col] = OrderLog[col].astype('int32')\n",
    "#     for cols in [\"Price\"]:\n",
    "#         print(cols)\n",
    "#         print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "    assert(OrderLog[((OrderLog[\"Side\"] != 1) & (OrderLog[\"Side\"] != 2)) | (OrderLog[\"OrderType\"].isnull())].shape[0] == 0)\n",
    "    da_te = str(OrderLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 2000000).unique()\n",
    "    del db1\n",
    "    try:\n",
    "        assert(len(set(sl) - set(OrderLog[\"skey\"].unique())) == 0)\n",
    "    except:\n",
    "        print(\"less stocks\")\n",
    "        display(set(sl) - set(OrderLog[\"skey\"].unique()))\n",
    "    if len(set(OrderLog[\"skey\"].unique()) - set(sl)) != 0:\n",
    "        print(\"more stocks\")\n",
    "        print(set(OrderLog[\"skey\"].unique()) - set(sl))\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"Side\":\"order_side\", \"OrderType\":\"order_type\", \"Price\":\"order_price\", \"OrderQty\":\"order_qty\"})\n",
    "    OrderLog = OrderLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"order_side\", \"order_type\", \"order_price\",\n",
    "                                                 \"order_qty\"]]\n",
    "    \n",
    "    print(OrderLog[\"date\"].iloc[0])\n",
    "    print(\"order finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_order', OrderLog)\n",
    "    \n",
    "    del OrderLog\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "#     pd.set_option(\"max_rows\", 200)\n",
    "#     display(OrderLog.dtypes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price\n",
      "[2 1]\n"
     ]
    }
   ],
   "source": [
    "    for cols in [\"Price\"]:\n",
    "        print(cols)\n",
    "        print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrderLog[(OrderLog['skey'] == 2000006) & (OrderLog['ApplSeqNum'] == 43854)]['Price'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52699.99999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(OrderLog[(OrderLog['skey'] == 2000006) & (OrderLog['ApplSeqNum'] == 43854)]['Price'].iloc[0], 2) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:195: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:42.716834\n",
      "0:00:28.009523\n",
      "20190222 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190222\n",
      "order finished\n",
      "0:11:51.116715\n",
      "0:00:45.433259\n",
      "20190225 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190225\n",
      "order finished\n",
      "0:15:16.157583\n",
      "0:00:55.183759\n",
      "20190226 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190226\n",
      "order finished\n",
      "0:17:04.825116\n",
      "0:00:48.858391\n",
      "20190227 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190227\n",
      "order finished\n",
      "0:15:11.217893\n",
      "0:00:44.249489\n",
      "20190228 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190228\n",
      "order finished\n",
      "0:13:54.617168\n",
      "0:00:41.705737\n",
      "20190301 unzip finished\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190222'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/order.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/order/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    OrderLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, encoding='GBK')\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        OrderLog += [df]\n",
    "    OrderLog = pd.concat(OrderLog).reset_index(drop=True)\n",
    "    OrderLog = OrderLog[OrderLog[\"ChannelNo\"] != 4001]\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"OrdType\": \"OrderType\"})\n",
    "    OrderLog[\"date\"] = OrderLog[\"TransactTime\"].iloc[0]//1000000000\n",
    "    OrderLog[\"OrderType\"] = np.where(OrderLog[\"OrderType\"] == 'U', 3, OrderLog[\"OrderType\"])\n",
    "    OrderLog[\"skey\"] = OrderLog[\"SecurityID\"] + 2000000\n",
    "    OrderLog[\"clockAtArrival\"] = OrderLog[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    OrderLog['datetime'] = OrderLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    OrderLog[\"time\"] = (OrderLog['TransactTime'] - int(OrderLog['TransactTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"OrderQty\", \"Side\", \"OrderType\"]:\n",
    "        OrderLog[col] = OrderLog[col].astype('int32')\n",
    "#     for cols in [\"Price\"]:\n",
    "#         print(cols)\n",
    "#         print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "    assert(OrderLog[((OrderLog[\"Side\"] != 1) & (OrderLog[\"Side\"] != 2)) | (OrderLog[\"OrderType\"].isnull())].shape[0] == 0)\n",
    "    da_te = str(OrderLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 2000000).unique()\n",
    "    del db1\n",
    "    try:\n",
    "        assert(len(set(sl) - set(OrderLog[\"skey\"].unique())) == 0)\n",
    "    except:\n",
    "        print(\"less stocks\")\n",
    "        display(set(sl) - set(OrderLog[\"skey\"].unique()))\n",
    "    if len(set(OrderLog[\"skey\"].unique()) - set(sl)) != 0:\n",
    "        print(\"more stocks\")\n",
    "        print(set(OrderLog[\"skey\"].unique()) - set(sl))\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"Side\":\"order_side\", \"OrderType\":\"order_type\", \"Price\":\"order_price\", \"OrderQty\":\"order_qty\"})\n",
    "    OrderLog = OrderLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"order_side\", \"order_type\", \"order_price\",\n",
    "                                                 \"order_qty\"]]\n",
    "    \n",
    "    print(OrderLog[\"date\"].iloc[0])\n",
    "    print(\"order finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_order', OrderLog)\n",
    "    \n",
    "    del OrderLog\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "#     pd.set_option(\"max_rows\", 200)\n",
    "#     display(OrderLog.dtypes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:195: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:53.683019\n",
      "0:00:00.439361\n",
      "20190301 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190301\n",
      "order finished\n",
      "0:12:02.083538\n",
      "0:00:00.399924\n",
      "20190304 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190304\n",
      "order finished\n",
      "0:15:22.357403\n",
      "0:00:46.952309\n",
      "20190305 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190305\n",
      "order finished\n",
      "0:14:35.914131\n",
      "0:00:50.626311\n",
      "20190306 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190306\n",
      "order finished\n",
      "0:16:51.548859\n",
      "0:00:47.603686\n",
      "20190307 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190307\n",
      "order finished\n",
      "0:18:17.707988\n",
      "0:00:55.416558\n",
      "20190308 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (6,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190308\n",
      "order finished\n",
      "0:17:41.232718\n",
      "0:00:48.760603\n",
      "20190311 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190311\n",
      "order finished\n",
      "0:14:14.091425\n",
      "0:00:58.329263\n",
      "20190312 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190312\n",
      "order finished\n",
      "0:16:25.949296\n",
      "0:00:50.254852\n",
      "20190313 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190313\n",
      "order finished\n",
      "0:16:26.846031\n",
      "0:00:49.309542\n",
      "20190314 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190314\n",
      "order finished\n",
      "0:13:51.792288\n",
      "0:00:40.722545\n",
      "20190315 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190315\n",
      "order finished\n",
      "0:12:32.200219\n",
      "0:00:41.889053\n",
      "20190318 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190318\n",
      "order finished\n",
      "0:12:23.420709\n",
      "0:00:40.070480\n",
      "20190319 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190319\n",
      "order finished\n",
      "0:12:53.314058\n",
      "0:00:41.568306\n",
      "20190320 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190320\n",
      "order finished\n",
      "0:14:06.662110\n",
      "0:00:53.720726\n",
      "20190321 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190321\n",
      "order finished\n",
      "0:13:46.968337\n",
      "0:00:45.895149\n",
      "20190322 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190322\n",
      "order finished\n",
      "0:13:30.281265\n",
      "0:00:41.967373\n",
      "20190325 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,6,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190325\n",
      "order finished\n",
      "0:12:50.549575\n",
      "0:00:42.305788\n",
      "20190326 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190326\n",
      "order finished\n",
      "0:13:14.933893\n",
      "0:00:38.328252\n",
      "20190327 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190327\n",
      "order finished\n",
      "0:11:44.109336\n",
      "0:00:41.418229\n",
      "20190328 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190328\n",
      "order finished\n",
      "0:11:33.652323\n",
      "0:00:41.472482\n",
      "20190329 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190329\n",
      "order finished\n",
      "0:13:23.092413\n",
      "0:00:37.330752\n",
      "20190401 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190401\n",
      "order finished\n",
      "0:14:38.158474\n",
      "0:00:38.000824\n",
      "20190402 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190402\n",
      "order finished\n",
      "0:15:27.680415\n",
      "0:00:36.977697\n",
      "20190403 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190403\n",
      "order finished\n",
      "0:14:33.591816\n",
      "0:00:34.836342\n",
      "20190404 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190404\n",
      "order finished\n",
      "0:14:40.159866\n",
      "0:00:37.777734\n",
      "20190408 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190408\n",
      "order finished\n",
      "0:15:22.206058\n",
      "0:00:32.694536\n",
      "20190409 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190409\n",
      "order finished\n",
      "0:13:54.143316\n",
      "0:00:35.930288\n",
      "20190410 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190410\n",
      "order finished\n",
      "0:14:29.082375\n",
      "0:00:34.177579\n",
      "20190411 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190411\n",
      "order finished\n",
      "0:12:59.891575\n",
      "0:00:28.699538\n",
      "20190412 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190412\n",
      "order finished\n",
      "0:11:15.559490\n",
      "0:00:30.609825\n",
      "20190415 unzip finished\n",
      "20190415\n",
      "order finished\n",
      "0:11:47.869468\n",
      "0:00:30.713321\n",
      "20190416 unzip finished\n",
      "20190416\n",
      "order finished\n",
      "0:12:32.111716\n",
      "0:00:32.948806\n",
      "20190417 unzip finished\n",
      "20190417\n",
      "order finished\n",
      "0:13:12.879726\n",
      "0:00:31.897563\n",
      "20190418 unzip finished\n",
      "20190418\n",
      "order finished\n",
      "0:13:00.296170\n",
      "0:00:30.138042\n",
      "20190419 unzip finished\n",
      "20190419\n",
      "order finished\n",
      "0:12:02.211870\n",
      "0:00:30.708792\n",
      "20190422 unzip finished\n",
      "20190422\n",
      "order finished\n",
      "0:12:46.629853\n",
      "0:00:29.620053\n",
      "20190423 unzip finished\n",
      "20190423\n",
      "order finished\n",
      "0:12:00.533193\n",
      "0:00:30.564052\n",
      "20190424 unzip finished\n",
      "20190424\n",
      "order finished\n",
      "0:11:19.329433\n",
      "0:00:32.638669\n",
      "20190425 unzip finished\n",
      "20190425\n",
      "order finished\n",
      "0:12:25.738354\n",
      "0:00:29.677666\n",
      "20190426 unzip finished\n",
      "20190426\n",
      "order finished\n",
      "0:11:43.072700\n",
      "0:00:40.662634\n",
      "20190429 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190429\n",
      "order finished\n",
      "0:10:32.538072\n",
      "0:00:20.790852\n",
      "20190430 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2000039,\n",
       " 2000040,\n",
       " 2000042,\n",
       " 2000043,\n",
       " 2000045,\n",
       " 2000046,\n",
       " 2000048,\n",
       " 2000049,\n",
       " 2000050,\n",
       " 2000055,\n",
       " 2000056,\n",
       " 2000058,\n",
       " 2000059,\n",
       " 2000060,\n",
       " 2000061,\n",
       " 2000062,\n",
       " 2000063,\n",
       " 2000065,\n",
       " 2000066,\n",
       " 2000068,\n",
       " 2000069,\n",
       " 2000070,\n",
       " 2000078,\n",
       " 2000088,\n",
       " 2000089,\n",
       " 2000090,\n",
       " 2000096,\n",
       " 2000099,\n",
       " 2000100,\n",
       " 2000150,\n",
       " 2000151,\n",
       " 2000153,\n",
       " 2000155,\n",
       " 2000156,\n",
       " 2000157,\n",
       " 2000158,\n",
       " 2000159,\n",
       " 2000166,\n",
       " 2000301,\n",
       " 2000333,\n",
       " 2000338,\n",
       " 2000400,\n",
       " 2000401,\n",
       " 2000402,\n",
       " 2000403,\n",
       " 2000404,\n",
       " 2000407,\n",
       " 2000408,\n",
       " 2000409,\n",
       " 2000410,\n",
       " 2000411,\n",
       " 2000413,\n",
       " 2000415,\n",
       " 2000416,\n",
       " 2000417,\n",
       " 2000418,\n",
       " 2000419,\n",
       " 2000420,\n",
       " 2000421,\n",
       " 2000422,\n",
       " 2000423,\n",
       " 2000425,\n",
       " 2000426,\n",
       " 2000428,\n",
       " 2000429,\n",
       " 2000430,\n",
       " 2000488,\n",
       " 2000498,\n",
       " 2000501,\n",
       " 2000502,\n",
       " 2000503,\n",
       " 2000505,\n",
       " 2000506,\n",
       " 2000507,\n",
       " 2000509,\n",
       " 2000510,\n",
       " 2000513,\n",
       " 2000514,\n",
       " 2000516,\n",
       " 2000517,\n",
       " 2000518,\n",
       " 2000519,\n",
       " 2000520,\n",
       " 2000521,\n",
       " 2000523,\n",
       " 2000524,\n",
       " 2000525,\n",
       " 2000526,\n",
       " 2000528,\n",
       " 2000529,\n",
       " 2000530,\n",
       " 2000531,\n",
       " 2000532,\n",
       " 2000533,\n",
       " 2000534,\n",
       " 2000536,\n",
       " 2000537,\n",
       " 2000538,\n",
       " 2000539,\n",
       " 2000540,\n",
       " 2000541,\n",
       " 2000543,\n",
       " 2000544,\n",
       " 2000545,\n",
       " 2000546,\n",
       " 2000547,\n",
       " 2000548,\n",
       " 2000550,\n",
       " 2000551,\n",
       " 2000552,\n",
       " 2000553,\n",
       " 2000554,\n",
       " 2000555,\n",
       " 2000557,\n",
       " 2000558,\n",
       " 2000559,\n",
       " 2000560,\n",
       " 2000561,\n",
       " 2000563,\n",
       " 2000564,\n",
       " 2000565,\n",
       " 2000567,\n",
       " 2000568,\n",
       " 2000570,\n",
       " 2000571,\n",
       " 2000572,\n",
       " 2000573,\n",
       " 2000576,\n",
       " 2000581,\n",
       " 2000582,\n",
       " 2000584,\n",
       " 2000585,\n",
       " 2000586,\n",
       " 2000587,\n",
       " 2000589,\n",
       " 2000590,\n",
       " 2000591,\n",
       " 2000592,\n",
       " 2000593,\n",
       " 2000595,\n",
       " 2000596,\n",
       " 2000597,\n",
       " 2000598,\n",
       " 2000599,\n",
       " 2000600,\n",
       " 2000601,\n",
       " 2000603,\n",
       " 2000605,\n",
       " 2000606,\n",
       " 2000607,\n",
       " 2000608,\n",
       " 2000609,\n",
       " 2000610,\n",
       " 2000612,\n",
       " 2000613,\n",
       " 2000615,\n",
       " 2000616,\n",
       " 2000617,\n",
       " 2000619,\n",
       " 2000620,\n",
       " 2000622,\n",
       " 2000623,\n",
       " 2000625,\n",
       " 2000626,\n",
       " 2000627,\n",
       " 2000628,\n",
       " 2000629,\n",
       " 2000630,\n",
       " 2000631,\n",
       " 2000632,\n",
       " 2000633,\n",
       " 2000635,\n",
       " 2000636,\n",
       " 2000637,\n",
       " 2000638,\n",
       " 2000639,\n",
       " 2000650,\n",
       " 2000651,\n",
       " 2000652,\n",
       " 2000655,\n",
       " 2000656,\n",
       " 2000657,\n",
       " 2000659,\n",
       " 2000661,\n",
       " 2000662,\n",
       " 2000663,\n",
       " 2000665,\n",
       " 2000666,\n",
       " 2000667,\n",
       " 2000668,\n",
       " 2000669,\n",
       " 2000670,\n",
       " 2000671,\n",
       " 2000672,\n",
       " 2000673,\n",
       " 2000676,\n",
       " 2000677,\n",
       " 2000678,\n",
       " 2000679,\n",
       " 2000680,\n",
       " 2000681,\n",
       " 2000682,\n",
       " 2000683,\n",
       " 2000685,\n",
       " 2000686,\n",
       " 2000687,\n",
       " 2000688,\n",
       " 2000690,\n",
       " 2000691,\n",
       " 2000692,\n",
       " 2000695,\n",
       " 2000697,\n",
       " 2000698,\n",
       " 2000700,\n",
       " 2000701,\n",
       " 2000702,\n",
       " 2000703,\n",
       " 2000705,\n",
       " 2000707,\n",
       " 2000708,\n",
       " 2000709,\n",
       " 2000710,\n",
       " 2000711,\n",
       " 2000712,\n",
       " 2000713,\n",
       " 2000715,\n",
       " 2000716,\n",
       " 2000717,\n",
       " 2000718,\n",
       " 2000719,\n",
       " 2000720,\n",
       " 2000721,\n",
       " 2000722,\n",
       " 2000723,\n",
       " 2000725,\n",
       " 2000726,\n",
       " 2000727,\n",
       " 2000728,\n",
       " 2000729,\n",
       " 2000731,\n",
       " 2000732,\n",
       " 2000733,\n",
       " 2000735,\n",
       " 2000736,\n",
       " 2000737,\n",
       " 2000738,\n",
       " 2000739,\n",
       " 2000750,\n",
       " 2000751,\n",
       " 2000753,\n",
       " 2000755,\n",
       " 2000756,\n",
       " 2000757,\n",
       " 2000758,\n",
       " 2000759,\n",
       " 2000761,\n",
       " 2000762,\n",
       " 2000766,\n",
       " 2000767,\n",
       " 2000768,\n",
       " 2000776,\n",
       " 2000777,\n",
       " 2000778,\n",
       " 2000779,\n",
       " 2000780,\n",
       " 2000782,\n",
       " 2000783,\n",
       " 2000785,\n",
       " 2000786,\n",
       " 2000788,\n",
       " 2000789,\n",
       " 2000790,\n",
       " 2000791,\n",
       " 2000792,\n",
       " 2000793,\n",
       " 2000795,\n",
       " 2000796,\n",
       " 2000797,\n",
       " 2000798,\n",
       " 2000799,\n",
       " 2000800,\n",
       " 2000801,\n",
       " 2000802,\n",
       " 2000803,\n",
       " 2000806,\n",
       " 2000807,\n",
       " 2000809,\n",
       " 2000810,\n",
       " 2000811,\n",
       " 2000812,\n",
       " 2000813,\n",
       " 2000815,\n",
       " 2000816,\n",
       " 2000818,\n",
       " 2000819,\n",
       " 2000821,\n",
       " 2000822,\n",
       " 2000823,\n",
       " 2000825,\n",
       " 2000826,\n",
       " 2000828,\n",
       " 2000829,\n",
       " 2000830,\n",
       " 2000831,\n",
       " 2000833,\n",
       " 2000835,\n",
       " 2000836,\n",
       " 2000837,\n",
       " 2000838,\n",
       " 2000839,\n",
       " 2000848,\n",
       " 2000850,\n",
       " 2000851,\n",
       " 2000852,\n",
       " 2000856,\n",
       " 2000858,\n",
       " 2000859,\n",
       " 2000860,\n",
       " 2000861,\n",
       " 2000862,\n",
       " 2000863,\n",
       " 2000868,\n",
       " 2000869,\n",
       " 2000875,\n",
       " 2000876,\n",
       " 2000877,\n",
       " 2000878,\n",
       " 2000880,\n",
       " 2000881,\n",
       " 2000882,\n",
       " 2000883,\n",
       " 2000885,\n",
       " 2000886,\n",
       " 2000887,\n",
       " 2000888,\n",
       " 2000889,\n",
       " 2000890,\n",
       " 2000892,\n",
       " 2000893,\n",
       " 2000895,\n",
       " 2000897,\n",
       " 2000898,\n",
       " 2000899,\n",
       " 2000900,\n",
       " 2000901,\n",
       " 2000902,\n",
       " 2000903,\n",
       " 2000905,\n",
       " 2000906,\n",
       " 2000908,\n",
       " 2000909,\n",
       " 2000910,\n",
       " 2000911,\n",
       " 2000912,\n",
       " 2000913,\n",
       " 2000915,\n",
       " 2000917,\n",
       " 2000918,\n",
       " 2000919,\n",
       " 2000920,\n",
       " 2000921,\n",
       " 2000922,\n",
       " 2000923,\n",
       " 2000925,\n",
       " 2000926,\n",
       " 2000927,\n",
       " 2000928,\n",
       " 2000929,\n",
       " 2000930,\n",
       " 2000931,\n",
       " 2000932,\n",
       " 2000933,\n",
       " 2000935,\n",
       " 2000936,\n",
       " 2000937,\n",
       " 2000938,\n",
       " 2000948,\n",
       " 2000949,\n",
       " 2000950,\n",
       " 2000951,\n",
       " 2000952,\n",
       " 2000953,\n",
       " 2000955,\n",
       " 2000957,\n",
       " 2000958,\n",
       " 2000959,\n",
       " 2000960,\n",
       " 2000961,\n",
       " 2000962,\n",
       " 2000963,\n",
       " 2000965,\n",
       " 2000966,\n",
       " 2000967,\n",
       " 2000968,\n",
       " 2000969,\n",
       " 2000970,\n",
       " 2000971,\n",
       " 2000972,\n",
       " 2000973,\n",
       " 2000975,\n",
       " 2000976,\n",
       " 2000977,\n",
       " 2000978,\n",
       " 2000980,\n",
       " 2000982,\n",
       " 2000983,\n",
       " 2000985,\n",
       " 2000987,\n",
       " 2000988,\n",
       " 2000989,\n",
       " 2000990,\n",
       " 2000993,\n",
       " 2000996,\n",
       " 2000997,\n",
       " 2000998,\n",
       " 2000999,\n",
       " 2001696,\n",
       " 2001872,\n",
       " 2001896,\n",
       " 2001914,\n",
       " 2001965,\n",
       " 2001979,\n",
       " 2002001,\n",
       " 2002002,\n",
       " 2002003,\n",
       " 2002004,\n",
       " 2002005,\n",
       " 2002006,\n",
       " 2002007,\n",
       " 2002008,\n",
       " 2002009,\n",
       " 2002010,\n",
       " 2002011,\n",
       " 2002012,\n",
       " 2002013,\n",
       " 2002014,\n",
       " 2002015,\n",
       " 2002016,\n",
       " 2002017,\n",
       " 2002018,\n",
       " 2002019,\n",
       " 2002020,\n",
       " 2002021,\n",
       " 2002022,\n",
       " 2002023,\n",
       " 2002024,\n",
       " 2002025,\n",
       " 2002026,\n",
       " 2002027,\n",
       " 2002028,\n",
       " 2002029,\n",
       " 2002030,\n",
       " 2002031,\n",
       " 2002032,\n",
       " 2002033,\n",
       " 2002034,\n",
       " 2002035,\n",
       " 2002036,\n",
       " 2002037,\n",
       " 2002038,\n",
       " 2002039,\n",
       " 2002040,\n",
       " 2002041,\n",
       " 2002042,\n",
       " 2002043,\n",
       " 2002044,\n",
       " 2002045,\n",
       " 2002046,\n",
       " 2002047,\n",
       " 2002048,\n",
       " 2002049,\n",
       " 2002050,\n",
       " 2002051,\n",
       " 2002052,\n",
       " 2002053,\n",
       " 2002054,\n",
       " 2002055,\n",
       " 2002056,\n",
       " 2002057,\n",
       " 2002058,\n",
       " 2002059,\n",
       " 2002060,\n",
       " 2002061,\n",
       " 2002062,\n",
       " 2002063,\n",
       " 2002064,\n",
       " 2002065,\n",
       " 2002066,\n",
       " 2002067,\n",
       " 2002068,\n",
       " 2002069,\n",
       " 2002071,\n",
       " 2002073,\n",
       " 2002074,\n",
       " 2002075,\n",
       " 2002076,\n",
       " 2002077,\n",
       " 2002078,\n",
       " 2002079,\n",
       " 2002080,\n",
       " 2002081,\n",
       " 2002082,\n",
       " 2002083,\n",
       " 2002084,\n",
       " 2002085,\n",
       " 2002086,\n",
       " 2002087,\n",
       " 2002088,\n",
       " 2002090,\n",
       " 2002091,\n",
       " 2002092,\n",
       " 2002093,\n",
       " 2002094,\n",
       " 2002095,\n",
       " 2002096,\n",
       " 2002097,\n",
       " 2002098,\n",
       " 2002099,\n",
       " 2002100,\n",
       " 2002101,\n",
       " 2002102,\n",
       " 2002103,\n",
       " 2002104,\n",
       " 2002105,\n",
       " 2002106,\n",
       " 2002107,\n",
       " 2002108,\n",
       " 2002109,\n",
       " 2002110,\n",
       " 2002111,\n",
       " 2002112,\n",
       " 2002113,\n",
       " 2002114,\n",
       " 2002115,\n",
       " 2002116,\n",
       " 2002117,\n",
       " 2002118,\n",
       " 2002119,\n",
       " 2002120,\n",
       " 2002121,\n",
       " 2002122,\n",
       " 2002123,\n",
       " 2002124,\n",
       " 2002125,\n",
       " 2002126,\n",
       " 2002127,\n",
       " 2002128,\n",
       " 2002129,\n",
       " 2002130,\n",
       " 2002131,\n",
       " 2002132,\n",
       " 2002133,\n",
       " 2002134,\n",
       " 2002135,\n",
       " 2002136,\n",
       " 2002137,\n",
       " 2002138,\n",
       " 2002139,\n",
       " 2002140,\n",
       " 2002141,\n",
       " 2002142,\n",
       " 2002143,\n",
       " 2002144,\n",
       " 2002145,\n",
       " 2002146,\n",
       " 2002147,\n",
       " 2002148,\n",
       " 2002149,\n",
       " 2002150,\n",
       " 2002151,\n",
       " 2002152,\n",
       " 2002153,\n",
       " 2002154,\n",
       " 2002155,\n",
       " 2002156,\n",
       " 2002157,\n",
       " 2002158,\n",
       " 2002159,\n",
       " 2002160,\n",
       " 2002161,\n",
       " 2002162,\n",
       " 2002163,\n",
       " 2002164,\n",
       " 2002165,\n",
       " 2002166,\n",
       " 2002167,\n",
       " 2002168,\n",
       " 2002169,\n",
       " 2002170,\n",
       " 2002171,\n",
       " 2002172,\n",
       " 2002173,\n",
       " 2002174,\n",
       " 2002175,\n",
       " 2002176,\n",
       " 2002177,\n",
       " 2002178,\n",
       " 2002179,\n",
       " 2002180,\n",
       " 2002181,\n",
       " 2002182,\n",
       " 2002183,\n",
       " 2002184,\n",
       " 2002185,\n",
       " 2002186,\n",
       " 2002187,\n",
       " 2002188,\n",
       " 2002189,\n",
       " 2002190,\n",
       " 2002191,\n",
       " 2002192,\n",
       " 2002193,\n",
       " 2002194,\n",
       " 2002195,\n",
       " 2002196,\n",
       " 2002197,\n",
       " 2002198,\n",
       " 2002199,\n",
       " 2002200,\n",
       " 2002201,\n",
       " 2002202,\n",
       " 2002203,\n",
       " 2002204,\n",
       " 2002205,\n",
       " 2002206,\n",
       " 2002207,\n",
       " 2002208,\n",
       " 2002209,\n",
       " 2002211,\n",
       " 2002212,\n",
       " 2002213,\n",
       " 2002214,\n",
       " 2002215,\n",
       " 2002216,\n",
       " 2002217,\n",
       " 2002218,\n",
       " 2002219,\n",
       " 2002220,\n",
       " 2002221,\n",
       " 2002222,\n",
       " 2002223,\n",
       " 2002224,\n",
       " 2002225,\n",
       " 2002226,\n",
       " 2002227,\n",
       " 2002228,\n",
       " 2002229,\n",
       " 2002230,\n",
       " 2002231,\n",
       " 2002232,\n",
       " 2002233,\n",
       " 2002234,\n",
       " 2002235,\n",
       " 2002236,\n",
       " 2002237,\n",
       " 2002238,\n",
       " 2002239,\n",
       " 2002240,\n",
       " 2002241,\n",
       " 2002242,\n",
       " 2002243,\n",
       " 2002244,\n",
       " 2002245,\n",
       " 2002246,\n",
       " 2002247,\n",
       " 2002248,\n",
       " 2002249,\n",
       " 2002250,\n",
       " 2002251,\n",
       " 2002252,\n",
       " 2002253,\n",
       " 2002254,\n",
       " 2002255,\n",
       " 2002256,\n",
       " 2002258,\n",
       " 2002261,\n",
       " 2002262,\n",
       " 2002263,\n",
       " 2002264,\n",
       " 2002265,\n",
       " 2002266,\n",
       " 2002267,\n",
       " 2002268,\n",
       " 2002269,\n",
       " 2002270,\n",
       " 2002271,\n",
       " 2002272,\n",
       " 2002273,\n",
       " 2002274,\n",
       " 2002275,\n",
       " 2002276,\n",
       " 2002277,\n",
       " 2002278,\n",
       " 2002279,\n",
       " 2002280,\n",
       " 2002281,\n",
       " 2002282,\n",
       " 2002283,\n",
       " 2002284,\n",
       " 2002285,\n",
       " 2002286,\n",
       " 2002287,\n",
       " 2002288,\n",
       " 2002289,\n",
       " 2002291,\n",
       " 2002292,\n",
       " 2002293,\n",
       " 2002294,\n",
       " 2002295,\n",
       " 2002296,\n",
       " 2002297,\n",
       " 2002298,\n",
       " 2002299,\n",
       " 2002300,\n",
       " 2002301,\n",
       " 2002302,\n",
       " 2002303,\n",
       " 2002304,\n",
       " 2002305,\n",
       " 2002306,\n",
       " 2002307,\n",
       " 2002308,\n",
       " 2002309,\n",
       " 2002310,\n",
       " 2002311,\n",
       " 2002312,\n",
       " 2002313,\n",
       " 2002314,\n",
       " 2002315,\n",
       " 2002316,\n",
       " 2002317,\n",
       " 2002318,\n",
       " 2002319,\n",
       " 2002320,\n",
       " 2002321,\n",
       " 2002322,\n",
       " 2002324,\n",
       " 2002325,\n",
       " 2002326,\n",
       " 2002327,\n",
       " 2002328,\n",
       " 2002329,\n",
       " 2002330,\n",
       " 2002331,\n",
       " 2002332,\n",
       " 2002334,\n",
       " 2002335,\n",
       " 2002336,\n",
       " 2002337,\n",
       " 2002338,\n",
       " 2002339,\n",
       " 2002340,\n",
       " 2002341,\n",
       " 2002342,\n",
       " 2002343,\n",
       " 2002344,\n",
       " 2002345,\n",
       " 2002346,\n",
       " 2002347,\n",
       " 2002348,\n",
       " 2002349,\n",
       " 2002350,\n",
       " 2002351,\n",
       " 2002352,\n",
       " 2002353,\n",
       " 2002354,\n",
       " 2002355,\n",
       " 2002357,\n",
       " 2002358,\n",
       " 2002360,\n",
       " 2002361,\n",
       " 2002362,\n",
       " 2002363,\n",
       " 2002364,\n",
       " 2002365,\n",
       " 2002366,\n",
       " 2002367,\n",
       " 2002368,\n",
       " 2002369,\n",
       " 2002370,\n",
       " 2002371,\n",
       " 2002372,\n",
       " 2002373,\n",
       " 2002374,\n",
       " 2002375,\n",
       " 2002376,\n",
       " 2002377,\n",
       " 2002378,\n",
       " 2002379,\n",
       " 2002380,\n",
       " 2002381,\n",
       " 2002382,\n",
       " 2002383,\n",
       " 2002384,\n",
       " 2002385,\n",
       " 2002386,\n",
       " 2002387,\n",
       " 2002388,\n",
       " 2002389,\n",
       " 2002390,\n",
       " 2002391,\n",
       " 2002392,\n",
       " 2002393,\n",
       " 2002394,\n",
       " 2002395,\n",
       " 2002396,\n",
       " 2002397,\n",
       " 2002398,\n",
       " 2002399,\n",
       " 2002400,\n",
       " 2002401,\n",
       " 2002402,\n",
       " 2002403,\n",
       " 2002404,\n",
       " 2002405,\n",
       " 2002406,\n",
       " 2002407,\n",
       " 2002408,\n",
       " 2002409,\n",
       " 2002410,\n",
       " 2002411,\n",
       " 2002412,\n",
       " 2002413,\n",
       " 2002414,\n",
       " 2002415,\n",
       " 2002416,\n",
       " 2002418,\n",
       " 2002419,\n",
       " 2002420,\n",
       " 2002421,\n",
       " 2002422,\n",
       " 2002423,\n",
       " 2002424,\n",
       " 2002425,\n",
       " 2002426,\n",
       " 2002427,\n",
       " 2002428,\n",
       " 2002429,\n",
       " 2002430,\n",
       " 2002431,\n",
       " 2002432,\n",
       " 2002433,\n",
       " 2002434,\n",
       " 2002435,\n",
       " 2002436,\n",
       " 2002437,\n",
       " 2002438,\n",
       " 2002439,\n",
       " 2002440,\n",
       " 2002441,\n",
       " 2002442,\n",
       " 2002443,\n",
       " 2002444,\n",
       " 2002445,\n",
       " 2002446,\n",
       " 2002447,\n",
       " 2002448,\n",
       " 2002449,\n",
       " 2002451,\n",
       " 2002452,\n",
       " 2002453,\n",
       " 2002454,\n",
       " 2002455,\n",
       " 2002457,\n",
       " 2002458,\n",
       " 2002459,\n",
       " 2002460,\n",
       " 2002461,\n",
       " 2002462,\n",
       " 2002463,\n",
       " 2002464,\n",
       " 2002465,\n",
       " 2002466,\n",
       " 2002467,\n",
       " 2002468,\n",
       " 2002469,\n",
       " 2002470,\n",
       " 2002471,\n",
       " 2002472,\n",
       " 2002473,\n",
       " 2002474,\n",
       " 2002475,\n",
       " 2002476,\n",
       " 2002477,\n",
       " 2002478,\n",
       " 2002479,\n",
       " 2002480,\n",
       " 2002481,\n",
       " 2002482,\n",
       " 2002483,\n",
       " 2002484,\n",
       " 2002485,\n",
       " 2002486,\n",
       " 2002487,\n",
       " 2002488,\n",
       " 2002489,\n",
       " 2002490,\n",
       " 2002491,\n",
       " 2002492,\n",
       " 2002493,\n",
       " 2002494,\n",
       " 2002495,\n",
       " 2002496,\n",
       " 2002497,\n",
       " 2002498,\n",
       " 2002499,\n",
       " 2002500,\n",
       " 2002502,\n",
       " 2002503,\n",
       " 2002504,\n",
       " 2002505,\n",
       " 2002506,\n",
       " 2002507,\n",
       " 2002508,\n",
       " 2002509,\n",
       " 2002510,\n",
       " 2002511,\n",
       " 2002512,\n",
       " 2002513,\n",
       " 2002514,\n",
       " 2002515,\n",
       " 2002516,\n",
       " 2002517,\n",
       " 2002518,\n",
       " 2002519,\n",
       " 2002520,\n",
       " 2002521,\n",
       " 2002522,\n",
       " 2002523,\n",
       " 2002524,\n",
       " 2002526,\n",
       " 2002527,\n",
       " 2002528,\n",
       " 2002529,\n",
       " 2002530,\n",
       " 2002531,\n",
       " 2002532,\n",
       " 2002533,\n",
       " 2002534,\n",
       " 2002535,\n",
       " 2002536,\n",
       " 2002537,\n",
       " 2002538,\n",
       " 2002539,\n",
       " 2002540,\n",
       " 2002541,\n",
       " 2002542,\n",
       " 2002543,\n",
       " 2002544,\n",
       " 2002545,\n",
       " 2002546,\n",
       " 2002547,\n",
       " 2002548,\n",
       " 2002549,\n",
       " 2002550,\n",
       " 2002551,\n",
       " 2002552,\n",
       " 2002553,\n",
       " 2002555,\n",
       " 2002556,\n",
       " 2002557,\n",
       " 2002558,\n",
       " 2002559,\n",
       " 2002560,\n",
       " 2002561,\n",
       " 2002562,\n",
       " 2002563,\n",
       " 2002564,\n",
       " 2002565,\n",
       " 2002566,\n",
       " 2002567,\n",
       " 2002568,\n",
       " 2002569,\n",
       " 2002570,\n",
       " 2002571,\n",
       " 2002572,\n",
       " 2002573,\n",
       " 2002574,\n",
       " 2002575,\n",
       " 2002576,\n",
       " 2002577,\n",
       " 2002578,\n",
       " 2002579,\n",
       " 2002580,\n",
       " 2002581,\n",
       " 2002582,\n",
       " 2002583,\n",
       " 2002584,\n",
       " 2002585,\n",
       " 2002586,\n",
       " 2002587,\n",
       " 2002588,\n",
       " 2002589,\n",
       " 2002590,\n",
       " 2002591,\n",
       " 2002592,\n",
       " 2002593,\n",
       " 2002594,\n",
       " 2002595,\n",
       " 2002596,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190430\n",
      "order finished\n",
      "0:00:08.613515\n",
      "0:00:22.890949\n",
      "20190506 unzip finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0663f68d24f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SecurityID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mOrderLog\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mOrderLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderLog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0mOrderLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOrderLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ChannelNo\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190301'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/order.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/order/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    OrderLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, encoding='GBK')\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        OrderLog += [df]\n",
    "    OrderLog = pd.concat(OrderLog).reset_index(drop=True)\n",
    "    OrderLog = OrderLog[OrderLog[\"ChannelNo\"] != 4001]\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"OrdType\": \"OrderType\"})\n",
    "    OrderLog[\"date\"] = OrderLog[\"TransactTime\"].iloc[0]//1000000000\n",
    "    OrderLog[\"OrderType\"] = np.where(OrderLog[\"OrderType\"] == 'U', 3, OrderLog[\"OrderType\"])\n",
    "    OrderLog[\"skey\"] = OrderLog[\"SecurityID\"] + 2000000\n",
    "    OrderLog[\"clockAtArrival\"] = OrderLog[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    OrderLog['datetime'] = OrderLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    OrderLog[\"time\"] = (OrderLog['TransactTime'] - int(OrderLog['TransactTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"OrderQty\", \"Side\", \"OrderType\"]:\n",
    "        OrderLog[col] = OrderLog[col].astype('int32')\n",
    "#     for cols in [\"Price\"]:\n",
    "#         print(cols)\n",
    "#         print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "    assert(OrderLog[((OrderLog[\"Side\"] != 1) & (OrderLog[\"Side\"] != 2)) | (OrderLog[\"OrderType\"].isnull())].shape[0] == 0)\n",
    "    da_te = str(OrderLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 2000000).unique()\n",
    "    del db1\n",
    "    try:\n",
    "        assert(len(set(sl) - set(OrderLog[\"skey\"].unique())) == 0)\n",
    "    except:\n",
    "        print(\"less stocks\")\n",
    "        display(set(sl) - set(OrderLog[\"skey\"].unique()))\n",
    "    if len(set(OrderLog[\"skey\"].unique()) - set(sl)) != 0:\n",
    "        print(\"more stocks\")\n",
    "        print(set(OrderLog[\"skey\"].unique()) - set(sl))\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"Side\":\"order_side\", \"OrderType\":\"order_type\", \"Price\":\"order_price\", \"OrderQty\":\"order_qty\"})\n",
    "    OrderLog = OrderLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"order_side\", \"order_type\", \"order_price\",\n",
    "                                                 \"order_qty\"]]\n",
    "    \n",
    "    print(OrderLog[\"date\"].iloc[0])\n",
    "    print(\"order finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_order', OrderLog)\n",
    "    \n",
    "    del OrderLog\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "#     pd.set_option(\"max_rows\", 200)\n",
    "#     display(OrderLog.dtypes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:195: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:55.347622\n",
      "0:00:00.400246\n",
      "20190430 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190430\n",
      "order finished\n",
      "0:10:42.801149\n",
      "0:00:00.585914\n",
      "20190506 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190506\n",
      "order finished\n",
      "0:11:37.269435\n",
      "0:00:00.698536\n",
      "20190507 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190507\n",
      "order finished\n",
      "0:10:52.471550\n",
      "0:00:00.937477\n",
      "20190508 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190508\n",
      "order finished\n",
      "0:10:52.595312\n",
      "0:00:00.928822\n",
      "20190509 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190509\n",
      "order finished\n",
      "0:09:56.419909\n",
      "0:00:00.838481\n",
      "20190510 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190510\n",
      "order finished\n",
      "0:13:27.600400\n",
      "0:00:00.912530\n",
      "20190513 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190513\n",
      "order finished\n",
      "0:10:29.605902\n",
      "0:00:00.652875\n",
      "20190514 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190514\n",
      "order finished\n",
      "0:10:33.576942\n",
      "0:00:00.928716\n",
      "20190515 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190515\n",
      "order finished\n",
      "0:09:52.974252\n",
      "0:00:25.628005\n",
      "20190516 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190516\n",
      "order finished\n",
      "0:09:55.215001\n",
      "0:00:28.107609\n",
      "20190517 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190517\n",
      "order finished\n",
      "0:11:29.250829\n",
      "0:00:28.145872\n",
      "20190520 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190520\n",
      "order finished\n",
      "0:10:41.090636\n",
      "0:00:24.752571\n",
      "20190521 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190521\n",
      "order finished\n",
      "0:10:52.023795\n",
      "0:00:23.945803\n",
      "20190522 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190522\n",
      "order finished\n",
      "0:10:35.117565\n",
      "0:00:22.836149\n",
      "20190523 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190523\n",
      "order finished\n",
      "0:10:03.381198\n",
      "0:00:21.130496\n",
      "20190524 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190524\n",
      "order finished\n",
      "0:09:16.515585\n",
      "0:00:00.202573\n",
      "20190525 unzip finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fe4ebda0d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SecurityID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mOrderLog\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mOrderLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrderLog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0mOrderLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOrderLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ChannelNo\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190430'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/order.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/order/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    OrderLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, encoding='GBK')\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        OrderLog += [df]\n",
    "    OrderLog = pd.concat(OrderLog).reset_index(drop=True)\n",
    "    OrderLog = OrderLog[OrderLog[\"ChannelNo\"] != 4001]\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"OrdType\": \"OrderType\"})\n",
    "    OrderLog[\"date\"] = OrderLog[\"TransactTime\"].iloc[0]//1000000000\n",
    "    OrderLog[\"OrderType\"] = np.where(OrderLog[\"OrderType\"] == 'U', 3, OrderLog[\"OrderType\"])\n",
    "    OrderLog[\"skey\"] = OrderLog[\"SecurityID\"] + 2000000\n",
    "    OrderLog[\"clockAtArrival\"] = OrderLog[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    OrderLog['datetime'] = OrderLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    OrderLog[\"time\"] = (OrderLog['TransactTime'] - int(OrderLog['TransactTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"OrderQty\", \"Side\", \"OrderType\"]:\n",
    "        OrderLog[col] = OrderLog[col].astype('int32')\n",
    "#     for cols in [\"Price\"]:\n",
    "#         print(cols)\n",
    "#         print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "    assert(OrderLog[((OrderLog[\"Side\"] != 1) & (OrderLog[\"Side\"] != 2)) | (OrderLog[\"OrderType\"].isnull())].shape[0] == 0)\n",
    "    da_te = str(OrderLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 2000000).unique()\n",
    "    del db1\n",
    "    try:\n",
    "        assert(len(set(sl) - set(OrderLog[\"skey\"].unique())) == 0)\n",
    "    except:\n",
    "        print(\"less stocks\")\n",
    "        display(set(sl) - set(OrderLog[\"skey\"].unique()))\n",
    "    if len(set(OrderLog[\"skey\"].unique()) - set(sl)) != 0:\n",
    "        print(\"more stocks\")\n",
    "        print(set(OrderLog[\"skey\"].unique()) - set(sl))\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"Side\":\"order_side\", \"OrderType\":\"order_type\", \"Price\":\"order_price\", \"OrderQty\":\"order_qty\"})\n",
    "    OrderLog = OrderLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"order_side\", \"order_type\", \"order_price\",\n",
    "                                                 \"order_qty\"]]\n",
    "    \n",
    "    print(OrderLog[\"date\"].iloc[0])\n",
    "    print(\"order finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_order', OrderLog)\n",
    "    \n",
    "    del OrderLog\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "#     pd.set_option(\"max_rows\", 200)\n",
    "#     display(OrderLog.dtypes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:195: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:35.141298\n",
      "0:00:22.990403\n",
      "20190527 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190527\n",
      "order finished\n",
      "0:09:47.739756\n",
      "0:00:22.401407\n",
      "20190528 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190528\n",
      "order finished\n",
      "0:09:53.393542\n",
      "0:00:22.151222\n",
      "20190529 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190529\n",
      "order finished\n",
      "0:10:07.339968\n",
      "0:00:22.610834\n",
      "20190530 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190530\n",
      "order finished\n",
      "0:10:24.698032\n",
      "0:00:24.193772\n",
      "20190531 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190531\n",
      "order finished\n",
      "0:09:21.672033\n",
      "0:00:24.532789\n",
      "20190603 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190603\n",
      "order finished\n",
      "0:10:14.586675\n",
      "0:00:21.232011\n",
      "20190604 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190604\n",
      "order finished\n",
      "0:09:00.005540\n",
      "0:00:22.146437\n",
      "20190605 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190605\n",
      "order finished\n",
      "0:08:56.289213\n",
      "0:00:22.202332\n",
      "20190606 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190606\n",
      "order finished\n",
      "0:08:47.256189\n",
      "0:00:20.262855\n",
      "20190610 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190610\n",
      "order finished\n",
      "0:08:04.772997\n",
      "0:00:26.198051\n",
      "20190611 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190611\n",
      "order finished\n",
      "0:10:36.691252\n",
      "0:00:25.510376\n",
      "20190612 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190612\n",
      "order finished\n",
      "0:10:36.942466\n",
      "0:00:23.676440\n",
      "20190613 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190613\n",
      "order finished\n",
      "0:09:02.331791\n",
      "0:00:23.524824\n",
      "20190614 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190614\n",
      "order finished\n",
      "0:09:24.101837\n",
      "0:00:24.261422\n",
      "20190617 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190617\n",
      "order finished\n",
      "0:08:07.223229\n",
      "0:00:21.134891\n",
      "20190618 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190618\n",
      "order finished\n",
      "0:08:43.737597\n",
      "0:00:24.863886\n",
      "20190619 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190619\n",
      "order finished\n",
      "0:09:27.255407\n",
      "0:00:29.917887\n",
      "20190620 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190620\n",
      "order finished\n",
      "0:10:38.425089\n",
      "0:00:29.841843\n",
      "20190621 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190621\n",
      "order finished\n",
      "0:11:03.686887\n",
      "0:00:26.114050\n",
      "20190624 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190624\n",
      "order finished\n",
      "0:09:40.419740\n",
      "0:00:23.990180\n",
      "20190625 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190625\n",
      "order finished\n",
      "0:09:53.727201\n",
      "0:00:21.122203\n",
      "20190626 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190626\n",
      "order finished\n",
      "0:07:54.049698\n",
      "0:00:22.671935\n",
      "20190627 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190627\n",
      "order finished\n",
      "0:08:42.623874\n",
      "0:00:22.159428\n",
      "20190628 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190628\n",
      "order finished\n",
      "0:08:10.782680\n",
      "0:00:25.771715\n",
      "20190701 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190701\n",
      "order finished\n",
      "0:11:04.608401\n",
      "0:00:25.098176\n",
      "20190702 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190702\n",
      "order finished\n",
      "0:10:41.323574\n",
      "0:00:23.656463\n",
      "20190703 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190703\n",
      "order finished\n",
      "0:09:26.092802\n",
      "0:00:21.672352\n",
      "20190704 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190704\n",
      "order finished\n",
      "0:08:38.819748\n",
      "0:00:21.982370\n",
      "20190705 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190705\n",
      "order finished\n",
      "0:07:51.753612\n",
      "0:00:23.560943\n",
      "20190708 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190708\n",
      "order finished\n",
      "0:09:37.698403\n",
      "0:00:20.118608\n",
      "20190709 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190709\n",
      "order finished\n",
      "0:07:34.179593\n",
      "0:00:18.000459\n",
      "20190710 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190710\n",
      "order finished\n",
      "0:07:40.653288\n",
      "0:00:18.636108\n",
      "20190711 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190711\n",
      "order finished\n",
      "0:07:43.352832\n",
      "0:00:17.764866\n",
      "20190712 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190712\n",
      "order finished\n",
      "0:07:35.507057\n",
      "0:00:25.457104\n",
      "20190715 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190715\n",
      "order finished\n",
      "0:09:16.869867\n",
      "0:00:20.345011\n",
      "20190716 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190716\n",
      "order finished\n",
      "0:09:20.840168\n",
      "0:00:20.990922\n",
      "20190717 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190717\n",
      "order finished\n",
      "0:09:49.912356\n",
      "0:00:20.821508\n",
      "20190718 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190718\n",
      "order finished\n",
      "0:08:26.139524\n",
      "0:00:19.639523\n",
      "20190719 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190719\n",
      "order finished\n",
      "0:08:05.515350\n",
      "0:00:20.888087\n",
      "20190722 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190722\n",
      "order finished\n",
      "0:08:31.341307\n",
      "0:00:19.144258\n",
      "20190723 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190723\n",
      "order finished\n",
      "0:07:28.384775\n",
      "0:00:19.686660\n",
      "20190724 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190724\n",
      "order finished\n",
      "0:08:37.764288\n",
      "0:00:19.880308\n",
      "20190725 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190725\n",
      "order finished\n",
      "0:08:27.726726\n",
      "0:00:17.770818\n",
      "20190726 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190726\n",
      "order finished\n",
      "0:07:34.581824\n",
      "0:00:18.078476\n",
      "20190729 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190729\n",
      "order finished\n",
      "0:07:25.262076\n",
      "0:00:18.839362\n",
      "20190730 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190730\n",
      "order finished\n",
      "0:07:51.933640\n",
      "0:00:19.434039\n",
      "20190731 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190731\n",
      "order finished\n",
      "0:07:46.155333\n",
      "0:00:27.036242\n",
      "20190801 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190801\n",
      "order finished\n",
      "0:08:17.292468\n",
      "0:00:30.142256\n",
      "20190802 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190802\n",
      "order finished\n",
      "0:09:13.978329\n",
      "0:00:27.779407\n",
      "20190805 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190805\n",
      "order finished\n",
      "0:08:38.201154\n",
      "0:00:37.327864\n",
      "20190806 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190806\n",
      "order finished\n",
      "0:10:14.492657\n",
      "0:00:29.821006\n",
      "20190807 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190807\n",
      "order finished\n",
      "0:08:48.411436\n",
      "0:00:27.176017\n",
      "20190808 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190808\n",
      "order finished\n",
      "0:07:42.074602\n",
      "0:00:25.957641\n",
      "20190809 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190809\n",
      "order finished\n",
      "0:08:37.330631\n",
      "0:00:32.223073\n",
      "20190812 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190812\n",
      "order finished\n",
      "0:07:44.364999\n",
      "0:00:23.706860\n",
      "20190813 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190813\n",
      "order finished\n",
      "0:07:41.730489\n",
      "0:00:27.690735\n",
      "20190814 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190814\n",
      "order finished\n",
      "0:08:32.379760\n",
      "0:00:26.738870\n",
      "20190815 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190815\n",
      "order finished\n",
      "0:09:01.935819\n",
      "0:00:27.489689\n",
      "20190816 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190816\n",
      "order finished\n",
      "0:08:56.258752\n",
      "0:00:38.053377\n",
      "20190819 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190819\n",
      "order finished\n",
      "0:10:31.590167\n",
      "0:00:27.906627\n",
      "20190820 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190820\n",
      "order finished\n",
      "0:10:27.760259\n",
      "0:00:27.693400\n",
      "20190821 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190821\n",
      "order finished\n",
      "0:09:26.830128\n",
      "0:00:27.371462\n",
      "20190822 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190822\n",
      "order finished\n",
      "0:09:37.447259\n",
      "0:00:28.704218\n",
      "20190823 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190823\n",
      "order finished\n",
      "0:09:31.327228\n",
      "0:00:32.795811\n",
      "20190826 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190826\n",
      "order finished\n",
      "0:09:25.005447\n",
      "0:00:36.802150\n",
      "20190827 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190827\n",
      "order finished\n",
      "0:10:23.359735\n",
      "0:00:34.998972\n",
      "20190828 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190828\n",
      "order finished\n",
      "0:09:38.029967\n",
      "0:00:30.971833\n",
      "20190829 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190829\n",
      "order finished\n",
      "0:09:46.484251\n",
      "0:00:29.053018\n",
      "20190830 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190830\n",
      "order finished\n",
      "0:11:25.145569\n",
      "0:00:25.314056\n",
      "20190902 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190902\n",
      "order finished\n",
      "0:10:30.049841\n",
      "0:00:26.217050\n",
      "20190903 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190903\n",
      "order finished\n",
      "0:10:26.285500\n",
      "0:00:25.122149\n",
      "20190904 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190904\n",
      "order finished\n",
      "0:10:39.211277\n",
      "0:00:30.178651\n",
      "20190905 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190905\n",
      "order finished\n",
      "0:12:44.845701\n",
      "0:00:26.868835\n",
      "20190906 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190906\n",
      "order finished\n",
      "0:11:13.892028\n",
      "0:00:28.415211\n",
      "20190909 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190909\n",
      "order finished\n",
      "0:12:08.616434\n",
      "0:00:30.098219\n",
      "20190910 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190910\n",
      "order finished\n",
      "0:11:26.591286\n",
      "0:00:26.946898\n",
      "20190911 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190911\n",
      "order finished\n",
      "0:11:14.622193\n",
      "0:00:25.320665\n",
      "20190912 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190912\n",
      "order finished\n",
      "0:09:46.453716\n",
      "0:00:24.769444\n",
      "20190916 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190916\n",
      "order finished\n",
      "0:10:15.735168\n",
      "0:00:28.220009\n",
      "20190917 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190917\n",
      "order finished\n",
      "0:11:37.661568\n",
      "0:00:23.307611\n",
      "20190918 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190918\n",
      "order finished\n",
      "0:09:25.394436\n",
      "0:00:23.331219\n",
      "20190919 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190919\n",
      "order finished\n",
      "0:09:45.229358\n",
      "0:00:24.992110\n",
      "20190920 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190920\n",
      "order finished\n",
      "0:10:31.279345\n",
      "0:00:24.022410\n",
      "20190923 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190923\n",
      "order finished\n",
      "0:10:02.339996\n",
      "0:00:25.187639\n",
      "20190924 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190924\n",
      "order finished\n",
      "0:10:39.812514\n",
      "0:00:25.537028\n",
      "20190925 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190925\n",
      "order finished\n",
      "0:10:16.622378\n",
      "0:00:27.105988\n",
      "20190926 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190926\n",
      "order finished\n",
      "0:10:47.593379\n",
      "0:00:21.252358\n",
      "20190927 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190927\n",
      "order finished\n",
      "0:08:42.254018\n",
      "0:00:19.564588\n",
      "20190930 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190930\n",
      "order finished\n",
      "0:07:53.700942\n",
      "0:00:19.631959\n",
      "20191008 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191008\n",
      "order finished\n",
      "0:07:53.669943\n",
      "0:00:19.480796\n",
      "20191009 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191009\n",
      "order finished\n",
      "0:07:40.567769\n",
      "0:00:20.900305\n",
      "20191010 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191010\n",
      "order finished\n",
      "0:08:28.584368\n",
      "0:00:22.118839\n",
      "20191011 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191011\n",
      "order finished\n",
      "0:08:52.256528\n",
      "0:00:24.102685\n",
      "20191014 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191014\n",
      "order finished\n",
      "0:09:15.677411\n",
      "0:00:22.704056\n",
      "20191015 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191015\n",
      "order finished\n",
      "0:09:57.838159\n",
      "0:00:21.571231\n",
      "20191016 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191016\n",
      "order finished\n",
      "0:08:30.597678\n",
      "0:00:21.040717\n",
      "20191017 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191017\n",
      "order finished\n",
      "0:07:28.889246\n",
      "0:00:20.997869\n",
      "20191018 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191018\n",
      "order finished\n",
      "0:08:17.137376\n",
      "0:00:18.796348\n",
      "20191021 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191021\n",
      "order finished\n",
      "0:07:54.422747\n",
      "0:00:18.309851\n",
      "20191022 unzip finished\n",
      "20191022\n",
      "order finished\n",
      "0:08:12.840994\n",
      "0:00:18.982626\n",
      "20191023 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191023\n",
      "order finished\n",
      "0:07:46.362286\n",
      "0:00:18.439491\n",
      "20191024 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191024\n",
      "order finished\n",
      "0:08:06.850954\n",
      "0:00:21.776921\n",
      "20191025 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191025\n",
      "order finished\n",
      "0:09:02.955297\n",
      "0:00:22.422839\n",
      "20191028 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191028\n",
      "order finished\n",
      "0:09:14.610745\n",
      "0:00:23.060357\n",
      "20191029 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191029\n",
      "order finished\n",
      "0:10:15.640071\n",
      "0:00:22.673560\n",
      "20191030 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191030\n",
      "order finished\n",
      "0:09:34.617712\n",
      "0:00:22.185885\n",
      "20191031 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191031\n",
      "order finished\n",
      "0:08:55.100994\n",
      "0:00:20.279480\n",
      "20191101 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191101\n",
      "order finished\n",
      "0:08:30.096239\n",
      "0:00:20.724594\n",
      "20191104 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191104\n",
      "order finished\n",
      "0:08:32.307209\n",
      "0:00:21.282143\n",
      "20191105 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191105\n",
      "order finished\n",
      "0:08:53.310341\n",
      "0:00:21.335863\n",
      "20191106 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191106\n",
      "order finished\n",
      "0:08:39.433881\n",
      "0:00:19.627962\n",
      "20191107 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191107\n",
      "order finished\n",
      "0:08:05.228445\n",
      "0:00:22.017958\n",
      "20191108 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191108\n",
      "order finished\n",
      "0:09:26.568635\n",
      "0:00:23.119014\n",
      "20191111 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191111\n",
      "order finished\n",
      "0:08:16.315355\n",
      "0:00:19.595439\n",
      "20191112 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191112\n",
      "order finished\n",
      "0:09:10.717776\n",
      "0:00:19.885579\n",
      "20191113 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191113\n",
      "order finished\n",
      "0:07:25.838760\n",
      "0:00:20.266298\n",
      "20191114 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191114\n",
      "order finished\n",
      "0:07:35.835186\n",
      "0:00:19.019275\n",
      "20191115 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191115\n",
      "order finished\n",
      "0:08:08.640806\n",
      "0:00:19.317923\n",
      "20191118 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191118\n",
      "order finished\n",
      "0:07:49.008762\n",
      "0:00:20.364230\n",
      "20191119 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191119\n",
      "order finished\n",
      "0:08:13.293998\n",
      "0:00:21.083769\n",
      "20191120 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191120\n",
      "order finished\n",
      "0:08:49.123102\n",
      "0:00:19.304347\n",
      "20191121 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191121\n",
      "order finished\n",
      "0:08:18.947016\n",
      "0:00:24.578105\n",
      "20191122 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191122\n",
      "order finished\n",
      "0:10:06.335101\n",
      "0:00:22.864816\n",
      "20191125 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191125\n",
      "order finished\n",
      "0:09:12.890770\n",
      "0:00:19.699031\n",
      "20191126 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191126\n",
      "order finished\n",
      "0:08:31.003756\n",
      "0:00:18.819437\n",
      "20191127 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191127\n",
      "order finished\n",
      "0:07:57.344673\n",
      "0:00:18.412235\n",
      "20191128 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191128\n",
      "order finished\n",
      "0:07:20.823235\n",
      "0:00:17.486287\n",
      "20191129 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191129\n",
      "order finished\n",
      "0:07:05.513072\n",
      "0:00:18.270373\n",
      "20191202 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191202\n",
      "order finished\n",
      "0:07:46.940920\n",
      "0:00:18.964879\n",
      "20191203 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191203\n",
      "order finished\n",
      "0:08:20.170395\n",
      "0:00:19.133039\n",
      "20191204 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191204\n",
      "order finished\n",
      "0:07:54.353593\n",
      "0:00:20.980585\n",
      "20191205 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191205\n",
      "order finished\n",
      "0:08:30.010305\n",
      "0:00:21.044664\n",
      "20191206 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191206\n",
      "order finished\n",
      "0:08:50.858527\n",
      "0:00:22.764617\n",
      "20191209 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191209\n",
      "order finished\n",
      "0:09:03.182850\n",
      "0:00:21.756621\n",
      "20191210 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191210\n",
      "order finished\n",
      "0:09:07.172170\n",
      "0:00:24.700073\n",
      "20191211 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191211\n",
      "order finished\n",
      "0:09:33.712568\n",
      "0:00:22.703192\n",
      "20191212 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191212\n",
      "order finished\n",
      "0:09:31.120926\n",
      "0:00:24.432693\n",
      "20191213 unzip finished\n",
      "less stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2001914}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191213\n",
      "order finished\n",
      "0:10:23.688870\n",
      "0:00:26.482914\n",
      "20191216 unzip finished\n",
      "20191216\n",
      "order finished\n",
      "0:11:21.525050\n",
      "0:00:32.143114\n",
      "20191217 unzip finished\n",
      "20191217\n",
      "order finished\n",
      "0:12:40.995338\n",
      "0:00:30.120971\n",
      "20191218 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (6,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191218\n",
      "order finished\n",
      "0:12:21.118419\n",
      "0:00:26.876494\n",
      "20191219 unzip finished\n",
      "20191219\n",
      "order finished\n",
      "0:11:55.194595\n",
      "0:00:25.713338\n",
      "20191220 unzip finished\n",
      "20191220\n",
      "order finished\n",
      "0:12:04.889293\n",
      "0:00:24.676542\n",
      "20191223 unzip finished\n",
      "20191223\n",
      "order finished\n",
      "0:11:19.281273\n",
      "0:00:23.926462\n",
      "20191224 unzip finished\n",
      "20191224\n",
      "order finished\n",
      "0:09:51.358490\n",
      "0:00:23.444827\n",
      "20191225 unzip finished\n",
      "20191225\n",
      "order finished\n",
      "0:09:30.105544\n",
      "0:00:22.957167\n",
      "20191226 unzip finished\n",
      "20191226\n",
      "order finished\n",
      "0:09:33.611516\n",
      "0:00:27.691835\n",
      "20191227 unzip finished\n",
      "20191227\n",
      "order finished\n",
      "0:10:54.642796\n",
      "0:00:24.457763\n",
      "20191230 unzip finished\n",
      "20191230\n",
      "order finished\n",
      "0:11:29.261076\n",
      "0:00:24.459737\n",
      "20191231 unzip finished\n",
      "20191231\n",
      "order finished\n",
      "0:09:34.728295\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2019\"\n",
    "startDate = '20190525'\n",
    "endDate = '20191231'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "less = []\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/order***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/order.7z'\n",
    "    path = '/mnt/e/unzip_data/2019/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/order/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    OrderLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, encoding='GBK')\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        OrderLog += [df]\n",
    "    OrderLog = pd.concat(OrderLog).reset_index(drop=True)\n",
    "    OrderLog = OrderLog[OrderLog[\"ChannelNo\"] != 4001]\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"OrdType\": \"OrderType\"})\n",
    "    OrderLog[\"date\"] = OrderLog[\"TransactTime\"].iloc[0]//1000000000\n",
    "    OrderLog[\"OrderType\"] = np.where(OrderLog[\"OrderType\"] == 'U', 3, OrderLog[\"OrderType\"])\n",
    "    OrderLog[\"skey\"] = OrderLog[\"SecurityID\"] + 2000000\n",
    "    OrderLog[\"clockAtArrival\"] = OrderLog[\"TransactTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    OrderLog['datetime'] = OrderLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    OrderLog[\"time\"] = (OrderLog['TransactTime'] - int(OrderLog['TransactTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"OrderQty\", \"Side\", \"OrderType\"]:\n",
    "        OrderLog[col] = OrderLog[col].astype('int32')\n",
    "#     for cols in [\"Price\"]:\n",
    "#         print(cols)\n",
    "#         print(OrderLog[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "    \n",
    "    assert(OrderLog[((OrderLog[\"Side\"] != 1) & (OrderLog[\"Side\"] != 2)) | (OrderLog[\"OrderType\"].isnull())].shape[0] == 0)\n",
    "    da_te = str(OrderLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    sl = (db1[\"ID\"].str[2:].astype(int) + 2000000).unique()\n",
    "    del db1\n",
    "    try:\n",
    "        assert(len(set(sl) - set(OrderLog[\"skey\"].unique())) == 0)\n",
    "    except:\n",
    "        print(\"less stocks\")\n",
    "        display(set(sl) - set(OrderLog[\"skey\"].unique()))\n",
    "    if len(set(OrderLog[\"skey\"].unique()) - set(sl)) != 0:\n",
    "        print(\"more stocks\")\n",
    "        print(set(OrderLog[\"skey\"].unique()) - set(sl))\n",
    "    \n",
    "    OrderLog = OrderLog.rename(columns={\"Side\":\"order_side\", \"OrderType\":\"order_type\", \"Price\":\"order_price\", \"OrderQty\":\"order_qty\"})\n",
    "    OrderLog = OrderLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"order_side\", \"order_type\", \"order_price\",\n",
    "                                                 \"order_qty\"]]\n",
    "    \n",
    "    print(OrderLog[\"date\"].iloc[0])\n",
    "    print(\"order finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_order', OrderLog)\n",
    "    \n",
    "    del OrderLog\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "#     pd.set_option(\"max_rows\", 200)\n",
    "#     display(OrderLog.dtypes)\n",
    "print(less) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

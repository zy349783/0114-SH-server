{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:40.755851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:258: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600000</td>\n",
       "      <td>78702779</td>\n",
       "      <td>8.088123e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600004</td>\n",
       "      <td>15938002</td>\n",
       "      <td>2.387209e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600006</td>\n",
       "      <td>59249553</td>\n",
       "      <td>3.760849e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600007</td>\n",
       "      <td>1677548</td>\n",
       "      <td>2.187450e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600008</td>\n",
       "      <td>40480635</td>\n",
       "      <td>1.235960e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688600</td>\n",
       "      <td>394629</td>\n",
       "      <td>8.193718e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688777</td>\n",
       "      <td>10591512</td>\n",
       "      <td>1.181580e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688788</td>\n",
       "      <td>1350571</td>\n",
       "      <td>3.057764e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688981</td>\n",
       "      <td>47975621</td>\n",
       "      <td>2.903784e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1689009</td>\n",
       "      <td>9188218</td>\n",
       "      <td>6.419352e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1756 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  max_volume  max_amount     skey  cum_volume    cum_amount\n",
       "0     NaN         NaN         NaN  1600000    78702779  8.088123e+08\n",
       "1     NaN         NaN         NaN  1600004    15938002  2.387209e+08\n",
       "2     NaN         NaN         NaN  1600006    59249553  3.760849e+08\n",
       "3     NaN         NaN         NaN  1600007     1677548  2.187450e+07\n",
       "4     NaN         NaN         NaN  1600008    40480635  1.235960e+08\n",
       "...   ...         ...         ...      ...         ...           ...\n",
       "1751  NaN         NaN         NaN  1688600      394629  8.193718e+06\n",
       "1752  NaN         NaN         NaN  1688777    10591512  1.181580e+09\n",
       "1753  NaN         NaN         NaN  1688788     1350571  3.057764e+08\n",
       "1754  NaN         NaN         NaN  1688981    47975621  2.903784e+09\n",
       "1755  NaN         NaN         NaN  1689009     9188218  6.419352e+08\n",
       "\n",
       "[1756 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>max_volume</th>\n",
       "      <th>max_amount</th>\n",
       "      <th>skey</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600000</td>\n",
       "      <td>78702779</td>\n",
       "      <td>8.088123e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600004</td>\n",
       "      <td>15938002</td>\n",
       "      <td>2.387209e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600006</td>\n",
       "      <td>59249553</td>\n",
       "      <td>3.760849e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600007</td>\n",
       "      <td>1677548</td>\n",
       "      <td>2.187450e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600008</td>\n",
       "      <td>40480635</td>\n",
       "      <td>1.235960e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688600</td>\n",
       "      <td>394629</td>\n",
       "      <td>8.193718e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688777</td>\n",
       "      <td>10591512</td>\n",
       "      <td>1.181580e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688788</td>\n",
       "      <td>1350571</td>\n",
       "      <td>3.057764e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1688981</td>\n",
       "      <td>47975621</td>\n",
       "      <td>2.903784e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1689009</td>\n",
       "      <td>9188218</td>\n",
       "      <td>6.419352e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1756 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  max_volume  max_amount     skey  cum_volume    cum_amount\n",
       "0     NaN         NaN         NaN  1600000    78702779  8.088123e+08\n",
       "1     NaN         NaN         NaN  1600004    15938002  2.387209e+08\n",
       "2     NaN         NaN         NaN  1600006    59249553  3.760849e+08\n",
       "3     NaN         NaN         NaN  1600007     1677548  2.187450e+07\n",
       "4     NaN         NaN         NaN  1600008    40480635  1.235960e+08\n",
       "...   ...         ...         ...      ...         ...           ...\n",
       "1751  NaN         NaN         NaN  1688600      394629  8.193718e+06\n",
       "1752  NaN         NaN         NaN  1688777    10591512  1.181580e+09\n",
       "1753  NaN         NaN         NaN  1688788     1350571  3.057764e+08\n",
       "1754  NaN         NaN         NaN  1688981    47975621  2.903784e+09\n",
       "1755  NaN         NaN         NaN  1689009     9188218  6.419352e+08\n",
       "\n",
       "[1756 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02\n",
      "trade finished\n",
      "0:07:19.719912\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/mnt/ShareWithServer/day_stock_20200820/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20201202'\n",
    "endDate = '20201202'\n",
    "readPath = '/mnt/Kevin_zhenyu/KR_daily_data/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "for data in dataPathLs:\n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = data + '/SH/tick/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    TradeLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i)\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        TradeLog += [df]\n",
    "    TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "    \n",
    "    TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "    TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                        \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                       \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                       \"BuyNo\":\"BidApplSeqNum\"})\n",
    "    TradeLog[\"trade_type\"] = 1\n",
    "    TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "    TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "        TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "        TradeLog[col] = TradeLog[col].astype('int32')\n",
    "    \n",
    "    da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "    db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "    t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "    del db1\n",
    "    t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "    trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "    trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "    trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "    t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "    re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "    re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "    try:\n",
    "        assert(t1.shape[0] == t2.shape[0])\n",
    "        assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "        assert(re[re[\"cum_amount\"] != re[\"max_amount\"]].shape[0] == 0)\n",
    "    except:\n",
    "        display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "        display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "        display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "    del t1\n",
    "    del t2\n",
    "    del re\n",
    " \n",
    "    TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                                 \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "    print(da_te)\n",
    "    print(\"trade finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_trade', TradeLog)\n",
    "    \n",
    "    del TradeLog\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:06.127837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:251: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-04\n",
      "trade finished\n",
      "0:05:09.835836\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/mnt/ShareWithServer/day_stock_20200820/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "\n",
    "readPath = '/mnt/ShareWithServer/data/tick/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "TradeLog = []\n",
    "ll = []\n",
    "\n",
    "for i in dataPathLs:\n",
    "    try:\n",
    "        df = pd.read_csv(i)\n",
    "    except:\n",
    "        print(\"empty data\")\n",
    "        print(i)\n",
    "        ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "        continue\n",
    "    df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "    TradeLog += [df]\n",
    "TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "\n",
    "TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                    \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                   \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                   \"BuyNo\":\"BidApplSeqNum\"})\n",
    "TradeLog[\"trade_type\"] = 1\n",
    "TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "    TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "    TradeLog[col] = TradeLog[col].astype('int32')\n",
    "\n",
    "da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "db1 = db[db[\"date\"] == da_te]\n",
    "db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "del db1\n",
    "t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "try:\n",
    "    assert(t1.shape[0] == t2.shape[0])\n",
    "    assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "    assert(re[re[\"cum_amount\"] != re[\"max_amount\"]].shape[0] == 0)\n",
    "except:\n",
    "    display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "    display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "    display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "del t1\n",
    "del t2\n",
    "del re\n",
    "\n",
    "TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                             \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "print(da_te)\n",
    "print(\"trade finished\")\n",
    "\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db1.write('md_trade', TradeLog)\n",
    "\n",
    "del TradeLog\n",
    "\n",
    "print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:57.588621\n",
      "0:00:00.589226\n",
      "20200120 unzip finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:278: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20\n",
      "trade finished\n",
      "0:06:14.479419\n",
      "0:00:00.461148\n",
      "20200121 unzip finished\n",
      "2020-01-21\n",
      "trade finished\n",
      "0:05:18.158012\n",
      "0:00:00.359357\n",
      "20200122 unzip finished\n",
      "2020-01-22\n",
      "trade finished\n",
      "0:05:02.626590\n",
      "0:00:00.480545\n",
      "20200123 unzip finished\n",
      "2020-01-23\n",
      "trade finished\n",
      "0:05:59.660380\n",
      "0:00:13.564633\n",
      "20200203 unzip finished\n",
      "2020-02-03\n",
      "trade finished\n",
      "0:03:33.989391\n",
      "0:00:23.720851\n",
      "20200204 unzip finished\n",
      "2020-02-04\n",
      "trade finished\n",
      "0:07:02.898349\n",
      "0:00:24.161732\n",
      "20200205 unzip finished\n",
      "2020-02-05\n",
      "trade finished\n",
      "0:06:25.358272\n",
      "0:00:23.280874\n",
      "20200206 unzip finished\n",
      "2020-02-06\n",
      "trade finished\n",
      "0:06:33.720786\n",
      "0:00:22.209445\n",
      "20200207 unzip finished\n",
      "2020-02-07\n",
      "trade finished\n",
      "0:06:47.101387\n",
      "0:00:21.218911\n",
      "20200210 unzip finished\n",
      "2020-02-10\n",
      "trade finished\n",
      "0:06:15.773043\n",
      "0:00:20.933922\n",
      "20200211 unzip finished\n",
      "2020-02-11\n",
      "trade finished\n",
      "0:05:48.234075\n",
      "0:00:19.249754\n",
      "20200212 unzip finished\n",
      "2020-02-12\n",
      "trade finished\n",
      "0:05:15.555724\n",
      "0:00:20.802790\n",
      "20200213 unzip finished\n",
      "2020-02-13\n",
      "trade finished\n",
      "0:05:46.725841\n",
      "0:00:21.272150\n",
      "20200214 unzip finished\n",
      "2020-02-14\n",
      "trade finished\n",
      "0:05:29.530302\n",
      "0:00:24.345747\n",
      "20200217 unzip finished\n",
      "2020-02-17\n",
      "trade finished\n",
      "0:06:07.326731\n",
      "0:00:26.841544\n",
      "20200218 unzip finished\n",
      "2020-02-18\n",
      "trade finished\n",
      "0:06:46.105142\n",
      "0:00:21.717263\n",
      "20200219 unzip finished\n",
      "2020-02-19\n",
      "trade finished\n",
      "0:06:11.973168\n",
      "0:00:23.948531\n",
      "20200220 unzip finished\n",
      "2020-02-20\n",
      "trade finished\n",
      "0:06:44.551676\n",
      "0:00:24.839226\n",
      "20200221 unzip finished\n",
      "2020-02-21\n",
      "trade finished\n",
      "0:06:45.618382\n",
      "0:00:23.988973\n",
      "20200224 unzip finished\n",
      "2020-02-24\n",
      "trade finished\n",
      "0:07:14.612721\n",
      "0:00:27.673568\n",
      "20200225 unzip finished\n",
      "2020-02-25\n",
      "trade finished\n",
      "0:08:27.760603\n",
      "0:00:28.878955\n",
      "20200226 unzip finished\n",
      "2020-02-26\n",
      "trade finished\n",
      "0:08:45.352989\n",
      "0:00:23.874199\n",
      "20200227 unzip finished\n",
      "2020-02-27\n",
      "trade finished\n",
      "0:07:04.155919\n",
      "0:00:29.361351\n",
      "20200228 unzip finished\n",
      "2020-02-28\n",
      "trade finished\n",
      "0:08:12.398835\n",
      "0:00:24.906114\n",
      "20200302 unzip finished\n",
      "2020-03-02\n",
      "trade finished\n",
      "0:07:05.546799\n",
      "0:00:25.560924\n",
      "20200303 unzip finished\n",
      "2020-03-03\n",
      "trade finished\n",
      "0:07:37.529516\n",
      "0:00:23.386493\n",
      "20200304 unzip finished\n",
      "2020-03-04\n",
      "trade finished\n",
      "0:06:39.781997\n",
      "0:00:26.083316\n",
      "20200305 unzip finished\n",
      "2020-03-05\n",
      "trade finished\n",
      "0:07:48.164651\n",
      "0:00:22.115820\n",
      "20200306 unzip finished\n",
      "2020-03-06\n",
      "trade finished\n",
      "0:06:53.389645\n",
      "0:00:28.380612\n",
      "20200309 unzip finished\n",
      "2020-03-09\n",
      "trade finished\n",
      "0:08:02.699797\n",
      "0:00:27.880176\n",
      "20200310 unzip finished\n",
      "2020-03-10\n",
      "trade finished\n",
      "0:07:49.082348\n",
      "0:00:28.256025\n",
      "20200311 unzip finished\n",
      "2020-03-11\n",
      "trade finished\n",
      "0:07:10.896905\n",
      "0:00:26.551918\n",
      "20200312 unzip finished\n",
      "2020-03-12\n",
      "trade finished\n",
      "0:06:31.047532\n",
      "0:00:25.860108\n",
      "20200313 unzip finished\n",
      "2020-03-13\n",
      "trade finished\n",
      "0:07:27.113304\n",
      "0:00:27.828631\n",
      "20200316 unzip finished\n",
      "2020-03-16\n",
      "trade finished\n",
      "0:07:20.020172\n",
      "0:00:23.278591\n",
      "20200317 unzip finished\n",
      "2020-03-17\n",
      "trade finished\n",
      "0:06:47.514932\n",
      "0:00:22.709448\n",
      "20200318 unzip finished\n",
      "2020-03-18\n",
      "trade finished\n",
      "0:06:51.161696\n",
      "0:00:23.787746\n",
      "20200319 unzip finished\n",
      "2020-03-19\n",
      "trade finished\n",
      "0:06:57.388931\n",
      "0:00:19.866566\n",
      "20200320 unzip finished\n",
      "2020-03-20\n",
      "trade finished\n",
      "0:05:29.791469\n",
      "0:00:20.317069\n",
      "20200323 unzip finished\n",
      "2020-03-23\n",
      "trade finished\n",
      "0:05:32.030773\n",
      "0:00:23.256162\n",
      "20200324 unzip finished\n",
      "2020-03-24\n",
      "trade finished\n",
      "0:05:36.382805\n",
      "0:00:20.487361\n",
      "20200325 unzip finished\n",
      "2020-03-25\n",
      "trade finished\n",
      "0:06:06.602594\n",
      "0:00:21.365718\n",
      "20200326 unzip finished\n",
      "2020-03-26\n",
      "trade finished\n",
      "0:05:30.938902\n",
      "0:00:18.663192\n",
      "20200327 unzip finished\n",
      "2020-03-27\n",
      "trade finished\n",
      "0:05:25.979746\n",
      "0:00:18.631902\n",
      "20200330 unzip finished\n",
      "2020-03-30\n",
      "trade finished\n",
      "0:05:35.083341\n",
      "0:00:19.468975\n",
      "20200331 unzip finished\n",
      "2020-03-31\n",
      "trade finished\n",
      "0:04:57.109403\n",
      "0:00:16.960720\n",
      "20200401 unzip finished\n",
      "2020-04-01\n",
      "trade finished\n",
      "0:05:11.897926\n",
      "0:00:17.516212\n",
      "20200402 unzip finished\n",
      "2020-04-02\n",
      "trade finished\n",
      "0:04:55.559860\n",
      "0:00:16.370641\n",
      "20200403 unzip finished\n",
      "2020-04-03\n",
      "trade finished\n",
      "0:04:37.919625\n",
      "0:00:20.969604\n",
      "20200407 unzip finished\n",
      "2020-04-07\n",
      "trade finished\n",
      "0:05:21.222978\n",
      "0:00:18.809040\n",
      "20200408 unzip finished\n",
      "2020-04-08\n",
      "trade finished\n",
      "0:05:12.095207\n",
      "0:00:17.070366\n",
      "20200409 unzip finished\n",
      "2020-04-09\n",
      "trade finished\n",
      "0:04:50.635593\n",
      "0:00:17.435749\n",
      "20200410 unzip finished\n",
      "2020-04-10\n",
      "trade finished\n",
      "0:05:40.828273\n",
      "0:00:13.703272\n",
      "20200413 unzip finished\n",
      "2020-04-13\n",
      "trade finished\n",
      "0:04:02.793880\n",
      "0:00:15.878404\n",
      "20200414 unzip finished\n",
      "2020-04-14\n",
      "trade finished\n",
      "0:04:37.686412\n",
      "0:00:16.446165\n",
      "20200415 unzip finished\n",
      "2020-04-15\n",
      "trade finished\n",
      "0:04:51.612640\n",
      "0:00:18.298806\n",
      "20200416 unzip finished\n",
      "2020-04-16\n",
      "trade finished\n",
      "0:04:36.057656\n",
      "0:00:18.512167\n",
      "20200417 unzip finished\n",
      "2020-04-17\n",
      "trade finished\n",
      "0:05:45.528801\n",
      "0:00:16.803548\n",
      "20200420 unzip finished\n",
      "2020-04-20\n",
      "trade finished\n",
      "0:04:43.552177\n",
      "0:00:17.510164\n",
      "20200421 unzip finished\n",
      "2020-04-21\n",
      "trade finished\n",
      "0:05:17.403719\n",
      "0:00:16.127573\n",
      "20200422 unzip finished\n",
      "2020-04-22\n",
      "trade finished\n",
      "0:04:59.720491\n",
      "0:00:18.635678\n",
      "20200423 unzip finished\n",
      "2020-04-23\n",
      "trade finished\n",
      "0:05:22.396316\n",
      "0:00:19.531160\n",
      "20200424 unzip finished\n",
      "2020-04-24\n",
      "trade finished\n",
      "0:05:36.935018\n",
      "0:00:16.799139\n",
      "20200427 unzip finished\n",
      "2020-04-27\n",
      "trade finished\n",
      "0:04:51.205454\n",
      "0:00:20.343345\n",
      "20200428 unzip finished\n",
      "2020-04-28\n",
      "trade finished\n",
      "0:05:40.392446\n",
      "0:00:15.353131\n",
      "20200429 unzip finished\n",
      "2020-04-29\n",
      "trade finished\n",
      "0:04:49.273528\n",
      "0:00:21.153626\n",
      "20200430 unzip finished\n",
      "2020-04-30\n",
      "trade finished\n",
      "0:05:12.097763\n",
      "0:00:00.119808\n",
      "20200506 unzip finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcd0937e64b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SecurityID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mTradeLog\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mTradeLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTradeLog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mTradeLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTradeLog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TradeTime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m1000000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200120'\n",
    "endDate = '20200529'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SH/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/tick.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/tick/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    TradeLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i)\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        TradeLog += [df]\n",
    "    TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "    \n",
    "    TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "    TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                        \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                       \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                       \"BuyNo\":\"BidApplSeqNum\"})\n",
    "    TradeLog[\"trade_type\"] = 1\n",
    "    TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "    TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "        TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "        TradeLog[col] = TradeLog[col].astype('int32')\n",
    "    \n",
    "    da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "    db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "    t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "    del db1\n",
    "    t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "    trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "    trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "    trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "    t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "    re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "    re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "    try:\n",
    "        assert(t1.shape[0] == t2.shape[0])\n",
    "        assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "        assert(re[re[\"cum_amount\"].round(2) != re[\"max_amount\"]].shape[0] == 0)\n",
    "    except:\n",
    "        display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "        display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "        display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "    del t1\n",
    "    del t2\n",
    "    del re\n",
    " \n",
    "    TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                                 \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "    print(da_te)\n",
    "    print(\"trade finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_trade', TradeLog)\n",
    "    \n",
    "    del TradeLog\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:201: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:02:42.797509\n",
      "20200506 less data!!!!!!!!!!!!!!!!!\n",
      "20200507 less data!!!!!!!!!!!!!!!!!\n",
      "20200508 less data!!!!!!!!!!!!!!!!!\n",
      "20200511 less data!!!!!!!!!!!!!!!!!\n",
      "20200512 less data!!!!!!!!!!!!!!!!!\n",
      "20200513 less data!!!!!!!!!!!!!!!!!\n",
      "20200514 less data!!!!!!!!!!!!!!!!!\n",
      "20200515 less data!!!!!!!!!!!!!!!!!\n",
      "20200518 less data!!!!!!!!!!!!!!!!!\n",
      "20200519 less data!!!!!!!!!!!!!!!!!\n",
      "20200520 less data!!!!!!!!!!!!!!!!!\n",
      "20200521 less data!!!!!!!!!!!!!!!!!\n",
      "20200522 less data!!!!!!!!!!!!!!!!!\n",
      "20200525 less data!!!!!!!!!!!!!!!!!\n",
      "20200526 less data!!!!!!!!!!!!!!!!!\n",
      "20200527 less data!!!!!!!!!!!!!!!!!\n",
      "20200528 less data!!!!!!!!!!!!!!!!!\n",
      "20200529 less data!!!!!!!!!!!!!!!!!\n",
      "['/mnt/usb/data/2020/202005/20200506', '/mnt/usb/data/2020/202005/20200507', '/mnt/usb/data/2020/202005/20200508', '/mnt/usb/data/2020/202005/20200511', '/mnt/usb/data/2020/202005/20200512', '/mnt/usb/data/2020/202005/20200513', '/mnt/usb/data/2020/202005/20200514', '/mnt/usb/data/2020/202005/20200515', '/mnt/usb/data/2020/202005/20200518', '/mnt/usb/data/2020/202005/20200519', '/mnt/usb/data/2020/202005/20200520', '/mnt/usb/data/2020/202005/20200521', '/mnt/usb/data/2020/202005/20200522', '/mnt/usb/data/2020/202005/20200525', '/mnt/usb/data/2020/202005/20200526', '/mnt/usb/data/2020/202005/20200527', '/mnt/usb/data/2020/202005/20200528', '/mnt/usb/data/2020/202005/20200529']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal, Context, ROUND_HALF_UP\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SH' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200506'\n",
    "endDate = '20200529'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:\n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SH/tick***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SH/tick.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SH'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    readPath = path1 + '/tick/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[((dateLs >= 600000) & (dateLs <= 700000))]\n",
    "    TradeLog = []\n",
    "    ll = []\n",
    "    \n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i)\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"SecurityID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        TradeLog += [df]\n",
    "    TradeLog = pd.concat(TradeLog).reset_index(drop=True)\n",
    "    \n",
    "    TradeLog[\"date\"] = TradeLog[\"TradeTime\"].iloc[0]//1000000000\n",
    "    TradeLog = TradeLog.rename(columns={\"TradeQty\":\"trade_qty\", \"TradePrice\":\"trade_price\", \n",
    "                                        \"TradeBSFlag\":\"trade_flag\", \"TradeAmount\":\"trade_money\",\n",
    "                                       \"TradeIndex\":\"ApplSeqNum\", \"SellNo\":\"OfferApplSeqNum\",\n",
    "                                       \"BuyNo\":\"BidApplSeqNum\"})\n",
    "    TradeLog[\"trade_type\"] = 1\n",
    "    TradeLog[\"skey\"] = TradeLog[\"SecurityID\"] + 1000000\n",
    "    TradeLog[\"clockAtArrival\"] = TradeLog[\"TradeTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    TradeLog['datetime'] = TradeLog[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    TradeLog[\"time\"] = (TradeLog['TradeTime'] - int(TradeLog['TradeTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)*1000\n",
    "    TradeLog[\"trade_flag\"] = np.where(TradeLog[\"trade_flag\"] == 'B', 1, np.where(\n",
    "        TradeLog[\"trade_flag\"] == 'S', 2, 0))\n",
    "    for col in [\"skey\", \"date\", \"ApplSeqNum\", \"BidApplSeqNum\", \"OfferApplSeqNum\", \"trade_qty\", \"trade_type\", \"trade_flag\"]:\n",
    "        TradeLog[col] = TradeLog[col].astype('int32')\n",
    "    \n",
    "    da_te = str(TradeLog[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"max_volume\"] = db1.groupby(\"ID\")[\"d_volume\"].transform(\"max\")\n",
    "    db1[\"max_amount\"] = db1.groupby(\"ID\")[\"d_amount\"].transform(\"max\")\n",
    "    t1 = db1.groupby(\"ID\")[\"max_volume\", \"max_amount\"].first().reset_index()\n",
    "    del db1\n",
    "    t1[\"skey\"] = t1[\"ID\"].str[2:].astype(int) + 1000000\n",
    "    trade1 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_qty\"].sum().reset_index()\n",
    "    trade1.columns=[\"skey\", \"cum_volume\"]\n",
    "    trade2 = TradeLog[TradeLog[\"trade_type\"] == 1].groupby(\"skey\")[\"trade_money\"].sum().reset_index()\n",
    "    trade2.columns=[\"skey\", \"cum_amount\"]\n",
    "    t2 = pd.merge(trade1, trade2, on=\"skey\")\n",
    "    re = pd.merge(t1, t2, on=\"skey\", how=\"outer\")\n",
    "    re[\"cum_amount\"] = re[\"cum_amount\"].apply(lambda x: np.float(Decimal(round(x, 2)).normalize(Context(prec=len(str(x).split('.')[0]), rounding=ROUND_HALF_UP))))\n",
    "    try:\n",
    "        assert(t1.shape[0] == t2.shape[0])\n",
    "        assert(re[re[\"cum_volume\"] != re[\"max_volume\"]].shape[0] == 0)\n",
    "        assert(re[re[\"cum_amount\"].round(2) != re[\"max_amount\"]].shape[0] == 0)\n",
    "    except:\n",
    "        display(set(t1[\"skey\"]) - set(t2[\"skey\"]))\n",
    "        display(re[re[\"cum_volume\"] != re[\"max_volume\"]])\n",
    "        display(re[re[\"cum_amount\"] != re[\"max_amount\"]])\n",
    "    del t1\n",
    "    del t2\n",
    "    del re\n",
    " \n",
    "    TradeLog = TradeLog[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ApplSeqNum\", \"trade_type\", \"trade_flag\",\n",
    "                                                 \"trade_price\", \"trade_qty\", \"BidApplSeqNum\", \"OfferApplSeqNum\"]]\n",
    "    print(da_te)\n",
    "    print(\"trade finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_trade', TradeLog)\n",
    "    \n",
    "    del TradeLog\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "print(less)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:203: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:56.472050\n",
      "0:01:03.349658\n",
      "20200102 unzip finished\n",
      "0:01:11.653377\n",
      "0:02:21.370455\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:48.250228\n",
      "0:00:50.626954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:418: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:419: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:07.072645\n",
      "no massive missing\n",
      "0:03:51.720357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:37.295916\n",
      "0:01:00.653053\n",
      "20200103 unzip finished\n",
      "0:01:10.604227\n",
      "0:02:22.143124\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:21.318145\n",
      "0:00:46.599925\n",
      "0:00:06.464187\n",
      "no massive missing\n",
      "0:03:43.972926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:43.232141\n",
      "0:02:10.059180\n",
      "20200106 unzip finished\n",
      "0:01:17.060512\n",
      "0:02:30.087332\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:56.937353\n",
      "0:00:49.967711\n",
      "0:00:06.521285\n",
      "no massive missing\n",
      "0:03:51.894693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:52.708505\n",
      "0:01:04.587110\n",
      "20200107 unzip finished\n",
      "0:01:15.321266\n",
      "0:02:40.219061\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:57.354453\n",
      "0:00:47.190079\n",
      "0:00:06.180242\n",
      "no massive missing\n",
      "0:03:27.962546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:45.338465\n",
      "0:01:17.149528\n",
      "20200108 unzip finished\n",
      "0:01:25.878897\n",
      "0:02:31.800300\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:00.667754\n",
      "0:00:50.649271\n",
      "0:00:07.221027\n",
      "no massive missing\n",
      "0:04:09.419687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:57.168347\n",
      "0:01:01.614774\n",
      "20200109 unzip finished\n",
      "0:01:18.213314\n",
      "0:02:21.965148\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:28.281031\n",
      "0:00:49.910799\n",
      "0:00:06.100605\n",
      "no massive missing\n",
      "0:03:32.069698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:38.846364\n",
      "0:01:02.622468\n",
      "20200110 unzip finished\n",
      "0:01:13.089526\n",
      "0:02:23.460373\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:17.261963\n",
      "0:00:54.909065\n",
      "0:00:07.425843\n",
      "no massive missing\n",
      "0:03:35.707809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:29.825123\n",
      "0:01:15.923028\n",
      "20200113 unzip finished\n",
      "0:01:11.346308\n",
      "0:02:22.560687\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:08.057711\n",
      "0:00:52.265826\n",
      "0:00:06.161512\n",
      "no massive missing\n",
      "0:03:15.135087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:31.718877\n",
      "0:01:00.730349\n",
      "20200114 unzip finished\n",
      "0:01:19.109737\n",
      "0:02:23.575928\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:43.282863\n",
      "0:00:49.944516\n",
      "0:00:09.236069\n",
      "no massive missing\n",
      "0:03:53.171259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:28.283215\n",
      "0:01:52.437474\n",
      "20200115 unzip finished\n",
      "0:01:32.057518\n",
      "0:02:23.769401\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:14.195233\n",
      "0:00:46.998790\n",
      "0:00:06.663041\n",
      "no massive missing\n",
      "0:03:38.118899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:30.570186\n",
      "0:00:58.631534\n",
      "20200116 unzip finished\n",
      "0:01:09.226261\n",
      "0:02:21.356849\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:43.402814\n",
      "0:00:50.582251\n",
      "0:00:08.178359\n",
      "no massive missing\n",
      "0:03:23.030430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200116"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:37.278102\n",
      "0:00:59.900047\n",
      "20200117 unzip finished\n",
      "0:01:06.351161\n",
      "0:02:17.701251\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:37.704534\n",
      "0:00:50.782208\n",
      "0:00:06.037754\n",
      "no massive missing\n",
      "0:03:05.902725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:28.314764\n",
      "0:01:21.828365\n",
      "20200120 unzip finished\n",
      "0:01:08.246713\n",
      "0:02:24.391710\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:27.504204\n",
      "0:00:48.251185\n",
      "0:00:06.019104\n",
      "no massive missing\n",
      "0:03:11.282766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:32.068393\n",
      "0:01:02.786239\n",
      "20200121 unzip finished\n",
      "0:01:07.487421\n",
      "0:02:20.861773\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:35.786099\n",
      "0:00:58.263278\n",
      "0:00:10.500404\n",
      "no massive missing\n",
      "0:03:30.125845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:26.472442\n",
      "0:01:48.259618\n",
      "20200122 unzip finished\n",
      "0:01:08.842035\n",
      "0:02:26.216198\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:41.850923\n",
      "0:00:47.952279\n",
      "0:00:06.088753\n",
      "no massive missing\n",
      "0:03:19.269005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200122"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:35.310652\n",
      "0:01:19.790865\n",
      "20200123 unzip finished\n",
      "0:01:11.167035\n",
      "0:02:27.116035\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:39.767433\n",
      "0:00:54.680983\n",
      "0:00:06.402555\n",
      "no massive missing\n",
      "0:03:28.838691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:37.356347\n",
      "0:00:49.871979\n",
      "20200203 unzip finished\n",
      "0:00:43.058703\n",
      "0:01:29.422364\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:11:21.651730\n",
      "0:00:30.259735\n",
      "0:00:04.304411\n",
      "no massive missing\n",
      "0:02:11.367203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:07.593526\n",
      "0:01:09.021336\n",
      "20200204 unzip finished\n",
      "0:01:29.481396\n",
      "0:02:36.863463\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:46.599393\n",
      "0:00:48.945341\n",
      "0:00:06.402323\n",
      "no massive missing\n",
      "0:03:24.342711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:42.159267\n",
      "0:01:32.791706\n",
      "20200205 unzip finished\n",
      "0:01:15.182471\n",
      "0:02:30.594137\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:19.695746\n",
      "0:00:52.310048\n",
      "0:00:06.468019\n",
      "no massive missing\n",
      "0:03:30.314840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:51.064542\n",
      "0:01:08.590791\n",
      "20200206 unzip finished\n",
      "0:01:20.136782\n",
      "0:02:34.385762\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:44.707095\n",
      "0:00:52.184979\n",
      "0:00:06.727720\n",
      "no massive missing\n",
      "0:04:15.173808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:53.906673\n",
      "0:01:27.740623\n",
      "20200207 unzip finished\n",
      "0:01:19.030931\n",
      "0:02:27.988778\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:47.191257\n",
      "0:00:47.192332\n",
      "0:00:06.191771\n",
      "no massive missing\n",
      "0:04:02.611035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:43.206010\n",
      "0:01:11.075034\n",
      "20200210 unzip finished\n",
      "0:01:13.285579\n",
      "0:02:28.774262\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:46.632052\n",
      "0:00:55.448064\n",
      "0:00:15.252414\n",
      "no massive missing\n",
      "0:03:38.320206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200210"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:46.279556\n",
      "0:01:54.596219\n",
      "20200211 unzip finished\n",
      "0:01:13.017865\n",
      "0:02:24.051444\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:27.109009\n",
      "0:00:55.376541\n",
      "0:00:06.914753\n",
      "no massive missing\n",
      "0:03:37.079606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:30.490082\n",
      "0:01:11.355572\n",
      "20200212 unzip finished\n",
      "0:01:31.655795\n",
      "0:02:24.030725\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:28.400309\n",
      "0:00:46.864573\n",
      "0:00:06.332593\n",
      "no massive missing\n",
      "0:03:39.837892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:47.430163\n",
      "0:01:06.336977\n",
      "20200213 unzip finished\n",
      "0:01:17.143935\n",
      "0:02:26.730003\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:51.241291\n",
      "0:00:50.675004\n",
      "0:00:06.530711\n",
      "no massive missing\n",
      "0:03:21.566206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:32.712033\n",
      "0:01:05.546440\n",
      "20200214 unzip finished\n",
      "0:01:10.450264\n",
      "0:02:27.816121\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:47.380709\n",
      "0:01:17.944638\n",
      "0:00:06.315649\n",
      "no massive missing\n",
      "0:03:27.720501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:54.479367\n",
      "0:01:07.567922\n",
      "20200217 unzip finished\n",
      "0:02:29.550067\n",
      "0:02:34.859714\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:30.333430\n",
      "0:00:50.417787\n",
      "0:00:06.617609\n",
      "no massive missing\n",
      "0:03:40.030587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:55.576081\n",
      "0:02:20.043964\n",
      "20200218 unzip finished\n",
      "0:01:17.210342\n",
      "0:02:33.333976\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:54.085364\n",
      "0:00:53.324078\n",
      "0:00:08.149940\n",
      "no massive missing\n",
      "0:04:18.844818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:45.051285\n",
      "0:01:14.537896\n",
      "20200219 unzip finished\n",
      "0:01:14.970556\n",
      "0:02:34.462546\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:24.034337\n",
      "0:00:55.810995\n",
      "0:00:07.026637\n",
      "no massive missing\n",
      "0:04:05.927543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:57.620978\n",
      "0:01:42.258294\n",
      "20200220 unzip finished\n",
      "0:01:16.967923\n",
      "0:02:34.175689\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200101'\n",
    "endDate = '20200530'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    SZ = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,4,5,6,7,9,12,17,18,19,24,25,26,28,29,30,32,33,34,35])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SZ += [df]\n",
    "    del df\n",
    "    SZ = pd.concat(SZ).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"skey\"] = SZ[\"StockID\"] + 2000000\n",
    "    SZ.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SZ[\"date\"] = int(SZ[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SZ[\"time\"] = (SZ['QuotTime'] - int(SZ['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SZ[\"clockAtArrival\"] = SZ[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SZ['datetime'] = SZ[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    SZ.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"BidPrice\"] = SZ[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferPrice\"] = SZ[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidOrderQty\"] = SZ[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrderQty\"] = SZ[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidNumOrders\"] = SZ[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferNumOrders\"] = SZ[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'p'] = SZ[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'p'] = SZ[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'q'] = SZ[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'q'] = SZ[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SZ.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SZ.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SZ[\"BidOrders\"] = SZ[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrders\"] = SZ[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "        \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ.columns = ['cum_trades_cnt', 'total_bid_quantity', 'total_ask_quantity', 'close',\n",
    "       'total_ask_vwap', 'cum_amount', 'cum_volume', 'open', 'high',\n",
    "       'prev_close', 'low', 'total_bid_vwap', 'skey', 'date', 'time',\n",
    "       'clockAtArrival', 'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p',\n",
    "       'bid5p', 'bid6p', 'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p',\n",
    "       'ask2p', 'ask3p', 'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p',\n",
    "       'ask9p', 'ask10p', 'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q',\n",
    "       'bid6q', 'bid7q', 'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q',\n",
    "       'ask3q', 'ask4q', 'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q',\n",
    "       'ask10q', 'bid1n', 'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n',\n",
    "       'bid7n', 'bid8n', 'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n',\n",
    "       'ask4n', 'ask5n', 'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n',\n",
    "       'bid1Top1q', 'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q',\n",
    "       'bid1Top6q', 'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q',\n",
    "       'bid1Top11q', 'bid1Top12q', 'bid1Top13q', 'bid1Top14q',\n",
    "       'bid1Top15q', 'bid1Top16q', 'bid1Top17q', 'bid1Top18q',\n",
    "       'bid1Top19q', 'bid1Top20q', 'bid1Top21q', 'bid1Top22q',\n",
    "       'bid1Top23q', 'bid1Top24q', 'bid1Top25q', 'bid1Top26q',\n",
    "       'bid1Top27q', 'bid1Top28q', 'bid1Top29q', 'bid1Top30q',\n",
    "       'bid1Top31q', 'bid1Top32q', 'bid1Top33q', 'bid1Top34q',\n",
    "       'bid1Top35q', 'bid1Top36q', 'bid1Top37q', 'bid1Top38q',\n",
    "       'bid1Top39q', 'bid1Top40q', 'bid1Top41q', 'bid1Top42q',\n",
    "       'bid1Top43q', 'bid1Top44q', 'bid1Top45q', 'bid1Top46q',\n",
    "       'bid1Top47q', 'bid1Top48q', 'bid1Top49q', 'bid1Top50q',\n",
    "       'ask1Top1q', 'ask1Top2q', 'ask1Top3q', 'ask1Top4q', 'ask1Top5q',\n",
    "       'ask1Top6q', 'ask1Top7q', 'ask1Top8q', 'ask1Top9q', 'ask1Top10q',\n",
    "       'ask1Top11q', 'ask1Top12q', 'ask1Top13q', 'ask1Top14q',\n",
    "       'ask1Top15q', 'ask1Top16q', 'ask1Top17q', 'ask1Top18q',\n",
    "       'ask1Top19q', 'ask1Top20q', 'ask1Top21q', 'ask1Top22q',\n",
    "       'ask1Top23q', 'ask1Top24q', 'ask1Top25q', 'ask1Top26q',\n",
    "       'ask1Top27q', 'ask1Top28q', 'ask1Top29q', 'ask1Top30q',\n",
    "       'ask1Top31q', 'ask1Top32q', 'ask1Top33q', 'ask1Top34q',\n",
    "       'ask1Top35q', 'ask1Top36q', 'ask1Top37q', 'ask1Top38q',\n",
    "       'ask1Top39q', 'ask1Top40q', 'ask1Top41q', 'ask1Top42q',\n",
    "       'ask1Top43q', 'ask1Top44q', 'ask1Top45q', 'ask1Top46q',\n",
    "       'ask1Top47q', 'ask1Top48q', 'ask1Top49q', 'ask1Top50q']\n",
    "    \n",
    "    SZ = SZ.fillna(0)\n",
    "#     SZ[\"p1\"] = SZ[\"bid1p\"] + SZ[\"ask1p\"]\n",
    "#     tt = SZ[(SZ[\"cum_volume\"] > 0) & (SZ[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SZ.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SZ = SZ[~((SZ[\"bid1p\"] == 0) & (SZ[\"ask1p\"] == 0))]\n",
    "    SZ[\"ordering\"] = SZ.groupby(\"skey\").cumcount()\n",
    "    SZ[\"ordering\"] = SZ[\"ordering\"] + 1\n",
    "\n",
    "    for cols in [\"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration',\n",
    "                 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume', \"cum_canceled_buy_amount\",\n",
    "                 \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\", \"has_missing\"]:\n",
    "        SZ[cols] = 0\n",
    "        \n",
    "#     for col in [\"cum_volume\", \"total_bid_quantity\", \"total_ask_quantity\",'cum_canceled_buy_volume',\n",
    "#         'cum_canceled_sell_volume']:\n",
    "#         SZ[col] = SZ[col].astype('int64')\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SZ[col] = SZ[col].astype('int32')\n",
    "    \n",
    "        \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'total_bid_vwap', \"total_ask_vwap\",\n",
    "#                 \"cum_amount\"]:\n",
    "# #         SZ[cols] = SZ[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SZ[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "\n",
    "        \n",
    "    for cols in [\"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "        SZ[cols] = SZ[cols].astype('float64')\n",
    "\n",
    "        \n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SZ[\"prev_close\"] = np.where(SZ[\"time\"] >= 91500000000, SZ.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SZ[\"prev_close\"]) \n",
    "    SZ[\"open\"] = np.where(SZ[\"cum_volume\"] > 0, SZ.groupby(\"skey\")[\"open\"].transform(\"max\"), SZ[\"open\"])\n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SZ[SZ[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SZ[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 2000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SZ[\"cum_max\"] = SZ.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SZ[SZ[\"cum_volume\"] == SZ[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    SZ.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        display(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    del re\n",
    "    del s2\n",
    "    del db1\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SZ[\"group\"] = SZ[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SZ[\"group\"].unique())\n",
    "    SZ[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SZ[\"order\"] = SZ.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SZ[\"t\"] = SZ[SZ[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SZ[\"has_missing1\"] = np.where((SZ[\"time\"] == SZ[\"t\"]) & (SZ[\"order\"] == 0), 1, 0)\n",
    "        SZ.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SZ.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SZ[\"time_interval\"] = SZ.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SZ[\"time_interval\"] = SZ[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SZ[\"tn_update\"] = SZ.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SZ[(SZ[\"time\"] >= 93000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SZ[(SZ[\"time\"] >= 130000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SZ[(SZ[\"time\"] >= 150000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SZ = pd.merge(SZ, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SZ = pd.merge(SZ, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SZ = pd.merge(SZ, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SZ[(SZ[\"time\"] > 93000000000) & (SZ[\"time\"] < 145700000000) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SZ = pd.merge(SZ, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SZ[\"has_missing2\"] = 0\n",
    "    SZ[\"has_missing2\"] = np.where((SZ[\"time_interval\"] > 60) & (SZ[\"tn_update\"] > SZ[\"99%\"]) & \n",
    "         (SZ[\"time\"] > SZ[\"time1\"]) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"time\"] != SZ[\"time3\"])& (SZ[\"time\"] != 100000000000), 1, 0)\n",
    "    SZ.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SZ[\"has_missing\"] = np.where((SZ[\"has_missing1\"] == 1) | (SZ[\"has_missing2\"] == 1), 1, 0)\n",
    "    SZ.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SZ[SZ[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SZ[SZ[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SZ[SZ[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"has_missing\"] = SZ[\"has_missing\"].astype('int32')\n",
    "    SZ = SZ[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SZ[\"date\"].iloc[0])\n",
    "    print(\"SZ finished\")\n",
    "    \n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SZ)\n",
    "    \n",
    "    del SZ\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:203: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:42.247842\n",
      "0:01:10.108887\n",
      "20200304 unzip finished\n",
      "0:01:12.850436\n",
      "0:02:29.630405\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:39.306830\n",
      "0:00:54.769073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:418: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:419: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:06.880974\n",
      "no massive missing\n",
      "0:03:46.515764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200304"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:46.490618\n",
      "0:01:11.241957\n",
      "20200305 unzip finished\n",
      "0:01:16.015038\n",
      "0:02:37.474830\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:20:04.021968\n",
      "0:00:49.346792\n",
      "0:00:06.758887\n",
      "no massive missing\n",
      "0:03:40.636709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200305"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:02.680569\n",
      "0:01:08.370309\n",
      "20200306 unzip finished\n",
      "0:01:16.995978\n",
      "0:02:33.266044\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:37.591911\n",
      "0:00:49.806816\n",
      "0:00:06.757177\n",
      "no massive missing\n",
      "0:03:42.947048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200306"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:51.087254\n",
      "0:01:11.429916\n",
      "20200309 unzip finished\n",
      "0:01:20.069140\n",
      "0:02:36.962895\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:47.218071\n",
      "0:00:50.309017\n",
      "0:00:06.657090\n",
      "no massive missing\n",
      "0:03:37.894927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200309"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:47.629885\n",
      "0:01:16.132958\n",
      "20200310 unzip finished\n",
      "0:01:14.845931\n",
      "0:02:29.796062\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:25.349374\n",
      "0:00:51.492361\n",
      "0:00:06.721872\n",
      "no massive missing\n",
      "0:03:49.994052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200310"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:01.580157\n",
      "0:01:09.418936\n",
      "20200311 unzip finished\n",
      "0:01:16.619510\n",
      "0:02:29.941150\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:16.461846\n",
      "0:00:51.954904\n",
      "0:00:07.077479\n",
      "no massive missing\n",
      "0:03:53.031078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:04.021060\n",
      "0:01:09.549097\n",
      "20200312 unzip finished\n",
      "0:01:14.902847\n",
      "0:02:31.506896\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:16.066898\n",
      "0:00:49.442736\n",
      "0:00:06.914615\n",
      "no massive missing\n",
      "0:03:46.816758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:02.302632\n",
      "0:01:14.425621\n",
      "20200313 unzip finished\n",
      "0:01:17.876679\n",
      "0:02:35.000873\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:07.009409\n",
      "0:00:53.001888\n",
      "0:00:06.831910\n",
      "no massive missing\n",
      "0:03:39.226955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200313"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:45.413213\n",
      "0:01:13.050883\n",
      "20200316 unzip finished\n",
      "0:01:14.956979\n",
      "0:02:27.104196\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:37.502792\n",
      "0:00:50.033543\n",
      "0:00:06.762802\n",
      "no massive missing\n",
      "0:03:52.263621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200316"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:09.771597\n",
      "0:01:10.810793\n",
      "20200317 unzip finished\n",
      "0:01:09.357225\n",
      "0:02:26.770451\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:22.452113\n",
      "0:00:50.620159\n",
      "0:00:07.859300\n",
      "no massive missing\n",
      "0:03:42.836464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200317"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:57.555654\n",
      "0:01:09.958655\n",
      "20200318 unzip finished\n",
      "0:01:13.700083\n",
      "0:02:33.579349\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:58.435807\n",
      "0:00:53.073192\n",
      "0:00:06.502494\n",
      "no massive missing\n",
      "0:03:48.950032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200318"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:49.036556\n",
      "0:01:32.620468\n",
      "20200319 unzip finished\n",
      "0:01:16.008588\n",
      "0:02:38.532940\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:19:16.560626\n",
      "0:00:53.847153\n",
      "0:00:06.745434\n",
      "no massive missing\n",
      "0:03:40.659517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:58.801703\n",
      "0:01:06.682473\n",
      "20200320 unzip finished\n",
      "0:01:12.017416\n",
      "0:02:28.338773\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:16.627001\n",
      "0:00:51.268002\n",
      "0:00:06.923410\n",
      "no massive missing\n",
      "0:03:24.135303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200320"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:46.423383\n",
      "0:01:05.076515\n",
      "20200323 unzip finished\n",
      "0:01:11.738298\n",
      "0:02:18.734533\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:20.450046\n",
      "0:00:49.004088\n",
      "0:00:06.916285\n",
      "no massive missing\n",
      "0:03:37.241969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200323"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:29.122443\n",
      "0:01:07.382039\n",
      "20200324 unzip finished\n",
      "0:01:09.230174\n",
      "0:02:18.298879\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:22.495167\n",
      "0:00:50.088800\n",
      "0:00:06.886330\n",
      "no massive missing\n",
      "0:03:33.693643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:05.663372\n",
      "0:01:07.622198\n",
      "20200325 unzip finished\n",
      "0:01:10.249647\n",
      "0:02:18.536935\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:46.875602\n",
      "0:00:49.766876\n",
      "0:00:06.638981\n",
      "no massive missing\n",
      "0:03:23.892937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200325"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:51.206851\n",
      "0:01:07.195735\n",
      "20200326 unzip finished\n",
      "0:01:13.344070\n",
      "0:02:26.090967\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:43.956657\n",
      "0:00:50.506632\n",
      "0:00:06.783785\n",
      "no massive missing\n",
      "0:03:26.951355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:54.871675\n",
      "0:01:04.166286\n",
      "20200327 unzip finished\n",
      "0:01:10.282541\n",
      "0:02:37.921721\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:01.576984\n",
      "0:00:49.324295\n",
      "0:00:06.565027\n",
      "no massive missing\n",
      "0:03:14.881633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200327"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:39.089042\n",
      "0:01:02.313477\n",
      "20200330 unzip finished\n",
      "0:01:09.979601\n",
      "0:02:18.568902\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:03.215262\n",
      "0:00:48.681513\n",
      "0:00:06.861446\n",
      "no massive missing\n",
      "0:03:31.165059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200330"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:51.931732\n",
      "0:01:06.936557\n",
      "20200331 unzip finished\n",
      "0:01:05.367712\n",
      "0:02:10.249268\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:04.445286\n",
      "0:00:47.348787\n",
      "0:00:05.823904\n",
      "no massive missing\n",
      "0:03:11.158853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:51.847197\n",
      "0:01:06.631588\n",
      "20200401 unzip finished\n",
      "0:01:08.070617\n",
      "0:02:18.050388\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:17.173810\n",
      "0:00:49.338170\n",
      "0:00:07.359944\n",
      "no massive missing\n",
      "0:03:23.788621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:48.472229\n",
      "0:01:00.644439\n",
      "20200402 unzip finished\n",
      "0:01:07.544385\n",
      "0:02:17.604241\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:58.183378\n",
      "0:00:50.850184\n",
      "0:00:06.275002\n",
      "no massive missing\n",
      "0:03:34.599925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:38.165909\n",
      "0:00:59.879483\n",
      "20200403 unzip finished\n",
      "0:01:08.805288\n",
      "0:02:08.655893\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:41.349963\n",
      "0:00:47.365938\n",
      "0:00:06.466148\n",
      "no massive missing\n",
      "0:03:26.281294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200403"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:32.570446\n",
      "0:01:03.279628\n",
      "20200407 unzip finished\n",
      "0:01:11.677970\n",
      "0:02:27.071643\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:36.307184\n",
      "0:00:50.472099\n",
      "0:00:06.416290\n",
      "no massive missing\n",
      "0:03:27.453808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200407"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:36.698740\n",
      "0:01:04.923292\n",
      "20200408 unzip finished\n",
      "0:01:12.760221\n",
      "0:02:26.590323\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:28.458626\n",
      "0:00:49.286308\n",
      "0:00:06.688718\n",
      "no massive missing\n",
      "0:03:31.647971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:57.712621\n",
      "0:01:01.086786\n",
      "20200409 unzip finished\n",
      "0:01:10.329717\n",
      "0:02:18.445857\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:07.491925\n",
      "0:00:45.632015\n",
      "0:00:06.165944\n",
      "no massive missing\n",
      "0:03:20.780951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:46.429267\n",
      "0:01:06.712113\n",
      "20200410 unzip finished\n",
      "0:01:09.285572\n",
      "0:02:19.334964\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:10.984336\n",
      "0:00:47.349787\n",
      "0:00:06.276717\n",
      "no massive missing\n",
      "0:03:28.805524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200410"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:07.850649\n",
      "0:01:00.957368\n",
      "20200413 unzip finished\n",
      "0:01:11.303164\n",
      "0:02:13.406855\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:58.517836\n",
      "0:00:47.157059\n",
      "0:00:06.182820\n",
      "no massive missing\n",
      "0:03:18.160779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:26.546669\n",
      "0:01:04.425196\n",
      "20200414 unzip finished\n",
      "0:01:06.108975\n",
      "0:02:08.422682\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:46.726006\n",
      "0:00:45.649106\n",
      "0:00:06.107745\n",
      "no massive missing\n",
      "0:03:11.820308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200414"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:52.169071\n",
      "0:01:00.604003\n",
      "20200415 unzip finished\n",
      "0:01:06.289338\n",
      "0:02:18.116965\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:05.928488\n",
      "0:00:46.969818\n",
      "0:00:05.734430\n",
      "no massive missing\n",
      "0:03:13.532166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200415"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:50.200947\n",
      "0:01:00.188434\n",
      "20200416 unzip finished\n",
      "0:01:04.606400\n",
      "0:02:14.842708\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:58.298555\n",
      "0:00:47.167338\n",
      "0:00:06.452408\n",
      "no massive missing\n",
      "0:03:18.909186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:26.531180\n",
      "0:01:03.631327\n",
      "20200417 unzip finished\n",
      "0:01:09.478251\n",
      "0:02:13.090744\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:00.054589\n",
      "0:00:48.777288\n",
      "0:00:06.464897\n",
      "no massive missing\n",
      "0:03:24.034915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200417"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:33.795554\n",
      "0:01:03.848737\n",
      "20200420 unzip finished\n",
      "0:01:07.154598\n",
      "0:02:09.819522\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:45.516069\n",
      "0:00:47.883747\n",
      "0:00:05.672282\n",
      "no massive missing\n",
      "0:03:26.784543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200420"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:45.325356\n",
      "0:01:02.487302\n",
      "20200421 unzip finished\n",
      "0:01:08.422182\n",
      "0:02:14.279985\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:41.415128\n",
      "0:00:47.188423\n",
      "0:00:06.011476\n",
      "no massive missing\n",
      "0:03:20.245369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200421"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:31.017417\n",
      "0:01:00.027592\n",
      "20200422 unzip finished\n",
      "0:01:02.892136\n",
      "0:02:09.490541\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:13.449070\n",
      "0:00:45.622929\n",
      "0:00:05.822999\n",
      "no massive missing\n",
      "0:03:14.184131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200422"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:28.009022\n",
      "0:00:59.376658\n",
      "20200423 unzip finished\n",
      "0:01:04.855701\n",
      "0:02:08.710898\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:42.806661\n",
      "0:00:48.402792\n",
      "0:00:07.251356\n",
      "no massive missing\n",
      "0:03:24.373916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200423"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:31.725661\n",
      "0:01:03.307082\n",
      "20200424 unzip finished\n",
      "0:01:11.950116\n",
      "0:02:10.440613\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:53.303839\n",
      "0:00:47.690567\n",
      "0:00:06.061743\n",
      "no massive missing\n",
      "0:03:22.116975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200424"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:24.439828\n",
      "0:00:54.965933\n",
      "20200427 unzip finished\n",
      "0:01:02.462662\n",
      "0:02:06.049100\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:16.562435\n",
      "0:00:47.617948\n",
      "0:00:06.077613\n",
      "no massive missing\n",
      "0:03:08.754103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200427"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:33.386892\n",
      "0:01:05.541421\n",
      "20200428 unzip finished\n",
      "0:01:11.276062\n",
      "0:02:12.448106\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:23.508185\n",
      "0:00:45.870959\n",
      "0:00:06.056162\n",
      "no massive missing\n",
      "0:03:12.042352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200428"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:42.054461\n",
      "0:00:57.478093\n",
      "20200429 unzip finished\n",
      "0:01:05.239964\n",
      "0:02:12.931958\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:25.762578\n",
      "0:00:47.205651\n",
      "0:00:06.390896\n",
      "no massive missing\n",
      "0:03:06.712133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:29.123333\n",
      "0:01:01.778664\n",
      "20200430 unzip finished\n",
      "0:01:08.994147\n",
      "0:02:20.594591\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:33.885971\n",
      "0:00:47.346054\n",
      "0:00:05.910887\n",
      "no massive missing\n",
      "0:03:18.190434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200430"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:42.796965\n",
      "0:01:04.453530\n",
      "20200506 unzip finished\n",
      "0:01:08.838847\n",
      "0:02:15.792247\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:52.988365\n",
      "0:00:45.271777\n",
      "0:00:05.985510\n",
      "no massive missing\n",
      "0:03:18.125559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200506"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:46.389561\n",
      "0:01:00.915564\n",
      "20200507 unzip finished\n",
      "0:01:06.589327\n",
      "0:02:10.991359\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:49.054799\n",
      "0:00:48.557273\n",
      "0:00:07.297826\n",
      "no massive missing\n",
      "0:03:19.241185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:31.597260\n",
      "0:01:09.787322\n",
      "20200508 unzip finished\n",
      "0:01:06.766939\n",
      "0:02:16.734842\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:06.901154\n",
      "0:00:48.030162\n",
      "0:00:05.927416\n",
      "no massive missing\n",
      "0:03:15.374688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:39.172152\n",
      "0:00:59.382624\n",
      "20200511 unzip finished\n",
      "0:01:06.986698\n",
      "0:02:13.465502\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:16.809552\n",
      "0:00:45.737996\n",
      "0:00:05.807336\n",
      "no massive missing\n",
      "0:03:10.450454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200511"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:44.644093\n",
      "0:00:56.151998\n",
      "20200512 unzip finished\n",
      "0:01:02.472359\n",
      "0:02:20.728795\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:01.565126\n",
      "0:00:45.800396\n",
      "0:00:07.611162\n",
      "no massive missing\n",
      "0:03:17.199666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200512"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:40.711196\n",
      "0:00:56.520072\n",
      "20200513 unzip finished\n",
      "0:00:58.822455\n",
      "0:02:00.276312\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:15:54.006819\n",
      "0:00:44.141853\n",
      "0:00:05.999586\n",
      "no massive missing\n",
      "0:03:02.564870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200513"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:15.918998\n",
      "0:01:08.899283\n",
      "20200514 unzip finished\n",
      "0:01:01.887267\n",
      "0:02:14.097579\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:11.165965\n",
      "0:00:45.963114\n",
      "0:00:05.865453\n",
      "no massive missing\n",
      "0:03:27.135130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:24.227377\n",
      "0:00:58.951980\n",
      "20200515 unzip finished\n",
      "0:01:03.267856\n",
      "0:02:14.209140\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:20.253140\n",
      "0:00:50.751425\n",
      "0:00:05.733326\n",
      "no massive missing\n",
      "0:03:19.211755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:48.698552\n",
      "0:01:01.009390\n",
      "20200518 unzip finished\n",
      "0:01:05.515667\n",
      "0:02:25.365465\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:56.624007\n",
      "0:00:43.312067\n",
      "0:00:05.611548\n",
      "no massive missing\n",
      "0:03:06.750550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:31.759751\n",
      "0:00:59.378940\n",
      "20200519 unzip finished\n",
      "0:01:05.493210\n",
      "0:02:08.817382\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:47.375070\n",
      "0:00:44.749565\n",
      "0:00:05.752127\n",
      "no massive missing\n",
      "0:03:04.944761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200519"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:45.926204\n",
      "0:01:00.054229\n",
      "20200520 unzip finished\n",
      "0:01:05.108091\n",
      "0:02:09.104796\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:52.865945\n",
      "0:00:44.921758\n",
      "0:00:05.713393\n",
      "no massive missing\n",
      "0:03:15.779046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:23.891245\n",
      "0:01:02.236417\n",
      "20200521 unzip finished\n",
      "0:01:03.562724\n",
      "0:02:08.058716\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:16:50.067692\n",
      "0:00:44.728453\n",
      "0:00:05.834183\n",
      "no massive missing\n",
      "0:03:16.900585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200521"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:49.220607\n",
      "0:01:06.288717\n",
      "20200522 unzip finished\n",
      "0:01:08.802127\n",
      "0:02:15.382525\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:17:17.533480\n",
      "0:00:48.363898\n",
      "0:00:06.741837\n",
      "no massive missing\n",
      "0:04:00.961934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:01:40.768322\n",
      "0:01:59.115366\n",
      "20200525 unzip finished\n",
      "0:01:08.077360\n",
      "0:02:16.176195\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200304'\n",
    "endDate = '20200530'\n",
    "readPath = '/mnt/usb/data/' + year + '/***/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in dataPathLs:    \n",
    "    \n",
    "    if len(np.array(glob.glob(data + '/SZ/***'))) == 0:\n",
    "        if int(os.path.basename(data)) not in date_list[\"Date\"].values:\n",
    "            continue\n",
    "        else:\n",
    "            print(os.path.basename(data) + \" less data!!!!!!!!!!!!!!!!!\")\n",
    "            less.append(data)\n",
    "            continue\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = os.path.basename(data)\n",
    "    rar_path = data + '/SZ/snapshot.7z'\n",
    "    path = '/mnt/e/unzip_data/2020/SZ'\n",
    "    path1 = path + '/' + date\n",
    "    un_path = path1\n",
    "    cmd = '7za x {} -o{}'.format(rar_path, un_path)\n",
    "    os.system(cmd)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    print(date + ' unzip finished')\n",
    "    \n",
    "    readPath = path1 + '/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    SZ = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,4,5,6,7,9,12,17,18,19,24,25,26,28,29,30,32,33,34,35])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SZ += [df]\n",
    "    del df\n",
    "    SZ = pd.concat(SZ).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"skey\"] = SZ[\"StockID\"] + 2000000\n",
    "    SZ.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SZ[\"date\"] = int(SZ[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SZ[\"time\"] = (SZ['QuotTime'] - int(SZ['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SZ[\"clockAtArrival\"] = SZ[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SZ['datetime'] = SZ[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    SZ.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"BidPrice\"] = SZ[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferPrice\"] = SZ[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidOrderQty\"] = SZ[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrderQty\"] = SZ[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidNumOrders\"] = SZ[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferNumOrders\"] = SZ[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'p'] = SZ[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'p'] = SZ[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'q'] = SZ[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'q'] = SZ[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SZ.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SZ.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SZ[\"BidOrders\"] = SZ[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrders\"] = SZ[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "        \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ.columns = ['cum_trades_cnt', 'total_bid_quantity', 'total_ask_quantity', 'close',\n",
    "       'total_ask_vwap', 'cum_amount', 'cum_volume', 'open', 'high',\n",
    "       'prev_close', 'low', 'total_bid_vwap', 'skey', 'date', 'time',\n",
    "       'clockAtArrival', 'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p',\n",
    "       'bid5p', 'bid6p', 'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p',\n",
    "       'ask2p', 'ask3p', 'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p',\n",
    "       'ask9p', 'ask10p', 'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q',\n",
    "       'bid6q', 'bid7q', 'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q',\n",
    "       'ask3q', 'ask4q', 'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q',\n",
    "       'ask10q', 'bid1n', 'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n',\n",
    "       'bid7n', 'bid8n', 'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n',\n",
    "       'ask4n', 'ask5n', 'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n',\n",
    "       'bid1Top1q', 'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q',\n",
    "       'bid1Top6q', 'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q',\n",
    "       'bid1Top11q', 'bid1Top12q', 'bid1Top13q', 'bid1Top14q',\n",
    "       'bid1Top15q', 'bid1Top16q', 'bid1Top17q', 'bid1Top18q',\n",
    "       'bid1Top19q', 'bid1Top20q', 'bid1Top21q', 'bid1Top22q',\n",
    "       'bid1Top23q', 'bid1Top24q', 'bid1Top25q', 'bid1Top26q',\n",
    "       'bid1Top27q', 'bid1Top28q', 'bid1Top29q', 'bid1Top30q',\n",
    "       'bid1Top31q', 'bid1Top32q', 'bid1Top33q', 'bid1Top34q',\n",
    "       'bid1Top35q', 'bid1Top36q', 'bid1Top37q', 'bid1Top38q',\n",
    "       'bid1Top39q', 'bid1Top40q', 'bid1Top41q', 'bid1Top42q',\n",
    "       'bid1Top43q', 'bid1Top44q', 'bid1Top45q', 'bid1Top46q',\n",
    "       'bid1Top47q', 'bid1Top48q', 'bid1Top49q', 'bid1Top50q',\n",
    "       'ask1Top1q', 'ask1Top2q', 'ask1Top3q', 'ask1Top4q', 'ask1Top5q',\n",
    "       'ask1Top6q', 'ask1Top7q', 'ask1Top8q', 'ask1Top9q', 'ask1Top10q',\n",
    "       'ask1Top11q', 'ask1Top12q', 'ask1Top13q', 'ask1Top14q',\n",
    "       'ask1Top15q', 'ask1Top16q', 'ask1Top17q', 'ask1Top18q',\n",
    "       'ask1Top19q', 'ask1Top20q', 'ask1Top21q', 'ask1Top22q',\n",
    "       'ask1Top23q', 'ask1Top24q', 'ask1Top25q', 'ask1Top26q',\n",
    "       'ask1Top27q', 'ask1Top28q', 'ask1Top29q', 'ask1Top30q',\n",
    "       'ask1Top31q', 'ask1Top32q', 'ask1Top33q', 'ask1Top34q',\n",
    "       'ask1Top35q', 'ask1Top36q', 'ask1Top37q', 'ask1Top38q',\n",
    "       'ask1Top39q', 'ask1Top40q', 'ask1Top41q', 'ask1Top42q',\n",
    "       'ask1Top43q', 'ask1Top44q', 'ask1Top45q', 'ask1Top46q',\n",
    "       'ask1Top47q', 'ask1Top48q', 'ask1Top49q', 'ask1Top50q']\n",
    "    \n",
    "    SZ = SZ.fillna(0)\n",
    "#     SZ[\"p1\"] = SZ[\"bid1p\"] + SZ[\"ask1p\"]\n",
    "#     tt = SZ[(SZ[\"cum_volume\"] > 0) & (SZ[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SZ.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SZ = SZ[~((SZ[\"bid1p\"] == 0) & (SZ[\"ask1p\"] == 0))]\n",
    "    SZ[\"ordering\"] = SZ.groupby(\"skey\").cumcount()\n",
    "    SZ[\"ordering\"] = SZ[\"ordering\"] + 1\n",
    "\n",
    "    for cols in [\"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration',\n",
    "                 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume', \"cum_canceled_buy_amount\",\n",
    "                 \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\", \"has_missing\"]:\n",
    "        SZ[cols] = 0\n",
    "        \n",
    "#     for col in [\"cum_volume\", \"total_bid_quantity\", \"total_ask_quantity\",'cum_canceled_buy_volume',\n",
    "#         'cum_canceled_sell_volume']:\n",
    "#         SZ[col] = SZ[col].astype('int64')\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SZ[col] = SZ[col].astype('int32')\n",
    "    \n",
    "        \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'total_bid_vwap', \"total_ask_vwap\",\n",
    "#                 \"cum_amount\"]:\n",
    "# #         SZ[cols] = SZ[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SZ[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "\n",
    "        \n",
    "    for cols in [\"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "        SZ[cols] = SZ[cols].astype('float64')\n",
    "\n",
    "        \n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SZ[\"prev_close\"] = np.where(SZ[\"time\"] >= 91500000000, SZ.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SZ[\"prev_close\"]) \n",
    "    SZ[\"open\"] = np.where(SZ[\"cum_volume\"] > 0, SZ.groupby(\"skey\")[\"open\"].transform(\"max\"), SZ[\"open\"])\n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SZ[SZ[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SZ[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 2000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SZ[\"cum_max\"] = SZ.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SZ[SZ[\"cum_volume\"] == SZ[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    SZ.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        display(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    del re\n",
    "    del s2\n",
    "    del db1\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SZ[\"group\"] = SZ[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SZ[\"group\"].unique())\n",
    "    SZ[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SZ[\"order\"] = SZ.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SZ[\"t\"] = SZ[SZ[\"group\"] > i].groupby(\"StockID\")[\"time\"].transform(\"min\")\n",
    "            SZ[\"has_missing1\"] = np.where((SZ[\"time\"] == SZ[\"t\"]) & (SZ[\"order\"] == 0), 1, 0)\n",
    "        SZ.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SZ.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SZ[\"time_interval\"] = SZ.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SZ[\"time_interval\"] = SZ[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SZ[\"tn_update\"] = SZ.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SZ[(SZ[\"time\"] >= 93000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SZ[(SZ[\"time\"] >= 130000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SZ[(SZ[\"time\"] >= 150000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SZ = pd.merge(SZ, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SZ = pd.merge(SZ, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SZ = pd.merge(SZ, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SZ[(SZ[\"time\"] > 93000000000) & (SZ[\"time\"] < 145700000000) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SZ = pd.merge(SZ, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SZ[\"has_missing2\"] = 0\n",
    "    SZ[\"has_missing2\"] = np.where((SZ[\"time_interval\"] > 60) & (SZ[\"tn_update\"] > SZ[\"99%\"]) & \n",
    "         (SZ[\"time\"] > SZ[\"time1\"]) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"time\"] != SZ[\"time3\"])& (SZ[\"time\"] != 100000000000), 1, 0)\n",
    "    SZ.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SZ[\"has_missing\"] = np.where((SZ[\"has_missing1\"] == 1) | (SZ[\"has_missing2\"] == 1), 1, 0)\n",
    "    SZ.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SZ[SZ[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SZ[SZ[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SZ[SZ[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"has_missing\"] = SZ[\"has_missing\"].astype('int32')\n",
    "    SZ = SZ[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SZ[\"date\"].iloc[0])\n",
    "    print(\"SZ finished\")\n",
    "    \n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.223\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SZ)\n",
    "    \n",
    "    del SZ\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:203: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:45.189952\n",
      "0:06:11.240198\n",
      "0:02:15.907036\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "0:18:49.911519\n",
      "0:00:51.479542\n",
      "0:00:08.104956\n",
      "no massive missing\n",
      "0:03:38.935821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20200721"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SZ finished\n",
      "0:02:39.438971\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c377d5435dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m \u001b[0mwr_ong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwr_ong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0mmi_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_ss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/home/work516/day_stock/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200721'\n",
    "endDate = '20200721'\n",
    "readPath = '/mnt/Kevin_zhenyu/KR_daily_data' + '/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[0] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "date_list = pd.read_csv(\"/home/work516/KR_upload_code/trading_days.csv\")\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in np.sort(dataPathLs):    \n",
    "    readPath = data + '/SZ/snapshot/***2/***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "    SZ = []\n",
    "    ll = []\n",
    "    startTm = datetime.datetime.now()\n",
    "    for i in dataPathLs:\n",
    "        try:\n",
    "            df = pd.read_csv(i, usecols = [0,1,4,5,6,7,9,12,17,18,19,24,25,26,28,29,30,32,33,34,35])\n",
    "        except:\n",
    "            print(\"empty data\")\n",
    "            print(i)\n",
    "            ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "            continue\n",
    "        df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "        SZ += [df]\n",
    "    del df\n",
    "    SZ = pd.concat(SZ).reset_index(drop=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"skey\"] = SZ[\"StockID\"] + 2000000\n",
    "    SZ.drop([\"StockID\"],axis=1,inplace=True)\n",
    "    SZ[\"date\"] = int(SZ[\"QuotTime\"].iloc[0]//1000000000)\n",
    "    SZ[\"time\"] = (SZ['QuotTime'] - int(SZ['QuotTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "    SZ[\"clockAtArrival\"] = SZ[\"QuotTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    SZ['datetime'] = SZ[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    SZ.drop([\"QuotTime\"],axis=1,inplace=True)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"BidPrice\"] = SZ[\"BidPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferPrice\"] = SZ[\"OfferPrice\"].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidOrderQty\"] = SZ[\"BidOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrderQty\"] = SZ[\"OfferOrderQty\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"BidNumOrders\"] = SZ[\"BidNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferNumOrders\"] = SZ[\"OfferNumOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'p'] = SZ[\"BidPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"BidPrice\"],axis=1,inplace=True)\n",
    "    print(\"1\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'p'] = SZ[\"OfferPrice\"].apply(lambda x: x[i-1],2)\n",
    "    SZ.drop([\"OfferPrice\"],axis=1,inplace=True)\n",
    "    print(\"2\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'q'] = SZ[\"BidOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"BidOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"3\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'q'] = SZ[\"OfferOrderQty\"].apply(lambda x: x[i-1])\n",
    "    SZ.drop([\"OfferOrderQty\"],axis=1,inplace=True)\n",
    "    print(\"4\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"BidNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid\" + str(i) + 'n'] = SZ[\"bid\" + str(i) + 'n'].astype('int32')\n",
    "    SZ.drop([\"BidNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"5\")\n",
    "    for i in range(1, 11):\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"OfferNumOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask\" + str(i) + 'n'] = SZ[\"ask\" + str(i) + 'n'].astype('int32') \n",
    "    SZ.drop([\"OfferNumOrders\"],axis=1,inplace=True)\n",
    "    print(\"6\")\n",
    "    \n",
    "    SZ[\"BidOrders\"] = SZ[\"BidOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "    SZ[\"OfferOrders\"] = SZ[\"OfferOrders\"].apply(lambda x: [int(i) for i in x[1:-1].split(',')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"BidOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"bid1Top\" + str(i) + 'q'] = SZ[\"bid1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"BidOrders\"],axis=1,inplace=True)\n",
    "    print(\"7\")\n",
    "    \n",
    "    for i in range(1, 51):\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"OfferOrders\"].apply(lambda x: x[i-1])\n",
    "        SZ[\"ask1Top\" + str(i) + 'q'] = SZ[\"ask1Top\" + str(i) + 'q'].astype('int32') \n",
    "    SZ.drop([\"OfferOrders\"],axis=1,inplace=True)\n",
    "    print(\"8\")\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "        \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ.columns = ['cum_trades_cnt', 'total_bid_quantity', 'total_ask_quantity', 'close',\n",
    "       'total_ask_vwap', 'cum_amount', 'cum_volume', 'open', 'high',\n",
    "       'prev_close', 'low', 'total_bid_vwap', 'skey', 'date', 'time',\n",
    "       'clockAtArrival', 'datetime', 'bid1p', 'bid2p', 'bid3p', 'bid4p',\n",
    "       'bid5p', 'bid6p', 'bid7p', 'bid8p', 'bid9p', 'bid10p', 'ask1p',\n",
    "       'ask2p', 'ask3p', 'ask4p', 'ask5p', 'ask6p', 'ask7p', 'ask8p',\n",
    "       'ask9p', 'ask10p', 'bid1q', 'bid2q', 'bid3q', 'bid4q', 'bid5q',\n",
    "       'bid6q', 'bid7q', 'bid8q', 'bid9q', 'bid10q', 'ask1q', 'ask2q',\n",
    "       'ask3q', 'ask4q', 'ask5q', 'ask6q', 'ask7q', 'ask8q', 'ask9q',\n",
    "       'ask10q', 'bid1n', 'bid2n', 'bid3n', 'bid4n', 'bid5n', 'bid6n',\n",
    "       'bid7n', 'bid8n', 'bid9n', 'bid10n', 'ask1n', 'ask2n', 'ask3n',\n",
    "       'ask4n', 'ask5n', 'ask6n', 'ask7n', 'ask8n', 'ask9n', 'ask10n',\n",
    "       'bid1Top1q', 'bid1Top2q', 'bid1Top3q', 'bid1Top4q', 'bid1Top5q',\n",
    "       'bid1Top6q', 'bid1Top7q', 'bid1Top8q', 'bid1Top9q', 'bid1Top10q',\n",
    "       'bid1Top11q', 'bid1Top12q', 'bid1Top13q', 'bid1Top14q',\n",
    "       'bid1Top15q', 'bid1Top16q', 'bid1Top17q', 'bid1Top18q',\n",
    "       'bid1Top19q', 'bid1Top20q', 'bid1Top21q', 'bid1Top22q',\n",
    "       'bid1Top23q', 'bid1Top24q', 'bid1Top25q', 'bid1Top26q',\n",
    "       'bid1Top27q', 'bid1Top28q', 'bid1Top29q', 'bid1Top30q',\n",
    "       'bid1Top31q', 'bid1Top32q', 'bid1Top33q', 'bid1Top34q',\n",
    "       'bid1Top35q', 'bid1Top36q', 'bid1Top37q', 'bid1Top38q',\n",
    "       'bid1Top39q', 'bid1Top40q', 'bid1Top41q', 'bid1Top42q',\n",
    "       'bid1Top43q', 'bid1Top44q', 'bid1Top45q', 'bid1Top46q',\n",
    "       'bid1Top47q', 'bid1Top48q', 'bid1Top49q', 'bid1Top50q',\n",
    "       'ask1Top1q', 'ask1Top2q', 'ask1Top3q', 'ask1Top4q', 'ask1Top5q',\n",
    "       'ask1Top6q', 'ask1Top7q', 'ask1Top8q', 'ask1Top9q', 'ask1Top10q',\n",
    "       'ask1Top11q', 'ask1Top12q', 'ask1Top13q', 'ask1Top14q',\n",
    "       'ask1Top15q', 'ask1Top16q', 'ask1Top17q', 'ask1Top18q',\n",
    "       'ask1Top19q', 'ask1Top20q', 'ask1Top21q', 'ask1Top22q',\n",
    "       'ask1Top23q', 'ask1Top24q', 'ask1Top25q', 'ask1Top26q',\n",
    "       'ask1Top27q', 'ask1Top28q', 'ask1Top29q', 'ask1Top30q',\n",
    "       'ask1Top31q', 'ask1Top32q', 'ask1Top33q', 'ask1Top34q',\n",
    "       'ask1Top35q', 'ask1Top36q', 'ask1Top37q', 'ask1Top38q',\n",
    "       'ask1Top39q', 'ask1Top40q', 'ask1Top41q', 'ask1Top42q',\n",
    "       'ask1Top43q', 'ask1Top44q', 'ask1Top45q', 'ask1Top46q',\n",
    "       'ask1Top47q', 'ask1Top48q', 'ask1Top49q', 'ask1Top50q']\n",
    "    \n",
    "    SZ = SZ.fillna(0)\n",
    "#     SZ[\"p1\"] = SZ[\"bid1p\"] + SZ[\"ask1p\"]\n",
    "#     tt = SZ[(SZ[\"cum_volume\"] > 0) & (SZ[\"time\"] < 145700000000)].groupby(\"skey\")['p1'].min()\n",
    "#     SZ.drop(\"p1\", axis=1, inplace=True)\n",
    "#     try:\n",
    "#         assert(tt[tt == 0].shape[0] == 0)\n",
    "#     except:\n",
    "#         display(tt[tt == 0])\n",
    "#     SZ = SZ[~((SZ[\"bid1p\"] == 0) & (SZ[\"ask1p\"] == 0))]\n",
    "    SZ[\"ordering\"] = SZ.groupby(\"skey\").cumcount()\n",
    "    SZ[\"ordering\"] = SZ[\"ordering\"] + 1\n",
    "\n",
    "    for cols in [\"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration',\n",
    "                 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume', \"cum_canceled_buy_amount\",\n",
    "                 \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\", \"has_missing\"]:\n",
    "        SZ[cols] = 0\n",
    "        \n",
    "#     for col in [\"cum_volume\", \"total_bid_quantity\", \"total_ask_quantity\",'cum_canceled_buy_volume',\n",
    "#         'cum_canceled_sell_volume']:\n",
    "#         SZ[col] = SZ[col].astype('int64')\n",
    "    \n",
    "    for col in [\"skey\", \"date\", \"cum_trades_cnt\", \"total_bid_orders\",\n",
    "        'total_ask_orders', 'total_bid_levels', 'total_ask_levels', 'cum_canceled_buy_orders','cum_canceled_sell_orders',\n",
    "            \"ordering\", 'bid_trade_max_duration', 'ask_trade_max_duration','has_missing']:\n",
    "        SZ[col] = SZ[col].astype('int32')\n",
    "    \n",
    "        \n",
    "#     for cols in [\"prev_close\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "#              'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'total_bid_vwap', \"total_ask_vwap\",\n",
    "#                 \"cum_amount\"]:\n",
    "# #         SZ[cols] = SZ[cols].apply(lambda x: round(x, 2)).astype('float64')\n",
    "#         print(cols)\n",
    "#         print(SZ[cols].astype(str).apply(lambda x: len(str(x.split('.')[1]))).unique())\n",
    "\n",
    "        \n",
    "    for cols in [\"cum_canceled_sell_amount\", \"cum_canceled_buy_amount\"]:\n",
    "        SZ[cols] = SZ[cols].astype('float64')\n",
    "\n",
    "        \n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    SZ[\"prev_close\"] = np.where(SZ[\"time\"] >= 91500000000, SZ.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SZ[\"prev_close\"]) \n",
    "    SZ[\"open\"] = np.where(SZ[\"cum_volume\"] > 0, SZ.groupby(\"skey\")[\"open\"].transform(\"max\"), SZ[\"open\"])\n",
    "    assert(sum(SZ[SZ[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(SZ[SZ[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "    assert(SZ[SZ[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    # check 1\n",
    "    startTm = datetime.datetime.now()\n",
    "    da_te = str(SZ[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 2000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SZ[\"cum_max\"] = SZ.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SZ[SZ[\"cum_volume\"] == SZ[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    SZ.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        display(re[re[\"d_amount_y\"].isnull()])\n",
    "        wr_ong += [re[re[\"d_amount_y\"].isnull()]]\n",
    "    del re\n",
    "    del s2\n",
    "    del db1\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    # check 2\n",
    "    # first part\n",
    "    startTm = datetime.datetime.now()\n",
    "    date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "    date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "    date[\"group\"] = date[\"time\"]//10000\n",
    "    SZ[\"group\"] = SZ[\"time\"]//10000000\n",
    "    gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] <= 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 150000000))][\"group\"].unique()\n",
    "    l = set(gl) - set(SZ[\"group\"].unique())\n",
    "    SZ[\"has_missing1\"] = 0 \n",
    "    if len(l) != 0:\n",
    "        print(\"massive missing\")\n",
    "        print(l)\n",
    "        SZ[\"order\"] = SZ.groupby([\"skey\", \"time\"]).cumcount()\n",
    "        for i in l:\n",
    "            SZ[\"t\"] = SZ[SZ[\"group\"] > i].groupby(\"skey\")[\"time\"].transform(\"min\")\n",
    "            SZ[\"has_missing1\"] = np.where((SZ[\"time\"] == SZ[\"t\"]) & (SZ[\"order\"] == 0), 1, 0)\n",
    "        SZ.drop([\"order\", \"t\", \"group\"], axis=1, inplace=True)   \n",
    "    else:\n",
    "        print(\"no massive missing\")\n",
    "        SZ.drop([\"group\"], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # second part\n",
    "\n",
    "    SZ[\"time_interval\"] = SZ.groupby(\"skey\")[\"datetime\"].apply(lambda x: x - x.shift(1))\n",
    "    SZ[\"time_interval\"] = SZ[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "    SZ[\"tn_update\"] = SZ.groupby(\"skey\")[\"cum_trades_cnt\"].apply(lambda x: x-x.shift(1))\n",
    "\n",
    "    f1 = SZ[(SZ[\"time\"] >= 93000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f1 = f1.rename(columns={\"time\": \"time1\"})\n",
    "    f2 = SZ[(SZ[\"time\"] >= 130000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f2 = f2.rename(columns={\"time\": \"time2\"})\n",
    "    f3 = SZ[(SZ[\"time\"] >= 150000000000) & (SZ[\"tn_update\"] != 0)].groupby(\"skey\")[\"time\"].min().reset_index()\n",
    "    f3 = f3.rename(columns={\"time\": \"time3\"})\n",
    "    SZ = pd.merge(SZ, f1, on=\"skey\", how=\"left\")\n",
    "    del f1\n",
    "    SZ = pd.merge(SZ, f2, on=\"skey\", how=\"left\")\n",
    "    del f2\n",
    "    SZ = pd.merge(SZ, f3, on=\"skey\", how=\"left\")\n",
    "    del f3\n",
    "    p99 = SZ[(SZ[\"time\"] > 93000000000) & (SZ[\"time\"] < 145700000000) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"tn_update\"] != 0)]\\\n",
    "    .groupby(\"skey\")[\"tn_update\"].apply(lambda x: x.describe([0.99])[\"99%\"]).reset_index()\n",
    "    p99 = p99.rename(columns={\"tn_update\":\"99%\"})\n",
    "    SZ = pd.merge(SZ, p99, on=\"skey\", how=\"left\")\n",
    "\n",
    "    SZ[\"has_missing2\"] = 0\n",
    "    SZ[\"has_missing2\"] = np.where((SZ[\"time_interval\"] > 60) & (SZ[\"tn_update\"] > SZ[\"99%\"]) & \n",
    "         (SZ[\"time\"] > SZ[\"time1\"]) & (SZ[\"time\"] != SZ[\"time2\"]) & (SZ[\"time\"] != SZ[\"time3\"])& (SZ[\"time\"] != 100000000000), 1, 0)\n",
    "    SZ.drop([\"time_interval\", \"tn_update\", \"time1\", \"time2\", \"time3\", \"99%\"], axis=1, inplace=True) \n",
    "\n",
    "    SZ[\"has_missing\"] = np.where((SZ[\"has_missing1\"] == 1) | (SZ[\"has_missing2\"] == 1), 1, 0)\n",
    "    SZ.drop([\"has_missing1\", \"has_missing2\"], axis=1, inplace=True) \n",
    "    if SZ[SZ[\"has_missing\"] == 1].shape[0] != 0:\n",
    "        print(\"has missing!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(SZ[SZ[\"has_missing\"] == 1].shape[0])\n",
    "        mi_ss += [SZ[SZ[\"has_missing\"] == 1]]\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    \n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    SZ[\"has_missing\"] = SZ[\"has_missing\"].astype('int32')\n",
    "    SZ = SZ[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"has_missing\", \"cum_trades_cnt\", \"cum_volume\", \"cum_amount\", \"prev_close\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"total_bid_quantity\", \"total_ask_quantity\",\"total_bid_vwap\", \"total_ask_vwap\",\n",
    "        \"total_bid_orders\",'total_ask_orders','total_bid_levels', 'total_ask_levels', 'bid_trade_max_duration', 'ask_trade_max_duration', 'cum_canceled_buy_orders', 'cum_canceled_buy_volume',\n",
    "        \"cum_canceled_buy_amount\", \"cum_canceled_sell_orders\", 'cum_canceled_sell_volume',\"cum_canceled_sell_amount\"]]\n",
    "    \n",
    "    display(SZ[\"date\"].iloc[0])\n",
    "    print(\"SZ finished\")\n",
    "    \n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_snapshot_l2', SZ)\n",
    "    \n",
    "    del SZ\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "wr_ong = pd.concat(wr_ong).reset_index(drop=True)\n",
    "print(wr_ong)\n",
    "mi_ss = pd.concat(mi_ss).reset_index(drop=True)\n",
    "print(mi_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:50.625960\n",
      "20200601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:225: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/work516/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:226: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200602\n",
      "20200603\n",
      "20200604\n",
      "20200605\n",
      "20200608\n",
      "20200609\n",
      "20200610\n",
      "20200611\n",
      "20200612\n",
      "20200615\n",
      "20200616\n",
      "20200617\n",
      "20200618\n",
      "20200619\n",
      "20200622\n",
      "20200623\n",
      "20200624\n",
      "20200629\n",
      "20200630\n",
      "20200701\n",
      "20200702\n",
      "20200703\n",
      "20200706\n",
      "20200707\n",
      "20200708\n",
      "20200709\n",
      "20200710\n",
      "20200713\n",
      "20200714\n",
      "20200715\n",
      "20200716\n",
      "20200717\n",
      "20200720\n",
      "20200721\n",
      "20200722\n",
      "20200723\n",
      "20200724\n",
      "20200727\n",
      "20200728\n",
      "20200729\n",
      "20200730\n",
      "20200731\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "startTm = datetime.datetime.now()\n",
    "readPath = '/mnt/ShareWithServer/day_stock_20200820/***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "print(datetime.datetime.now() - startTm)\n",
    "\n",
    "startDate = 20200601\n",
    "endDate = 20200731\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "all_SZ = db1.read('md_snapshot_l2', start_date=startDate, end_date=endDate, symbol=[2000001])\n",
    "\n",
    "for da_te in all_SZ['date'].unique():\n",
    "    print(da_te)\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    SZ = db1.read('md_snapshot_l2', start_date=str(da_te), end_date=str(da_te))\n",
    "    SZ = SZ[SZ['skey'] > 2000000]\n",
    "    da_te = str(SZ[\"date\"].iloc[0]) \n",
    "    da_te = da_te[:4] + '-' + da_te[4:6] + '-' + da_te[6:8]\n",
    "    db1 = db[db[\"date\"] == da_te]\n",
    "    db1[\"ID\"] = db1[\"ID\"].str[2:].astype(int) + 2000000\n",
    "    db1[\"date\"] = (db1[\"date\"].str[:4] + db1[\"date\"].str[5:7] + db1[\"date\"].str[8:]).astype(int)\n",
    "    SZ[\"cum_max\"] = SZ.groupby(\"skey\")[\"cum_volume\"].transform(max)\n",
    "    s2 = SZ[SZ[\"cum_volume\"] == SZ[\"cum_max\"]].groupby(\"skey\").first().reset_index()\n",
    "    SZ.drop(\"cum_max\", axis=1, inplace=True)\n",
    "    s2 = s2.rename(columns={\"skey\": \"ID\", 'open':\"d_open\", \"prev_close\":\"d_yclose\",\"high\":\"d_high\", \"low\":\"d_low\", \"close\":\"d_close\", \"cum_volume\":\"d_volume\", \"cum_amount\":\"d_amount\"})\n",
    "    s2 = s2[[\"ID\", \"date\", \"d_open\", \"d_yclose\", \"d_high\", \"d_low\", \"d_close\", \"d_volume\", \"d_amount\"]]\n",
    "    re = pd.merge(db1, s2, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "    try:\n",
    "        assert(sum(re[\"d_amount_y\"].isnull()) == 0)\n",
    "    except:\n",
    "        display(re[re[\"d_amount_y\"].isnull()])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
